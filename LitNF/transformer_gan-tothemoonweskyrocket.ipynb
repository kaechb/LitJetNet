{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cddfa09-f01f-47b9-9bbb-072a4ff90294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from nflows.distributions.base import Distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self, mean=None, std=None, epsilon=1e-7):\n",
    "        \"\"\"Standard Scaler.\n",
    "        The class can be used to normalize PyTorch Tensors using native functions. The module does not expect the\n",
    "        tensors to be of any specific shape; as long as the features are the last dimension in the tensor, the module\n",
    "        will work fine.\n",
    "        :param mean: The mean of the features. The property will be set after a call to fit.\n",
    "        :param std: The standard deviation of the features. The property will be set after a call to fit.\n",
    "        :param epsilon: Used to avoid a Division-By-Zero exception.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, values):\n",
    "        dims = list(range(values.dim() - 1))\n",
    "        self.mean = torch.mean(values, dim=dims)\n",
    "        self.std = torch.std(values, dim=dims)\n",
    "\n",
    "    def transform(self, values):\n",
    "        return (values - self.mean) / (self.std + self.epsilon)\n",
    "    def inverse_transform(self,values):\n",
    "        return (values *self.std)+self.mean\n",
    "    def fit_transform(self, values):\n",
    "        self.fit(values)\n",
    "        return self.transform(values)\n",
    "    def to(self,dev):\n",
    "        self.std=self.std.to(dev)\n",
    "        self.mean=self.mean.to(dev)\n",
    "        return self\n",
    "  \n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class JetNetDataloader(pl.LightningDataModule):\n",
    "    '''This is more or less standard boilerplate coded that builds the data loader of the training\n",
    "       one thing to note is the custom standard scaler that works on tensors\n",
    "       Currently only jets with 30 particles are used but this maybe changes soon'''\n",
    "    def __init__(self,config,batch_size): \n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.n_dim=config[\"n_dim\"]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        self.batch_size=batch_size\n",
    "    def setup(self,stage):\n",
    "    # This just sets up the dataloader, nothing particularly important. it reads in a csv, calculates mass and reads out the number particles per jet\n",
    "    # And adds it to the dataset as variable. The only important thing is that we add noise to zero padded jets\n",
    "        data_dir=os.environ[\"HOME\"]+\"/JetNet_NF/train_{}_jets.csv\".format(self.config[\"parton\"])\n",
    "        data=pd.read_csv(data_dir,sep=\" \",header=None)\n",
    "        jets=[]\n",
    "        limit=int(self.config[\"limit\"]*1.1)\n",
    "        for njets in range(1,31):\n",
    "            masks=np.sum(data.values[:,np.arange(3,120,4)],axis=1)\n",
    "            df=data.loc[masks==njets,:]\n",
    "            df=df.drop(np.arange(3,120,4),axis=1)\n",
    "            df[\"n\"]=njets\n",
    "            if len(df)>100:\n",
    "                jets.append(df[:self.config[\"limit\"]])\n",
    "        #stacking together differnet samples with different number particles per jet\n",
    "        self.n=torch.empty((0,1))\n",
    "        self.data=torch.empty((0,self.n_dim*self.n_part))\n",
    "        for i in range(len(jets)):\n",
    "            x=torch.tensor(jets[i].values[:,:self.n_dim*self.n_part]).float()\n",
    "            n=torch.tensor(jets[i][\"n\"].values).float()\n",
    "            self.data=torch.vstack((self.data,x))\n",
    "            self.n=torch.vstack((self.n.reshape(-1,1),n.reshape(-1,1)))        \n",
    "        \n",
    "      \n",
    "        # calculating mass per jet\n",
    "#         self.m=mass(self.data[:,:self.n_dim]).reshape(-1,1)  \n",
    "      # Adding noise to zero padded jets.\n",
    "        for i in torch.unique(self.n):\n",
    "            i=int(i)\n",
    "            self.data[self.data[:,-1]==i,3*i:90]=torch.normal(mean=torch.zeros_like(self.data[self.data[:,-1]==i,3*i:90]),std=1).abs()*1e-7\n",
    "        #standard scaling \n",
    "        self.scaler=StandardScaler()\n",
    "#         self.data=torch.hstack((self.data,self.m))        \n",
    "        self.scaler.fit(self.data)\n",
    "        self.data=self.scaler.transform(self.data)\n",
    "#         self.min_m=self.scaler.transform(torch.zeros((1,self.n_dim+1)))[0,-1]\n",
    "# #         self.data=torch.hstack((self.data,self.n))\n",
    "        \n",
    "#         #calculating mass dist in different bins, this is needed for the testcase where we need to generate the conditoon\n",
    "#         if self.config[\"variable\"]:\n",
    "#             self.mdists={}\n",
    "#             for i in torch.unique(self.n):\n",
    "#                 self.mdists[int(i)]=F(self.data[self.n[:,0]==i,-2])    \n",
    "        self.data,self.test_set=train_test_split(self.data.cpu().numpy(),test_size=0.3)\n",
    "        \n",
    "#         self.n_train=self.data[:,-1]\n",
    "#         self.n_test=self.test_set[:,-1]\n",
    "        \n",
    "            \n",
    "        self.test_set=torch.tensor(self.test_set).float()\n",
    "        self.data=torch.tensor(self.data).float()\n",
    "        self.num_batches=len(self.data)//self.config[\"batch_size\"]\n",
    "#         assert self.data.shape[1]==92\n",
    "        assert (torch.isnan(self.data)).sum()==0\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data, batch_size=self.batch_size,drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=len(self.test_set),drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9b2afd-b520-4ed2-853d-0e75992979f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thx max\n",
      "good boy\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86b6337e-577a-4298-a767-1b61e0dac727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import traceback\n",
    "import os\n",
    "import nflows as nf\n",
    "from nflows.utils.torchutils import create_random_binary_mask\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.coupling import *\n",
    "from nflows.nn import nets\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.flows import base\n",
    "from nflows.transforms.coupling import *\n",
    "from nflows.transforms.autoregressive import *\n",
    "from particle_net import ParticleNet\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import LambdaLR,ReduceLROnPlateau,ExponentialLR\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as FF\n",
    "import numpy as np\n",
    "from jetnet.evaluation import w1p, w1efp, w1m, cov_mmd,fpnd\n",
    "import mplhep as hep\n",
    "import hist\n",
    "from hist import Hist\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from collections import OrderedDict\n",
    "from ray import tune\n",
    "from helpers import *\n",
    "from plotting import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from helpers import CosineWarmupScheduler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.nn.functional import leaky_relu,sigmoid\n",
    "from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "class Gen(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_dim=3,l_dim=10,hidden=300,num_layers=3,num_heads=1,n_part=5,fc=False,dropout=0.5,no_hidden=True):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes=hidden\n",
    "        self.n_dim=n_dim\n",
    "        self.l_dim=l_dim\n",
    "        self.n_part=n_part\n",
    "        self.no_hidden=no_hidden\n",
    "       \n",
    "        self.fc=fc\n",
    "        if fc:\n",
    "            self.l_dim*=n_part \n",
    "            self.embbed_flat=nn.Linear(n_dim*n_part,l_dim)\n",
    "            self.flat_hidden=nn.Linear(l_dim,hidden)\n",
    "            self.flat_hidden2=nn.Linear(hidden,hidden)\n",
    "            self.flat_hidden3=nn.Linear(hidden,hidden)\n",
    "            self.flat_out=nn.Linear(hidden,n_dim*n_part)\n",
    "        else:\n",
    "            self.embbed=nn.Linear(n_dim,l_dim)\n",
    "            self.encoder=nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "            d_model=l_dim,nhead=num_heads,batch_first=True,norm_first=False\n",
    "            ,dim_feedforward=hidden,dropout=dropout) ,num_layers=num_layers)\n",
    "            self.hidden=nn.Linear(l_dim,hidden)\n",
    "            self.hidden2=nn.Linear(hidden,hidden)\n",
    "            self.dropout=nn.Dropout(dropout/2)\n",
    "            self.out=nn.Linear(hidden, n_dim)\n",
    "            self.out2=nn.Linear(l_dim, n_dim)\n",
    "\n",
    "            self.out_flat=nn.Linear(hidden,n_dim*n_part )\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        if self.fc:\n",
    "            x=x.reshape(len(x),self.n_part*self.n_dim)\n",
    "            x=self.embbed_flat(x)\n",
    "            x=leaky_relu(self.flat_hidden(x))\n",
    "#             x = self.dropout(x)\n",
    "            x=self.flat_out(x)\n",
    "            x=x.reshape(len(x),self.n_part,self.n_dim)\n",
    "        else:\n",
    "            x=self.embbed(x)\n",
    "            x=self.encoder(x)\n",
    "            if not self.no_hidden:\n",
    "                \n",
    "                x=leaky_relu(self.hidden(x))\n",
    "                x=self.dropout(x)\n",
    "                x=leaky_relu(self.hidden2(x))\n",
    "                x=self.dropout(x)\n",
    "                x=self.out(x)\n",
    "            else:\n",
    "                x=leaky_relu(x)\n",
    "                x=self.out2(x)\n",
    "        return x\n",
    "\n",
    "class Disc(nn.Module):\n",
    "    def __init__(self,n_dim=3,l_dim=10,hidden=300,num_layers=3,num_heads=1,n_part=2,fc=False,dropout=0.5,mass=False,clf=False):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes=hidden\n",
    "        self.n_dim=n_dim\n",
    "#         l_dim=n_dim\n",
    "        self.l_dim=l_dim\n",
    "        self.n_part=n_part\n",
    "        self.fc=fc\n",
    "        self.clf=clf\n",
    "        \n",
    "        if fc:\n",
    "            self.l_dim*=n_part \n",
    "            self.embbed_flat=nn.Linear(n_dim*n_part,l_dim)\n",
    "            self.flat_hidden=nn.Linear(l_dim,hidden)\n",
    "            self.flat_hidden2=nn.Linear(hidden,hidden)\n",
    "            self.flat_hidden3=nn.Linear(hidden,hidden)\n",
    "            self.flat_out=nn.Linear(hidden,1)\n",
    "        else:\n",
    "            self.embbed=nn.Linear(n_dim,l_dim)\n",
    "            self.encoder=nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.l_dim,nhead=num_heads,dim_feedforward=hidden,dropout=dropout,norm_first=False,\n",
    "                                       activation=lambda x:leaky_relu(x,0.2),batch_first=True) ,num_layers=num_layers)\n",
    "            self.hidden=nn.Linear(l_dim+int(mass),2*hidden)\n",
    "            self.hidden2=nn.Linear(2*hidden,hidden)\n",
    "            self.out=nn.Linear(hidden,1)\n",
    "\n",
    "    def forward(self,x,m=None):\n",
    "\n",
    "        if self.fc==True:\n",
    "            x=x.reshape(len(x),self.n_dim*self.n_part)\n",
    "            x=self.embbed_flat(x)\n",
    "            x=leaky_relu(self.flat_hidden(x),0.2)\n",
    "            x=leaky_relu(self.flat_hidden2(x),0.2)\n",
    "            x=self.flat_out(x)\n",
    "        else:\n",
    "            x=self.embbed(x)\n",
    "            if self.clf:\n",
    "                x=torch.concat((torch.ones_like(x[:,0,:]).reshape(len(x),1,-1),x),axis=1)\n",
    "                x=self.encoder(x)\n",
    "                x=x[:,0,:]\n",
    "            else:\n",
    "                x=self.encoder(x)\n",
    "                x=torch.sum(x,axis=1)\n",
    "            if m is not None:\n",
    "                x=torch.concat((m.reshape(len(x),1),x),axis=1)\n",
    "\n",
    "            x=leaky_relu(self.hidden(x),0.2)\n",
    "            \n",
    "            x=leaky_relu(self.hidden2(x),0.2)\n",
    "            \n",
    "            x=self.out(x)\n",
    "            x=x\n",
    "        return x\n",
    "      \n",
    "class TransGan(pl.LightningModule):\n",
    "    def create_resnet(self,in_features, out_features):\n",
    "        '''This is the network that outputs the parameters of the invertible transformation\n",
    "        The only arguments can be the in dimension and the out dimenson, the structure\n",
    "        of the network is defined over the config which is a class attribute\n",
    "        Context Features: Amount of features used to condition the flow - in our case \n",
    "        this is usually the mass\n",
    "        num_blocks: How many Resnet blocks should be used, one res net block is are 1 input+ 2 layers\n",
    "        and an additive skip connection from the first to the third'''\n",
    "        c=self.config[\"context_features\"]\n",
    "        return nets.ResidualNet(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                hidden_features=self.config[\"network_nodes_nf\"],\n",
    "                context_features=c,\n",
    "                num_blocks=self.config[\"network_layers_nf\"],\n",
    "                activation=self.config[\"activation\"]  if \"activation\" in self.config.keys() else FF.relu,\n",
    "                #dropout_probability=self.config[\"dropout\"] if \"dropout\" in self.config.keys() else 0,\n",
    "                use_batch_norm=self.config[\"batchnorm\"] if \"batchnorm\" in self.config.keys() else 0\n",
    "        )\n",
    "\n",
    "    def __init__(self,config,hyperopt,num_batches):\n",
    "        '''This initializes the model and its hyperparameters'''\n",
    "        super().__init__()\n",
    "        self.hyperopt=True\n",
    "        \n",
    "        self.start=time.time()\n",
    "        # self.batch_size=batch_size\n",
    "        # print(batch_size)\n",
    "        self.config=config\n",
    "        self.automatic_optimization=False\n",
    "        self.freq_d=config[\"freq\"]\n",
    "        \n",
    "        self.wgan=config[\"wgan\"]\n",
    "        #Metrics to track during the training\n",
    "        self.metrics={\"val_w1p\":[],\"val_w1m\":[],\"val_w1efp\":[],\"val_cov\":[],\"val_mmd\":[],\"val_fpnd\":[],\"val_logprob\":[],\"step\":[]}\n",
    "        #Loss function of the Normalizing flows\n",
    "        self.logprobs=[]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        # self.hparams.update(config)\n",
    "        self.save_hyperparameters()\n",
    "        self.flows = []\n",
    "        self.n_dim=self.config[\"n_dim\"]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        self.add_corr=config[\"corr\"]\n",
    "        self.alpha=1\n",
    "        self.num_batches=int(num_batches)\n",
    "        K=self.config[\"coupling_layers\"]\n",
    "        for i in range(K):\n",
    "            '''This creates the masks for the coupling layers, particle masks are masks\n",
    "            created such that each feature particle (eta,phi,pt) is masked together or not'''\n",
    "           \n",
    "            if self.config[\"autoreg\"]:\n",
    "                self.flows += [MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "#                         random_mask=True,\n",
    "                        features=self.n_dim,\n",
    "                        hidden_features=128,\n",
    "                        use_residual_blocks=True, \n",
    "                        tails='linear',\n",
    "                        tail_bound=self.config[\"tail_bound\"],\n",
    "                        num_bins=self.config[\"bins\"] )]\n",
    "            else:\n",
    "                mask=create_random_binary_mask(self.n_dim*self.n_part)            \n",
    "                self.flows += [PiecewiseRationalQuadraticCouplingTransform(\n",
    "                            mask=mask,\n",
    "                            transform_net_create_fn= self.create_resnet, \n",
    "                            tails='linear',\n",
    "                            tail_bound=self.config[\"tail_bound\"],\n",
    "                            num_bins=self.config[\"bins\"] )]\n",
    "\n",
    "        self.q0 = nf.distributions.normal.StandardNormal([self.n_dim*self.n_part])\n",
    "        self.q_test =nf.distributions.normal.StandardNormal([self.n_dim*self.n_part])\n",
    "        #Creates working flow model from the list of layer modules\n",
    "        self.flows=CompositeTransform(self.flows)\n",
    "        # Construct flow model\n",
    "        self.flow_test= base.Flow(distribution=self.q_test, transform=self.flows)\n",
    "        self.flow = base.Flow(distribution=self.q0, transform=self.flows)\n",
    "        \n",
    "\n",
    "        self.gen_net = Gen(n_dim=self.n_dim,hidden=config[\"hidden\"],num_layers=config[\"num_layers\"],dropout=config[\"dropout\"],no_hidden=config[\"no_hidden\"],fc= config[\"fc\"],n_part=config[\"n_part\"],l_dim=config[\"l_dim\"],num_heads=config[\"heads\"]).cuda()\n",
    "        self.dis_net = Disc(n_dim=self.n_dim,hidden=config[\"hidden\"],l_dim=config[\"l_dim\"],num_layers=config[\"num_layers\"],mass=self.config[\"mass\"],  num_heads=config[\"heads\"],fc=config[\"fc\"],n_part=config[\"n_part\"],dropout=config[\"dropout\"],clf=config[\"clf\"]).cuda()\n",
    "        self.sig=nn.Sigmoid()\n",
    "        for p in self.dis_net.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal(p)\n",
    "        self.nf_train=True\n",
    "        self.train_nf=config[\"max_epochs\"]//40\n",
    "    \n",
    "\n",
    "    def load_datamodule(self,data_module):\n",
    "        '''needed for lightning training to work, it just sets the dataloader for training and validation'''\n",
    "        self.data_module=data_module\n",
    "        \n",
    "    def on_after_backward(self) -> None:\n",
    "        '''This is a genious little hook, sometimes my model dies, i have no clue why. This saves the training from crashing and continues'''\n",
    "        valid_gradients = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                valid_gradients = not (torch.isnan(param.grad).any() or torch.isinf(param.grad).any())\n",
    "                if not valid_gradients:\n",
    "                    break\n",
    "        if not valid_gradients:\n",
    "            print(\"not valid grads\",self.counter)\n",
    "            self.zero_grad()\n",
    "            self.counter+=1\n",
    "            if self.counter>5:\n",
    "                raise ValueError('5 nangrads in a row')\n",
    "        else:\n",
    "            self.counter=0\n",
    "\n",
    "    def sampleandscale(self,batch,scale=False):\n",
    "        '''This is a helper function that samples from the flow (i.e. generates a new sample) \n",
    "            and reverses the standard scaling that is done in the preprocessing. This allows to calculate the mass\n",
    "            on the generative sample and to compare to the simulated one, we need to inverse the scaling before calculating the mass\n",
    "            because calculating the mass is a non linear transformation and does not commute with the mass calculation''' \n",
    "        z=self.flow.sample(len(batch)).reshape(len(batch),self.n_part,self.n_dim)\n",
    "        if self.add_corr: \n",
    "            fake=z+self.gen_net(z)#(1-self.alpha)*\n",
    "            fake=fake.reshape(len(batch),self.n_part,self.n_dim)\n",
    "        else:\n",
    "            fake=self.gen_net(z)\n",
    "        assert batch.device==fake.device\n",
    "\n",
    "        if scale:\n",
    "            self.data_module.scaler=self.data_module.scaler.to(batch.device)\n",
    "\n",
    "            fake_scaled=self.data_module.scaler.inverse_transform(fake.reshape(len(batch),self.n_dim*self.n_part))\n",
    "            z_scaled=self.data_module.scaler.inverse_transform(z.reshape(len(batch),self.n_dim*self.n_part))\n",
    "            true=self.data_module.scaler.inverse_transform(batch)\n",
    "            return fake,batch,z,fake_scaled,true,z_scaled\n",
    "        else:\n",
    "            return fake\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        self.losses=[]\n",
    "        \n",
    "        #mlosses are initialized with None during the time it is not turned on, makes it easier to plot\n",
    "        \n",
    "        opt_nf = torch.optim.AdamW(self.flow.parameters(), lr=self.config[\"lr_nf\"] )\n",
    "        if self.config[\"opt\"]==\"Adam\":\n",
    "\n",
    "            opt_g = torch.optim.Adam(self.gen_net.parameters(), lr=self.config[\"lr_g\"],betas=(0,.9))\n",
    "            opt_d = torch.optim.Adam(self.dis_net.parameters(), lr=self.config[\"lr_d\"],betas=(0,.9))\n",
    "        elif self.config[\"opt\"]==\"AdamW\":\n",
    "            opt_g = torch.optim.AdamW(self.gen_net.parameters(), lr=self.config[\"lr_g\"],betas=(0,.9))\n",
    "            opt_d = torch.optim.AdamW(self.dis_net.parameters(), lr=self.config[\"lr_d\"],betas=(0,.9))\n",
    "        elif self.config[\"opt\"]==\"SGD\":\n",
    "            opt_g = torch.optim.SGD(self.gen_net.parameters(), lr=self.config[\"lr_g\"])\n",
    "            opt_d = torch.optim.SGD(self.dis_net.parameters(), lr=self.config[\"lr_d\"])\n",
    "        else:\n",
    "            opt_g = torch.optim.RMSprop(self.gen_net.parameters(), lr=self.config[\"lr_g\"])\n",
    "            opt_d = torch.optim.RMSprop(self.dis_net.parameters(), lr=self.config[\"lr_d\"])\n",
    "        if self.config[\"sched\"]==\"cosine\":   \n",
    "            lr_scheduler_nf=CosineWarmupScheduler(opt_nf,warmup=1,max_iters=10000000*self.config[\"freq\"]) \n",
    "            lr_scheduler_d=CosineWarmupScheduler(opt_d,warmup=15*self.num_batches,max_iters=(self.config[\"max_epochs\"]-self.train_nf//2)*self.num_batches)\n",
    "            lr_scheduler_g=CosineWarmupScheduler(opt_g,warmup=15*self.num_batches,max_iters=(self.config[\"max_epochs\"]-self.train_nf)*self.num_batches//self.freq_d)\n",
    "        elif self.config[\"sched\"]==\"cosine2\":   \n",
    "            lr_scheduler_nf=CosineWarmupScheduler(opt_nf,warmup=1,max_iters=10000000*self.config[\"freq\"]) \n",
    "            lr_scheduler_d=CosineWarmupScheduler(opt_d,warmup=15*self.num_batches,max_iters=(self.config[\"max_epochs\"]-self.train_nf//2)*self.num_batches//2)\n",
    "            lr_scheduler_g=CosineWarmupScheduler(opt_g,warmup=15*self.num_batches,max_iters=(self.config[\"max_epochs\"]-self.train_nf)*self.num_batches//self.freq_d//2)\n",
    "        else:\n",
    "            lr_scheduler_nf =None \n",
    "            lr_scheduler_d =None    \n",
    "            lr_scheduler_g =None\n",
    "        if self.config[\"sched\"] !=None:\n",
    "            return  [opt_nf,opt_d,opt_g],[lr_scheduler_nf,lr_scheduler_d,lr_scheduler_g]\n",
    "        else:\n",
    "            return [opt_nf,opt_d,opt_g] \n",
    "    \n",
    "    \n",
    "    def compute_gradient_penalty(self,D, real_samples, fake_samples, phi):\n",
    "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "        alpha = torch.Tensor(np.random.random((real_samples.size(0),1, 1))).to(real_samples.device)\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
    "        if self.config[\"mass\"]:\n",
    "            m=mass(interpolates.reshape(len(real_samples),self.n_part*self.n_dim).detach())\n",
    "            d_interpolates = D.train()(interpolates.requires_grad_(True),m.requires_grad_(True) )\n",
    "        else:\n",
    "            d_interpolates = D.train()(interpolates.requires_grad_(True))\n",
    "        fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(real_samples.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "\n",
    "    def _summary(self,temp):\n",
    "        self.summary_path=\"/beegfs/desy/user/{}/{}/summary.csv\".format(os.environ[\"USER\"],self.config[\"name\"])\n",
    "        if os.path.isfile(self.summary_path):\n",
    "            \n",
    "            summary=pd.read_csv(self.summary_path).set_index([\"path_index\"])\n",
    "        else:\n",
    "            print(\"summary not found\")\n",
    "            summary=pd.DataFrame()\n",
    "\n",
    "            \n",
    "        summary.loc[self.logger.log_dir,self.config.keys()]=self.config.values()\n",
    "        summary.loc[self.logger.log_dir,temp.keys()]=temp.values()\n",
    "        summary.loc[self.logger.log_dir,\"time\"]=time.time()-self.start          \n",
    "        summary.to_csv(self.summary_path,index_label=[\"path_index\"])  \n",
    "        return summary\n",
    "    \n",
    "    # def _results(self,temp):\n",
    "    #     self.metrics[\"step\"].append(self.global_step)\n",
    "    #     self.df=pd.DataFrame.from_dict(temp,index=)\n",
    "    #     self.df.to_csv(self.logger.log_dir+\"result.csv\",index_label=[\"index\"])\n",
    "    \n",
    "   \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"training loop of the model, here all the data is passed forward to a gaussian\n",
    "            This is the important part what is happening here. This is all the training we do \"\"\"\n",
    "        \n",
    "        opt_nf,opt_d,opt_g=self.optimizers()\n",
    "        if self.config[\"sched\"]:\n",
    "            sched_nf,sched_d,sched_g=self.lr_schedulers()\n",
    "            \n",
    "\n",
    "        nf_loss=0\n",
    "        d_loss_avg=0\n",
    "        gradient_penalty=0\n",
    "        \n",
    "        \n",
    "        ### NF PART\n",
    "        if self.config[\"sched\"]!=None:\n",
    "            self.log(\"lr_g\",sched_g.get_last_lr()[-1],logger=True)\n",
    "            self.log(\"lr_nf\",sched_nf.get_last_lr()[-1],logger=True)\n",
    "            self.log(\"lr_d\",sched_d.get_last_lr()[-1],logger=True)\n",
    "            \n",
    "        if self.current_epoch<self.train_nf:\n",
    "            if self.config[\"sched\"]!=None:\n",
    "                sched_nf.step()\n",
    "            nf_loss -=self.flow.to(self.device).log_prob(batch).mean()#c if self.config[\"context_features\"] else None\n",
    "            nf_loss/=(self.n_dim*self.n_part) \n",
    "            opt_nf.zero_grad()\n",
    "            self.manual_backward(nf_loss)\n",
    "            opt_nf.step()\n",
    "            self.log(\"logprob\", nf_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True) \n",
    "\n",
    "        ### GAN PART\n",
    "        if self.current_epoch>=self.train_nf/2 or self.global_step==1:\n",
    "            \n",
    "            batch=batch.reshape(len(batch),self.n_part,self.n_dim)\n",
    "            fake=self.sampleandscale(batch,scale=False)\n",
    "#             fake=torch.rand((len(batch),self.n_part,self.n_dim)).cuda()+10\n",
    "            if self.config[\"mass\"]:\n",
    "                m_t=mass(batch.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "                m_f=mass(fake.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "\n",
    "            # if self.current_epoch>pretrain/2:\n",
    "                \n",
    "            pred_real=self.dis_net(batch,None if not self.config[\"mass\"] else m_t)\n",
    "            pred_fake=self.dis_net(fake.detach(),None if not self.config[\"mass\"] else m_f.detach())\n",
    "            if self.wgan:\n",
    "                gradient_penalty =  self.compute_gradient_penalty(self.dis_net, batch, fake.detach(),1)\n",
    "                self.log(\"gradient penalty\",gradient_penalty,logger=True)\n",
    "                d_loss=-torch.mean(pred_real.view(-1))+torch.mean(pred_fake.view(-1))+self.config[\"lambda\"]*gradient_penalty\n",
    "            else:               \n",
    "                target_real=torch.ones_like(pred_real)\n",
    "                target_fake=torch.zeros_like(pred_fake)\n",
    "                pred=torch.vstack((pred_real,pred_fake))\n",
    "                target=torch.vstack((target_real,target_fake))\n",
    "                d_loss=nn.MSELoss()(pred,target).mean()\n",
    "            opt_d.zero_grad()\n",
    "            self.manual_backward(d_loss)\n",
    "            opt_d.step() if self.global_step>10 else opt_d.zero_grad()\n",
    "\n",
    "            self.log(\"dloss\",d_loss,logger=True,prog_bar=True)\n",
    "            if self.global_step==2:\n",
    "                print(\"passed test disc\")\n",
    "            #self.logger.experiment.add_scalars(\"d_losses\",{\"train_disc\":d_loss_avg},global_step=self.global_step)\n",
    "            if self.config[\"sched\"]:\n",
    "                sched_d.step()\n",
    "            \n",
    "            \n",
    "            if (self.current_epoch>self.train_nf and self.global_step%self.freq_d<2) or self.global_step==2 :\n",
    "                opt_g.zero_grad()\n",
    "                if self.config[\"mass\"]:\n",
    "                    m_t=mass(batch.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "                    m_f=mass(fake.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "                pred_fake=self.dis_net(fake,None if not self.config[\"mass\"] else m_f)\n",
    "                target_real=torch.ones_like(pred_fake)\n",
    "                if self.wgan:\n",
    "                    g_loss=-torch.mean(pred_fake.view(-1))\n",
    "                else:\n",
    "                    g_loss=nn.MSELoss()((pred_fake.view(-1)),target_real.view(-1))\n",
    "                self.manual_backward(g_loss)\n",
    "#                 if self.global_step>10:\n",
    "                opt_g.step() if self.global_step>10 else opt_g.zero_grad()\n",
    "#                 else:\n",
    "#                     opt_g.zero_grad()\n",
    "                #self.logger.experiment.add_scalars(\"g_losses\",{\"train_gen\":g_loss},global_step=self.global_step)\n",
    "                self.log(\"g_loss\",g_loss,logger=True,prog_bar=True)\n",
    "                if self.config[\"sched\"]:\n",
    "                    sched_g.step()\n",
    "                if self.global_step==3:\n",
    "                    print(\"passed test gen\")\n",
    "\n",
    "            # Control plot train\n",
    "            if self.current_epoch%config[\"val_check\"]==0 and self.current_epoch>self.train_nf/2 :\n",
    "                fig,ax=plt.subplots()\n",
    "                ax.hist(pred_fake.detach().cpu().numpy(),label=\"fake\",bins=np.linspace(0,1,30) if not self.wgan else 30,histtype='step')\n",
    "                ax.hist(pred_real.detach().cpu().numpy(),label=\"real\",bins=np.linspace(0,1,30) if not self.wgan else 30,histtype='step')\n",
    "                ax.legend()\n",
    "                plt.ylabel(\"Counts\")\n",
    "                plt.xlabel(\"Critic Score\")\n",
    "                self.logger.experiment.add_figure(\"class_train\",fig,global_step=self.current_epoch)  \n",
    "                plt.close()\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''This calculates some important metrics on the hold out set (checking for overtraining)'''\n",
    "\n",
    "        self.dis_net.train()\n",
    "        self.gen_net.train()\n",
    "        self.data_module.scaler.to(\"cpu\")  \n",
    "        batch=batch.to(\"cpu\")\n",
    "        self.flow=self.flow.to(\"cpu\")\n",
    "        self.dis_net=self.dis_net.cpu()\n",
    "        self.gen_net=self.gen_net.cpu()\n",
    "        c=None       \n",
    "\n",
    "        with torch.no_grad():\n",
    "            logprob=-self.flow.log_prob(batch).mean()/90\n",
    "\n",
    "            gen,true,z,fake_scaled,true_scaled,z_scaled=self.sampleandscale(batch,scale=True)\n",
    "#             gen=torch.rand((len(batch),self.n_part,self.n_dim)).cpu()+10\n",
    "            \n",
    "            if self.config[\"mass\"]:\n",
    "                m_t=mass(batch.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "                m_f=mass(gen.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "            scores_fake=self.dis_net(gen,None if not self.config[\"mass\"] else m_f)\n",
    "            scores_real=self.dis_net(true.reshape(len(batch),self.n_part,self.n_dim),None if not self.config[\"mass\"] else m_t)\n",
    "            #scores_nf=self.dis_net(z.reshape(len(batch),self.n_part,self.n_dim))\n",
    "        bins=50\n",
    "        # if not self.wgan:\n",
    "        #     bins=np.linspace(0,1,bins)\n",
    "        #     #scores_nf=nn.Sigmoid()(scores_nf)\n",
    "        #     scores_real=nn.Sigmoid()(scores_real)\n",
    "        #     scores_fake=nn.Sigmoid()(scores_fake)\n",
    "        #     real_loss=nn.BCELoss()(scores_real,torch.ones_like(scores_nf))\n",
    "        #     fake_loss_gan=nn.BCELoss()(scores_fake,torch.zeros_like(scores_nf))\n",
    "        #     #fake_loss_nf=nn.BCELoss()(scores_nf,torch.zeros_like(scores_nf))\n",
    "        #     g_loss=nn.BCELoss()(scores_fake,torch.ones_like(scores_nf))\n",
    "        #     num=2\n",
    "        #     d_loss=(real_loss+fake_loss_gan)/num\n",
    "        # else:\n",
    "        #     d_loss=-torch.mean(scores_real.view(-1))+torch.mean(scores_fake.view(-1))#+10*gradient_penalty\n",
    "        #     g_loss=-torch.mean(scores_fake.view(-1))\n",
    "        \n",
    "        fig=plt.figure()\n",
    "\n",
    "        _,bins,_=plt.hist(scores_real.numpy(),bins=bins,label=\"MC simulated\",alpha=0.5)\n",
    "        plt.hist(scores_fake.numpy(),bins=bins,label=\"ML generated\",alpha=0.5)\n",
    "        plt.xlabel(\"Critic Score\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.legend()\n",
    "        self.logger.experiment.add_figure(\"class_val\",fig,global_step=self.current_epoch)  \n",
    "        plt.close()\n",
    "        true_scaled,fake_scaled,z_scaled=true_scaled.reshape(-1,90),fake_scaled.reshape(-1,90),z_scaled.reshape(-1,90)\n",
    "        # Reverse Standard Scaling (this has nothing to do with flows, it is a standard preprocessing step)\n",
    "        m_t=mass(true_scaled[:,:self.n_dim*self.n_part].to(self.device),self.config[\"canonical\"]).cpu()\n",
    "        m_gen=mass(z_scaled[:,:self.n_dim*self.n_part],self.config[\"canonical\"]).cpu()\n",
    "        m_c=mass(fake_scaled[:,:self.n_dim*self.n_part],self.config[\"canonical\"]).cpu()\n",
    "        for i in range(30):\n",
    "            i=2+3*i\n",
    "            # gen[gen[:,i]<0,i]=0\n",
    "            fake_scaled[fake_scaled[:,i]<0,i]=0\n",
    "            true_scaled[true_scaled[:,i]<0,i]=0\n",
    "        #Some metrics we track\n",
    "        cov,mmd=cov_mmd(fake_scaled.reshape(-1,self.n_part,self.n_dim),true_scaled.reshape(-1,self.n_part,self.n_dim),use_tqdm=False)\n",
    "        try:\n",
    "            fpndv=fpnd(fake_scaled.reshape(-1,self.n_part,self.n_dim).numpy(),use_tqdm=False,jet_type=self.config[\"parton\"])\n",
    "        except:\n",
    "            fpndv=1000\n",
    "        self.metrics[\"val_fpnd\"].append(fpndv)\n",
    "        self.metrics[\"val_logprob\"].append(logprob)\n",
    "        self.metrics[\"val_mmd\"].append(mmd)\n",
    "        self.metrics[\"val_cov\"].append(cov)\n",
    "        self.metrics[\"val_w1p\"].append(w1p(fake_scaled.reshape(len(batch),self.n_part,self.n_dim),true_scaled.reshape(len(batch),self.n_part,self.n_dim)))\n",
    "        w1m_=w1m(fake_scaled.reshape(len(batch),self.n_part,self.n_dim),true_scaled.reshape(len(batch),self.n_part,self.n_dim))\n",
    "        if w1m_[0]>0.01 and self.current_epoch>100 and not self.config[\"sched\"]==\"cosine2\":\n",
    "            print(\"no convergence, stop training\")\n",
    "            raise\n",
    "        self.metrics[\"val_w1m\"].append(w1m_)\n",
    "        self.metrics[\"val_w1efp\"].append(w1efp(fake_scaled.reshape(len(batch),self.n_part,self.n_dim),true_scaled.reshape(len(batch),self.n_part,self.n_dim)))\n",
    "        \n",
    "        \n",
    "        temp={\"val_logprob\":float(logprob.numpy()),\"val_fpnd\":fpndv,\"val_mmd\":mmd,\"val_cov\":cov,\"val_w1m\":self.metrics[\"val_w1m\"][-1][0],\"val_w1efp\":self.metrics[\"val_w1efp\"][-1][0],\"val_w1p\":self.metrics[\"val_w1p\"][-1][0],\"step\":self.global_step}\n",
    "        \n",
    "        print(\"epoch {}: \".format(self.current_epoch),temp)\n",
    "        if self.hyperopt and self.global_step>3:\n",
    "            # self._results(temp)\n",
    "            summary=self._summary(temp)\n",
    "        self.log(\"hp_metric\",self.metrics[\"val_w1m\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1m\",self.metrics[\"val_w1m\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1p\",self.metrics[\"val_w1p\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1efp\",self.metrics[\"val_w1efp\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_logprob\",logprob,prog_bar=True,logger=True)\n",
    "        self.log(\"val_cov\",cov,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_fpnd\",fpndv,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mmd\",mmd,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "\n",
    "        self.plot=plotting(model=self,gen=z_scaled,gen_corr=fake_scaled,true=true_scaled,config=self.config,step=self.global_step,logger=self.logger.experiment)  \n",
    "        try:\n",
    "            self.plot.plot_mass(m=m_gen.cpu().numpy(),m_t=m_t.cpu().numpy(),m_c=m_c.cpu().numpy(),save=True,bins=50,quantile=True,plot_vline=False)\n",
    "            # self.plot.plot_2d(save=True)\n",
    "#             self.plot.var_part(true=true[:,:self.n_dim],gen=gen_corr[:,:self.n_dim],true_n=n_true,gen_n=n_gen_corr,m_true=m_t,m_gen=m_test ,save=True)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc() \n",
    "        self.flow=self.flow.to(\"cuda\")\n",
    "        self.gen_net=self.gen_net.to(\"cuda\")\n",
    "        self.dis_net=self.dis_net.to(\"cuda\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b07ef-08b3-4bbd-ae78-89bd19f30768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/nn/init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/tmp/ipykernel_29750/1940514772.py:233: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(p)\n",
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=True)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /beegfs/desy/user/kaechben/Transflow_final exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | q0        | StandardNormal     | 0     \n",
      "1 | q_test    | StandardNormal     | 0     \n",
      "2 | flows     | CompositeTransform | 6.6 M \n",
      "3 | flow_test | Flow               | 6.6 M \n",
      "4 | flow      | Flow               | 6.6 M \n",
      "5 | gen_net   | Gen                | 95.7 K\n",
      "6 | dis_net   | Disc               | 95.3 K\n",
      "7 | sig       | Sigmoid            | 0     \n",
      "-------------------------------------------------\n",
      "6.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.006    Total estimated model params size (MB)\n",
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd9849fa73a4673b15f6a9cc83e7d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/jetnet/evaluation/gen_metrics.py:239: RuntimeWarning: Recommended number of jets for FPND calculation is 50000\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49:  {'val_logprob': 0.5185331106185913, 'val_fpnd': 32.4072516748474, 'val_mmd': 0.08661059372852516, 'val_cov': 0.257, 'val_w1m': 0.015798835202362388, 'val_w1efp': 0.00027832312137666856, 'val_w1p': 0.023252358801423956, 'step': 3763}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/jetnet/evaluation/gen_metrics.py:239: RuntimeWarning: Recommended number of jets for FPND calculation is 50000\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99:  {'val_logprob': 0.5185331106185913, 'val_fpnd': 19.596721754712007, 'val_mmd': 0.07786583962330493, 'val_cov': 0.391, 'val_w1m': 0.00920265948953107, 'val_w1efp': 0.0001609642511678173, 'val_w1p': 0.010758718508268638, 'step': 6707}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/jetnet/evaluation/gen_metrics.py:239: RuntimeWarning: Recommended number of jets for FPND calculation is 50000\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 149:  {'val_logprob': 0.5185331106185913, 'val_fpnd': 13.972423818117846, 'val_mmd': 0.07885658372865456, 'val_cov': 0.45999999999999996, 'val_w1m': 0.005590532812271268, 'val_w1efp': 0.00015088370942319623, 'val_w1p': 0.014601729460354049, 'step': 9652}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/jetnet/evaluation/gen_metrics.py:239: RuntimeWarning: Recommended number of jets for FPND calculation is 50000\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199:  {'val_logprob': 0.5185331106185913, 'val_fpnd': 11.485946324436668, 'val_mmd': 0.07700029331935007, 'val_cov': 0.522, 'val_w1m': 0.0047123191112093624, 'val_w1efp': 0.0001761489806831363, 'val_w1p': 0.011896537161695303, 'step': 12596}\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "hyperopt = True  # This sets to run a hyperparameter optimization with ray or just running the training once\n",
    "\n",
    "config = {\n",
    "    \"autoreg\":False,\n",
    "    \"context_features\":0,\n",
    "   \"network_layers\": 3,  # sets amount hidden layers in transformation networks -scannable\n",
    "    \"network_layers_nf\": 2,  # sets amount hidden layers in transformation networks -scannable\n",
    "    \"network_nodes_nf\": 256,  # amount nodes in hidden layers in transformation networks -scannable\n",
    "    \"batch_size\": 2000,  # sets batch size -scannable #best one 4000\n",
    "    \"coupling_layers\": 15,  # amount of invertible transformations to use -scannable\n",
    "    \"lr\": 0.001,  # sets learning rate -scannable\n",
    "    \"batchnorm\": False,  # use batchnorm or not -scannable\n",
    "    \"bins\":5,  # amount of bins to use in rational quadratic splines -scannable\n",
    "    \"tail_bound\": 6,  # splines:max value that is transformed, over this value theree is id  -scannable\n",
    "    \"limit\": 150000,  # how many data points to use, test_set is 10% of this -scannable in a sense use 10 k for faster training\n",
    "    \"n_dim\": 3,  # how many dimensions to use or equivalently /3 gives the amount of particles to use NEVER EVER CHANGE THIS\n",
    "    \"dropout\": 0.2,  # use droput proportion, for 0 there is no dropout -scannable\n",
    "    \"canonical\": False,  # transform data coordinates to px,py,pz -scannable\n",
    "    \"max_steps\": 100000,  # how many steps to use at max - lower for quicker training\n",
    "    \"lambda\": 10,  # balance between massloss and nll -scannable\n",
    "    \"name\": \"Transflow_final\",  # name for logging folder\n",
    "    \"disc\": False,  # whether to train gan style discriminator that decides whether point is simulated or generated-semi-scannable\n",
    "    \"variable\":1, #use variable amount of particles otherwise only use 30, options are true or False \n",
    "    \"parton\":\"t\", #choose the dataset you want to train options: t for top,q for quark,g for gluon\n",
    "    \"wgan\":False,\n",
    "    \"corr\":True,\n",
    "    \"num_layers\":5,\n",
    "    \"freq\":10,\n",
    "    \"n_part\":30,\n",
    "    \"fc\":False,\n",
    "    \"hidden\":80,\n",
    "    \"heads\":3,\n",
    "    \"l_dim\":63,\n",
    "    \"lr_g\":1e-4,\n",
    "    \"lr_d\":1e-4,\n",
    "    \"lr_nf\":0.000722,\n",
    "    \"sched\":None,\n",
    "    \"opt\":\"Adam\",\n",
    "    \"lambda\":1,\n",
    "    \"max_epochs\":1600,\n",
    "    \"mass\":True,\n",
    "    \"no_hidden\":True,\n",
    "    \"clf\":True,\n",
    "    \"val_check\":50\n",
    "}     \n",
    "\n",
    "# if len(sys.argv)>2:\n",
    "#     root=\"/beegfs/desy/user/\"+os.environ[\"USER\"]+\"/\"+config[\"name\"]+\"/run\"+sys.argv[1]+\"_\"+str(sys.argv[2])\n",
    "# else:\n",
    "root=\"/beegfs/desy/user/\"+os.environ[\"USER\"]+\"/\"+config[\"name\"]\n",
    "\n",
    "            \n",
    "data_module = JetNetDataloader(config,config[\"batch_size\"]) #this loads the data\n",
    "data_module.setup(\"training\")\n",
    "model = TransGan(config,hyperopt,data_module.num_batches) # the sets up the model,  config are hparams we want to optimize\n",
    "model.data_module=data_module\n",
    "# Callbacks to use during the training, we  checkpoint our models\n",
    "\n",
    "callbacks = [ModelCheckpoint(monitor=\"val_w1m\",save_top_k=2, filename='{epoch}-{val_fpnd:.2f}-{val_w1m:.4f}', dirpath=root,every_n_epochs=10) ]\n",
    "\n",
    "if False:#load_ckpt:\n",
    "    model = TransGan.load_from_checkpoint(\"/beegfs/desy/user/kaechben/Transflow_reloaded2/2022_08_08-18_02-08/epoch=239-val_logprob=0.47-val_w1m=0.0014.ckpt\")\n",
    "    model.data_module=data_module\n",
    "\n",
    "# pl.seed_everything(model.config[\"seed\"], workers=True)\n",
    "# model.config[\"freq\"]=20\n",
    "# model.config[\"lr_g\"]=0.00001\n",
    "# model.config[\"lr_d\"]=0.00001\n",
    "# model.config = config #config are our hyperparams, we make this a class property now\n",
    "logger = TensorBoardLogger(root)\n",
    "#log every n steps could be important as it decides how often it should log to tensorboard\n",
    "# Also check val every n epochs, as validation checking takes some time\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, logger=logger,  log_every_n_steps=5,  # auto_scale_batch_size=\"binsearch\",\n",
    "                      max_epochs=config[\"max_epochs\"], callbacks=callbacks, progress_bar_refresh_rate=True,\n",
    "                      check_val_every_n_epoch=config[\"val_check\"] ,num_sanity_val_steps=0,#gradient_clip_val=.02, gradient_clip_algorithm=\"norm\",\n",
    "                     fast_dev_run=False,default_root_dir=root)\n",
    "# This calls the fit function which trains the model\n",
    "\n",
    "trainer.fit(model,datamodule=data_module )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0962d61-06a4-42ec-9857-7b74c2a738c3",
   "metadata": {},
   "source": [
    "## # STOP\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3b751-73c4-4fbc-8678-550beba87d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4788430-6b1d-4492-91be-c1c20719b400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c939fb6d-c05b-4f32-958b-ff069de5e01a",
   "metadata": {},
   "source": [
    "\n",
    "import traceback\n",
    "import os\n",
    "import nflows as nf\n",
    "from nflows.utils.torchutils import create_random_binary_mask\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.coupling import *\n",
    "from nflows.nn import nets\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.flows import base\n",
    "from nflows.transforms.coupling import *\n",
    "from nflows.transforms.autoregressive import *\n",
    "from particle_net import ParticleNet\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import OneCycleLR,ReduceLROnPlateau,ExponentialLR\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as FF\n",
    "import numpy as np\n",
    "from jetnet.evaluation import w1p, w1efp, w1m, cov_mmd,fpnd\n",
    "import mplhep as hep\n",
    "import hist\n",
    "from hist import Hist\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from collections import OrderedDict\n",
    "from ray import tune\n",
    "from helpers import *\n",
    "from plotting import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from helpers import CosineWarmupScheduler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.nn.functional import leaky_relu,sigmoid\n",
    "class Gen(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_dim=3,l_dim=10,hidden=300,num_layers=3,num_heads=1,n_part=5,fc=False,dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes=hidden\n",
    "        self.n_dim=n_dim\n",
    "        self.l_dim=l_dim\n",
    "        self.n_part=n_part\n",
    "        \n",
    "       \n",
    "        self.fc=fc\n",
    "        if fc:\n",
    "            self.l_dim*=n_part \n",
    "            self.embbed_flat=nn.Linear(n_dim*n_part,l_dim)\n",
    "            self.flat_hidden=nn.Linear(l_dim,hidden)\n",
    "            self.flat_hidden2=nn.Linear(hidden,hidden)\n",
    "            self.flat_hidden3=nn.Linear(hidden,hidden)\n",
    "            self.flat_out=nn.Linear(hidden,n_dim*n_part)\n",
    "        else:\n",
    "            self.embbed=nn.Linear(n_dim,l_dim)\n",
    "            self.encoder=nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "            d_model=l_dim,nhead=num_heads,batch_first=True,norm_first=False\n",
    "            ,dim_feedforward=hidden,dropout=dropout) ,num_layers=num_layers)\n",
    "            self.hidden=nn.Linear(l_dim,hidden)\n",
    "            self.hidden2=nn.Linear(hidden,hidden)\n",
    "            self.dropout=nn.Dropout(dropout/2)\n",
    "            self.out=nn.Linear(hidden, n_dim)\n",
    "            self.out_flat=nn.Linear(hidden,n_dim*n_part )\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        if self.fc:\n",
    "            x=x.reshape(len(x),self.n_part*self.n_dim)\n",
    "            x=self.embbed_flat(x)\n",
    "            x=leaky_relu(self.flat_hidden(x))\n",
    "#             x = self.dropout(x)\n",
    "            x=self.flat_out(x)\n",
    "            x=x.reshape(len(x),self.n_part,self.n_dim)\n",
    "        else:\n",
    "            x=self.embbed(x)\n",
    "            x=self.encoder(x)\n",
    "            x=leaky_relu(self.hidden(x))\n",
    "            x=self.dropout(x)\n",
    "            x=leaky_relu(self.hidden2(x))\n",
    "            x=self.dropout(x)\n",
    "            \n",
    "            x=self.out(x)\n",
    "        return x\n",
    "\n",
    "class Disc(nn.Module):\n",
    "    def __init__(self,n_dim=3,l_dim=10,hidden=300,num_layers=3,num_heads=1,n_part=2,fc=False,dropout=0.5,mass=False):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes=hidden\n",
    "        self.n_dim=n_dim\n",
    "#         l_dim=n_dim\n",
    "        self.l_dim=l_dim\n",
    "        self.n_part=n_part\n",
    "        self.fc=fc\n",
    "\n",
    "        \n",
    "        if fc:\n",
    "            self.l_dim*=n_part \n",
    "            self.embbed_flat=nn.Linear(n_dim*n_part,l_dim)\n",
    "            self.flat_hidden=nn.Linear(l_dim,hidden)\n",
    "            self.flat_hidden2=nn.Linear(hidden,hidden)\n",
    "            self.flat_hidden3=nn.Linear(hidden,hidden)\n",
    "            self.flat_out=nn.Linear(hidden,1)\n",
    "        else:\n",
    "            self.embbed=nn.Linear(n_dim,l_dim)\n",
    "            self.encoder=nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.l_dim,nhead=num_heads,dim_feedforward=hidden,dropout=dropout,norm_first=False,\n",
    "                                       activation=lambda x:leaky_relu(x,0.2),batch_first=True) ,num_layers=num_layers)\n",
    "            self.hidden=nn.Linear(l_dim+int(mass),2*hidden)\n",
    "            self.hidden2=nn.Linear(2*hidden,hidden)\n",
    "            self.out=nn.Linear(hidden,1)\n",
    "\n",
    "    def forward(self,x,m=None):\n",
    "\n",
    "        if self.fc==True:\n",
    "            x=x.reshape(len(x),self.n_dim*self.n_part)\n",
    "            x=self.embbed_flat(x)\n",
    "            x=leaky_relu(self.flat_hidden(x),0.2)\n",
    "            x=leaky_relu(self.flat_hidden2(x),0.2)\n",
    "            x=self.flat_out(x)\n",
    "        else:\n",
    "            x=self.embbed(x)\n",
    "            x=torch.concat((torch.ones_like(x[:,0,:]).reshape(len(x),1,-1),x),axis=1)\n",
    "            \n",
    "            x=self.encoder(x)\n",
    "            \n",
    "#             x=torch.sum(x,axis=1)\n",
    "            x=x[:,0,:]\n",
    "            if m is not None:\n",
    "                x=torch.concat((m.reshape(len(x),1),x),axis=1)\n",
    "\n",
    "            x=leaky_relu(self.hidden(x),0.2)\n",
    "            \n",
    "            x=leaky_relu(self.hidden2(x),0.2)\n",
    "            \n",
    "            x=self.out(x)\n",
    "        return x\n",
    "      \n",
    "class TransGan(pl.LightningModule):\n",
    "    def create_resnet(self,in_features, out_features):\n",
    "        '''This is the network that outputs the parameters of the invertible transformation\n",
    "        The only arguments can be the in dimension and the out dimenson, the structure\n",
    "        of the network is defined over the config which is a class attribute\n",
    "        Context Features: Amount of features used to condition the flow - in our case \n",
    "        this is usually the mass\n",
    "        num_blocks: How many Resnet blocks should be used, one res net block is are 1 input+ 2 layers\n",
    "        and an additive skip connection from the first to the third'''\n",
    "        c=self.config[\"context_features\"]\n",
    "        return nets.ResidualNet(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                hidden_features=self.config[\"network_nodes_nf\"],\n",
    "                context_features=c,\n",
    "                num_blocks=self.config[\"network_layers_nf\"],\n",
    "                activation=self.config[\"activation\"]  if \"activation\" in self.config.keys() else FF.relu,\n",
    "                #dropout_probability=self.config[\"dropout\"] if \"dropout\" in self.config.keys() else 0,\n",
    "                use_batch_norm=self.config[\"batchnorm\"] if \"batchnorm\" in self.config.keys() else 0\n",
    "        )\n",
    "\n",
    "    def __init__(self,config,hyperopt,):\n",
    "        '''This initializes the model and its hyperparameters'''\n",
    "        super().__init__()\n",
    "        self.hyperopt=True\n",
    "        \n",
    "        self.start=time.time()\n",
    "        # self.batch_size=batch_size\n",
    "        # print(batch_size)\n",
    "        self.config=config\n",
    "        self.automatic_optimization=False\n",
    "        self.freq_d=config[\"freq\"]\n",
    "        self.wgan=config[\"wgan\"]\n",
    "        #Metrics to track during the training\n",
    "        self.metrics={\"val_w1p\":[],\"val_w1m\":[],\"val_w1efp\":[],\"val_cov\":[],\"val_mmd\":[],\"val_fpnd\":[],\"val_logprob\":[],\"step\":[]}\n",
    "        #Loss function of the Normalizing flows\n",
    "        self.logprobs=[]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        # self.hparams.update(config)\n",
    "        self.save_hyperparameters()\n",
    "        self.flows = []\n",
    "        self.n_dim=self.config[\"n_dim\"]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        self.add_corr=config[\"corr\"]\n",
    "        self.alpha=1\n",
    "        K=self.config[\"coupling_layers\"]\n",
    "        for i in range(K):\n",
    "            '''This creates the masks for the coupling layers, particle masks are masks\n",
    "            created such that each feature particle (eta,phi,pt) is masked together or not'''\n",
    "           \n",
    "            if self.config[\"autoreg\"]:\n",
    "                self.flows += [MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "#                         random_mask=True,\n",
    "                        features=self.n_dim,\n",
    "                        hidden_features=128,\n",
    "                        use_residual_blocks=True, \n",
    "                        tails='linear',\n",
    "                        tail_bound=self.config[\"tail_bound\"],\n",
    "                        num_bins=self.config[\"bins\"] )]\n",
    "            else:\n",
    "                mask=create_random_binary_mask(self.n_dim*self.n_part)            \n",
    "                self.flows += [PiecewiseRationalQuadraticCouplingTransform(\n",
    "                            mask=mask,\n",
    "                            transform_net_create_fn= self.create_resnet, \n",
    "                            tails='linear',\n",
    "                            tail_bound=self.config[\"tail_bound\"],\n",
    "                            num_bins=self.config[\"bins\"] )]\n",
    "\n",
    "        self.q0 = nf.distributions.normal.StandardNormal([self.n_dim*self.n_part])\n",
    "        self.q_test =nf.distributions.normal.StandardNormal([self.n_dim*self.n_part])\n",
    "        #Creates working flow model from the list of layer modules\n",
    "        self.flows=CompositeTransform(self.flows)\n",
    "        # Construct flow model\n",
    "        self.flow_test= base.Flow(distribution=self.q_test, transform=self.flows)\n",
    "        self.flow = base.Flow(distribution=self.q0, transform=self.flows)\n",
    "        \n",
    "\n",
    "        self.gen_net = Gen(n_dim=self.n_dim,hidden=config[\"hidden\"],num_layers=config[\"num_layers\"],dropout=config[\"dropout\"],\n",
    "                           fc= config[\"fc\"],n_part=config[\"n_part\"],l_dim=config[\"l_dim\"],num_heads=config[\"heads\"]).cuda()\n",
    "        self.dis_net = Disc(n_dim=self.n_dim,hidden=config[\"hidden\"],l_dim=config[\"l_dim\"],num_layers=config[\"num_layers\"],mass=self.config[\"mass\"],\n",
    "                            num_heads=config[\"heads\"],fc=config[\"fc\"],n_part=config[\"n_part\"],dropout=config[\"dropout\"]).cuda()\n",
    "        self.sig=nn.Sigmoid()\n",
    "#         self.flow.load_state_dict(torch.load(\"/beegfs/desy/user/kaechben/pretrained_flow.pt\"))\n",
    "        for p in self.dis_net.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal(p)\n",
    "        self.d_train=True\n",
    "    \n",
    "\n",
    "    def load_datamodule(self,data_module):\n",
    "        '''needed for lightning training to work, it just sets the dataloader for training and validation'''\n",
    "        self.data_module=data_module\n",
    "        \n",
    "    def on_after_backward(self) -> None:\n",
    "        '''This is a genious little hook, sometimes my model dies, i have no clue why. This saves the training from crashing and continues'''\n",
    "        valid_gradients = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                valid_gradients = not (torch.isnan(param.grad).any() or torch.isinf(param.grad).any())\n",
    "                if not valid_gradients:\n",
    "                    break\n",
    "        if not valid_gradients:\n",
    "            print(\"not valid grads\",self.counter)\n",
    "            self.zero_grad()\n",
    "            self.counter+=1\n",
    "            if self.counter>5:\n",
    "                raise ValueError('5 nangrads in a row')\n",
    "        else:\n",
    "            self.counter=0\n",
    "\n",
    "    def sampleandscale(self,batch,scale=False):\n",
    "        '''This is a helper function that samples from the flow (i.e. generates a new sample) \n",
    "            and reverses the standard scaling that is done in the preprocessing. This allows to calculate the mass\n",
    "            on the generative sample and to compare to the simulated one, we need to inverse the scaling before calculating the mass\n",
    "            because calculating the mass is a non linear transformation and does not commute with the mass calculation''' \n",
    "        z=self.flow.sample(len(batch)).reshape(len(batch),self.n_part,self.n_dim)\n",
    "        if self.add_corr: \n",
    "            fake=z+self.gen_net(z)#(1-self.alpha)*\n",
    "            fake=fake.reshape(len(batch),self.n_part,self.n_dim)\n",
    "        else:\n",
    "            fake=self.gen_net(z)\n",
    "        assert batch.device==fake.device\n",
    "\n",
    "        if scale:\n",
    "            self.data_module.scaler=self.data_module.scaler.to(batch.device)\n",
    "\n",
    "            fake_scaled=self.data_module.scaler.inverse_transform(fake.reshape(len(batch),self.n_dim*self.n_part))\n",
    "            z_scaled=self.data_module.scaler.inverse_transform(z.reshape(len(batch),self.n_dim*self.n_part))\n",
    "            true=self.data_module.scaler.inverse_transform(batch)\n",
    "            return fake,batch,z,fake_scaled,true,z_scaled\n",
    "        else:\n",
    "            return fake\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        self.losses=[]\n",
    "        \n",
    "        #mlosses are initialized with None during the time it is not turned on, makes it easier to plot\n",
    "        if self.config[\"opt\"]==\"Adam\":\n",
    "\n",
    "            opt_g = torch.optim.Adam(self.gen_net.parameters(), lr=self.config[\"lr_g\"],betas=(0,.9))\n",
    "            opt_d = torch.optim.Adam(self.dis_net.parameters(), lr=self.config[\"lr_d\"],betas=(0,.9))\n",
    "        elif self.config[\"opt\"]==\"AdamW\":\n",
    "            opt_g = torch.optim.Adam(self.gen_net.parameters(), lr=self.config[\"lr_g\"],betas=(0,.9))\n",
    "            opt_d = torch.optim.AdamW(self.dis_net.parameters(), lr=self.config[\"lr_d\"],betas=(0,.9))\n",
    "        else:\n",
    "            opt_g = torch.optim.RMSprop(self.gen_net.parameters(), lr=self.config[\"lr_g\"])\n",
    "            opt_d = torch.optim.RMSprop(self.dis_net.parameters(), lr=self.config[\"lr_d\"])\n",
    "        opt_nf = torch.optim.AdamW(self.flow.parameters(), lr=self.config[\"lr_nf\"] )\n",
    "        lr_scheduler_nf =None if not self.config[\"sched\"] else  CosineWarmupScheduler(opt_nf,warmup=1,max_iters=10000000*self.config[\"freq\"]) \n",
    "        factor=100000//self.config[\"batch_size\"]\n",
    "        lr_scheduler_d =None if not self.config[\"sched\"] else CosineWarmupScheduler(opt_d,warmup=30*factor,max_iters=self.config[\"max_epochs\"]*factor)\n",
    "        lr_scheduler_g =None if not self.config[\"sched\"] else CosineWarmupScheduler(opt_g,warmup=30*factor,max_iters=int(self.config[\"max_epochs\"]*factor/self.config[\"freq\"]))\n",
    "        if self.config[\"sched\"]:\n",
    "            return  [opt_nf,opt_d,opt_g],[lr_scheduler_nf,lr_scheduler_d,lr_scheduler_g]\n",
    "        else:\n",
    "            return [opt_nf,opt_d,opt_g] \n",
    "    \n",
    "    \n",
    "    def compute_gradient_penalty(self,D, real_samples, fake_samples, phi):\n",
    "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "        alpha = torch.Tensor(np.random.random((real_samples.size(0),1, 1))).to(real_samples.device)\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
    "        if self.config[\"mass\"]:\n",
    "            m=mass(interpolates.reshape(len(real_samples),self.n_part*self.n_dim).detach())\n",
    "            d_interpolates = D.train()(interpolates.requires_grad_(True),m.requires_grad_(True) )\n",
    "        else:\n",
    "            d_interpolates = D.train()(interpolates.requires_grad_(True))\n",
    "        fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(real_samples.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "\n",
    "    def _summary(self,temp):\n",
    "        self.summary_path=\"/beegfs/desy/user/{}/{}/summary.csv\".format(os.environ[\"USER\"],self.config[\"name\"])\n",
    "        if os.path.isfile(self.summary_path):\n",
    "            \n",
    "            summary=pd.read_csv(self.summary_path).set_index([\"path_index\"])\n",
    "        else:\n",
    "            print(\"summary not found\")\n",
    "            summary=pd.DataFrame()\n",
    "\n",
    "            \n",
    "        summary.loc[self.logger.log_dir,self.config.keys()]=self.config.values()\n",
    "        summary.loc[self.logger.log_dir,temp.keys()]=temp.values()\n",
    "        summary.loc[self.logger.log_dir,\"time\"]=time.time()-self.start          \n",
    "        summary.to_csv(self.summary_path,index_label=[\"path_index\"])  \n",
    "        return summary\n",
    "    \n",
    "    # def _results(self,temp):\n",
    "    #     self.metrics[\"step\"].append(self.global_step)\n",
    "    #     self.df=pd.DataFrame.from_dict(temp,index=)\n",
    "    #     self.df.to_csv(self.logger.log_dir+\"result.csv\",index_label=[\"index\"])\n",
    "    \n",
    "   \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"training loop of the model, here all the data is passed forward to a gaussian\n",
    "            This is the important part what is happening here. This is all the training we do \"\"\"\n",
    "        \n",
    "        opt_nf,opt_d,opt_g=self.optimizers()\n",
    "        if self.config[\"sched\"]:\n",
    "            sched_nf,sched_d,sched_g=self.lr_schedulers()\n",
    "\n",
    "        pretrain=self.config[\"pretrain\"]\n",
    "        nf_loss=0\n",
    "        d_loss_avg=0\n",
    "        gradient_penalty=0\n",
    "\n",
    "        ### NF PART\n",
    "        if self.config[\"sched\"]:\n",
    "            self.log(\"lr_g\",sched_g.get_lr()[-1],logger=True)\n",
    "            self.log(\"lr_nf\",sched_nf.get_lr()[-1],logger=True)\n",
    "            self.log(\"lr_d\",sched_d.get_lr()[-1],logger=True)\n",
    "            sched_nf.step()\n",
    "        # if self.current_epoch<15:\n",
    "        #     nf_loss -=self.flow.to(self.device).log_prob(batch).mean()#c if self.config[\"context_features\"] else None\n",
    "        #     nf_loss/=(self.n_dim*self.n_part) \n",
    "        #     opt_nf.zero_grad()\n",
    "        #     self.manual_backward(nf_loss)\n",
    "        #     opt_nf.step()\n",
    "        #     self.log(\"logprob\", nf_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True) \n",
    "        # if self.current_epoch==15:\n",
    "        #     torch.save(self.flow.state_dict(),\"/beegfs/desy/user/kaechben/pretrained_flow.pt\")\n",
    "        ### GAN PART\n",
    "        \n",
    "        batch=batch.reshape(len(batch),self.n_part,self.n_dim)\n",
    "        fake=self.sampleandscale(batch,scale=False)\n",
    "        if self.config[\"mass\"]:\n",
    "            m_t=mass(batch.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "            m_f=mass(fake.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "\n",
    "        # if self.current_epoch>pretrain/2:\n",
    "            \n",
    "        pred_real=self.dis_net(batch,None if not self.config[\"mass\"] else m_t)\n",
    "        pred_fake=self.dis_net(fake.detach(),None if not self.config[\"mass\"] else m_f.detach())\n",
    "        if self.wgan:\n",
    "            gradient_penalty =  self.compute_gradient_penalty(self.dis_net, batch, fake.detach(),1)\n",
    "            self.log(\"gradient penalty\",gradient_penalty,logger=True)\n",
    "            d_loss=-torch.mean(pred_real.view(-1))+torch.mean(pred_fake.view(-1))+self.config[\"lambda\"]*gradient_penalty\n",
    "        else:               \n",
    "            target_real=torch.ones_like(pred_real)\n",
    "            target_fake=torch.zeros_like(pred_fake)\n",
    "            pred=torch.vstack((pred_real,pred_fake))\n",
    "            target=torch.vstack((target_real,target_fake))\n",
    "            d_loss=nn.MSELoss()(pred,target).mean()\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "        d_loss_avg+=d_loss.cpu().detach().numpy()-10*gradient_penalty.cpu().detach().numpy()\n",
    "        self.log(\"d_loss\",d_loss,logger=True,prog_bar=True)\n",
    "        #self.logger.experiment.add_scalars(\"d_losses\",{\"train_disc\":d_loss_avg},global_step=self.global_step)\n",
    "        if self.config[\"sched\"]:\n",
    "            sched_d.step()\n",
    "        if self.current_epoch>(pretrain/2) and self.global_step%self.freq_d<2 :\n",
    "            opt_g.zero_grad()\n",
    "            pred_fake=self.dis_net(fake,None if not self.config[\"mass\"] else m_f)\n",
    "            target_real=torch.ones_like(pred_fake)\n",
    "            if self.wgan:\n",
    "                g_loss=-torch.mean(pred_fake.view(-1))\n",
    "            else:\n",
    "                g_loss=nn.MSELoss()((pred_fake.view(-1)),target_real.view(-1))\n",
    "            self.manual_backward(g_loss)\n",
    "            opt_g.step()\n",
    "            #self.logger.experiment.add_scalars(\"g_losses\",{\"train_gen\":g_loss},global_step=self.global_step)\n",
    "            self.log(\"g_loss\",g_loss,logger=True,prog_bar=True)\n",
    "            if self.config[\"sched\"]:\n",
    "                sched_g.step()\n",
    "\n",
    "        # Control plot train\n",
    "        if self.current_epoch%5==0 and self.current_epoch>pretrain/2 :\n",
    "            fig,ax=plt.subplots()\n",
    "            ax.hist(pred_fake.detach().cpu().numpy(),label=\"fake\",bins=np.linspace(0,1,30) if not self.wgan else 30,histtype='step')\n",
    "            ax.hist(pred_real.detach().cpu().numpy(),label=\"real\",bins=np.linspace(0,1,30) if not self.wgan else 30,histtype='step')\n",
    "            ax.legend()\n",
    "            plt.ylabel(\"Counts\")\n",
    "            plt.xlabel(\"Critic Score\")\n",
    "            self.logger.experiment.add_figure(\"class_train\",fig,global_step=self.current_epoch)            \n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''This calculates some important metrics on the hold out set (checking for overtraining)'''\n",
    "\n",
    "\n",
    "        self.data_module.scaler.to(\"cpu\")  \n",
    "        batch=batch.to(\"cpu\")\n",
    "        self.flow=self.flow.to(\"cpu\")\n",
    "        self.dis_net=self.dis_net.cpu()\n",
    "        self.gen_net=self.gen_net.cpu()\n",
    "        c=None       \n",
    "        print(self.config)\n",
    "        with torch.no_grad():\n",
    "            logprob=-self.flow.log_prob(batch).mean()/90\n",
    "\n",
    "            gen,true,z,fake_scaled,true_scaled,z_scaled=self.sampleandscale(batch,scale=True)\n",
    "            print(\"mass\", self.config[\"mass\"])\n",
    "            if self.config[\"mass\"]:\n",
    "                m_t=mass(batch.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "                m_f=mass(gen.reshape(len(batch),self.n_part*self.n_dim),self.config[\"canonical\"])\n",
    "            scores_fake=self.dis_net(gen,None if not self.config[\"mass\"] else m_f)\n",
    "            scores_real=self.dis_net(true.reshape(len(batch),self.n_part,self.n_dim),None if not self.config[\"mass\"] else m_t)\n",
    "            #scores_nf=self.dis_net(z.reshape(len(batch),self.n_part,self.n_dim))\n",
    "        bins=50\n",
    "        if not self.wgan:\n",
    "            bins=np.linspace(0,1,bins)\n",
    "            #scores_nf=nn.Sigmoid()(scores_nf)\n",
    "            scores_real=nn.Sigmoid()(scores_real)\n",
    "            scores_fake=nn.Sigmoid()(scores_fake)\n",
    "            real_loss=nn.BCELoss()(scores_real,torch.ones_like(scores_nf))\n",
    "            fake_loss_gan=nn.BCELoss()(scores_fake,torch.zeros_like(scores_nf))\n",
    "            #fake_loss_nf=nn.BCELoss()(scores_nf,torch.zeros_like(scores_nf))\n",
    "            g_loss=nn.BCELoss()(scores_fake,torch.ones_like(scores_nf))\n",
    "            num=2\n",
    "            d_loss=(real_loss+fake_loss_gan)/num\n",
    "        else:\n",
    "            d_loss=-torch.mean(scores_real.view(-1))+torch.mean(scores_fake.view(-1))#+10*gradient_penalty\n",
    "            g_loss=-torch.mean(scores_fake.view(-1))\n",
    "        \n",
    "        fig=plt.figure()\n",
    "\n",
    "        _,bins,_=plt.hist(scores_real.numpy(),bins=bins,label=\"MC simulated\",alpha=0.5)\n",
    "        plt.hist(scores_fake.numpy(),bins=bins,label=\"ML generated\",alpha=0.5)\n",
    "        plt.xlabel(\"Critic Score\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.legend()\n",
    "        true_scaled,fake_scaled,z_scaled=true_scaled.reshape(-1,90),fake_scaled.reshape(-1,90),z_scaled.reshape(-1,90)\n",
    "        # Reverse Standard Scaling (this has nothing to do with flows, it is a standard preprocessing step)\n",
    "        m_t=mass(true_scaled[:,:self.n_dim*self.n_part].to(self.device),self.config[\"canonical\"]).cpu()\n",
    "        m_gen=mass(z_scaled[:,:self.n_dim*self.n_part],self.config[\"canonical\"]).cpu()\n",
    "        m_c=mass(fake_scaled[:,:self.n_dim*self.n_part],self.config[\"canonical\"]).cpu()\n",
    "        for i in range(30):\n",
    "            i=2+3*i\n",
    "            # gen[gen[:,i]<0,i]=0\n",
    "            fake_scaled[fake_scaled[:,i]<0,i]=0\n",
    "            true_scaled[true_scaled[:,i]<0,i]=0\n",
    "        #Some metrics we track\n",
    "        cov,mmd=cov_mmd(fake_scaled.reshape(-1,self.n_part,self.n_dim),true_scaled.reshape(-1,self.n_part,self.n_dim),use_tqdm=False)\n",
    "        try:\n",
    "            fpndv=fpnd(fake_scaled.reshape(-1,self.n_part,self.n_dim).numpy(),use_tqdm=False,jet_type=self.config[\"parton\"])\n",
    "        except:\n",
    "            fpndv=1000\n",
    "        self.metrics[\"val_fpnd\"].append(fpndv)\n",
    "        self.metrics[\"val_logprob\"].append(logprob)\n",
    "        self.metrics[\"val_mmd\"].append(mmd)\n",
    "        self.metrics[\"val_cov\"].append(cov)\n",
    "        self.metrics[\"val_w1p\"].append(w1p(fake_scaled.reshape(len(batch),self.n_part,self.n_dim),true_scaled.reshape(len(batch),self.n_part,self.n_dim)))\n",
    "        w1m_=w1m(fake_scaled.reshape(len(batch),self.n_part,self.n_dim),true_scaled.reshape(len(batch),self.n_part,self.n_dim))\n",
    "        if w1m_[0]>0.02 and self.current_epoch>100:\n",
    "            print(\"no convergence, stop training\")\n",
    "            raise\n",
    "        self.metrics[\"val_w1m\"].append(w1m_)\n",
    "        self.metrics[\"val_w1efp\"].append(w1efp(fake_scaled.reshape(len(batch),self.n_part,self.n_dim),true_scaled.reshape(len(batch),self.n_part,self.n_dim)))\n",
    "        \n",
    "        \n",
    "        temp={\"val_logprob\":float(logprob.numpy()),\"val_fpnd\":fpndv,\"val_mmd\":mmd,\"val_cov\":cov,\"val_w1m\":self.metrics[\"val_w1m\"][-1][0],\"val_w1efp\":self.metrics[\"val_w1efp\"][-1][0],\"val_w1p\":self.metrics[\"val_w1p\"][-1][0],\"step\":self.global_step}\n",
    "        \n",
    "        print(\"step {}: \".format(self.global_step),temp)\n",
    "        if self.hyperopt and self.global_step>3:\n",
    "            # self._results(temp)\n",
    "            summary=self._summary(temp)\n",
    "        self.log(\"hp_metric\",self.metrics[\"val_w1m\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1m\",self.metrics[\"val_w1m\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1p\",self.metrics[\"val_w1p\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1efp\",self.metrics[\"val_w1efp\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_logprob\",logprob,prog_bar=True,logger=True)\n",
    "        self.log(\"val_cov\",cov,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_fpnd\",fpndv,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mmd\",mmd,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "\n",
    "        self.plot=plotting(model=self,gen=z_scaled,gen_corr=fake_scaled,true=true_scaled,config=self.config,step=self.global_step,logger=self.logger.experiment)  \n",
    "        try:\n",
    "            self.plot.plot_mass(m=m_gen.cpu().numpy(),m_t=m_t.cpu().numpy(),m_c=m_c.cpu().numpy(),save=True,bins=15,quantile=True,plot_vline=False)\n",
    "            self.plot.plot_2d(save=True)\n",
    "#             self.plot.var_part(true=true[:,:self.n_dim],gen=gen_corr[:,:self.n_dim],true_n=n_true,gen_n=n_gen_corr,m_true=m_t,m_gen=m_test ,save=True)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc() \n",
    "        self.flow=self.flow.to(\"cuda\")\n",
    "        self.gen_net=self.gen_net.to(\"cuda\")\n",
    "        self.dis_net=self.dis_net.to(\"cuda\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52a41622-19c2-4c9e-b644-d21f2c809087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transflow_reloaded2\n",
      "{'autoreg': False, 'context_features': 0, 'network_layers': 3, 'network_layers_nf': 2, 'network_nodes_nf': 256, 'batch_size': 1024, 'coupling_layers': 10, 'lr': 0.001, 'batchnorm': False, 'bins': 5, 'tail_bound': 6, 'limit': 150000, 'n_dim': 3, 'dropout': 0.2, 'canonical': False, 'max_steps': 100000, 'lambda': 1, 'name': 'Transflow_reloaded2', 'disc': False, 'variable': 1, 'parton': 't', 'wgan': True, 'corr': True, 'num_layers': 5, 'freq': 10, 'n_part': 30, 'fc': False, 'hidden': 16, 'heads': 3, 'l_dim': 63, 'lr_g': 0.0001, 'lr_d': 0.0001, 'lr_nf': 0.000722, 'sched': False, 'pretrain': 30, 'opt': 'RMSprop', 'max_epochs': 300, 'mass': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16679/2645246114.py:225: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(p)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import ray\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import CometLogger, TensorBoardLogger\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.integration.pytorch_lightning import (\n",
    "    TuneReportCallback, TuneReportCheckpointCallback)\n",
    "from scipy import stats\n",
    "from torch.nn import functional as FF\n",
    "\n",
    "from helpers import *\n",
    "from jetnet_dataloader import JetNetDataloader\n",
    "\n",
    "from plotting import plotting\n",
    "\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "hyperopt = True  # This sets to run a hyperparameter optimization with ray or just running the training once\n",
    "\n",
    "config = {\n",
    "    \"autoreg\":False,\n",
    "    \"context_features\":0,\n",
    "   \"network_layers\": 3,  # sets amount hidden layers in transformation networks -scannable\n",
    "    \"network_layers_nf\": 2,  # sets amount hidden layers in transformation networks -scannable\n",
    "    \"network_nodes_nf\": 256,  # amount nodes in hidden layers in transformation networks -scannable\n",
    "    \"batch_size\": 1024,  # sets batch size -scannable #best one 4000\n",
    "    \"coupling_layers\": 10,  # amount of invertible transformations to use -scannable\n",
    "    \"lr\": 0.001,  # sets learning rate -scannable\n",
    "    \"batchnorm\": False,  # use batchnorm or not -scannable\n",
    "    \"bins\":5,  # amount of bins to use in rational quadratic splines -scannable\n",
    "    \"tail_bound\": 6,  # splines:max value that is transformed, over this value theree is id  -scannable\n",
    "    \"limit\": 150000,  # how many data points to use, test_set is 10% of this -scannable in a sense use 10 k for faster training\n",
    "    \"n_dim\": 3,  # how many dimensions to use or equivalently /3 gives the amount of particles to use NEVER EVER CHANGE THIS\n",
    "    \"dropout\": 0.2,  # use droput proportion, for 0 there is no dropout -scannable\n",
    "    \"canonical\": False,  # transform data coordinates to px,py,pz -scannable\n",
    "    \"max_steps\": 100000,  # how many steps to use at max - lower for quicker training\n",
    "    \"lambda\": 10,  # balance between massloss and nll -scannable\n",
    "    \"name\": \"Transflow_reloaded2\",  # name for logging folder\n",
    "    \"disc\": False,  # whether to train gan style discriminator that decides whether point is simulated or generated-semi-scannable\n",
    "    \"variable\":1, #use variable amount of particles otherwise only use 30, options are true or False \n",
    "    \"parton\":\"t\", #choose the dataset you want to train options: t for top,q for quark,g for gluon\n",
    "    \"wgan\":True,\n",
    "    \"corr\":True,\n",
    "    \"num_layers\":5,\n",
    "    \"freq\":10,\n",
    "    \"n_part\":30,\n",
    "    \"fc\":False,\n",
    "    \"hidden\":16,\n",
    "    \"heads\":3,\n",
    "    \"l_dim\":63,\n",
    "    \"lr_g\":1e-4,\n",
    "    \"lr_d\":1e-4,\n",
    "    \"lr_nf\":0.000722,\n",
    "    \"sched\":False,\n",
    "    \"pretrain\":30,\n",
    "    \"opt\":\"RMSprop\",\n",
    "    \"lambda\":1,\n",
    "    \"max_epochs\":300,\n",
    "    \"mass\":True\n",
    "\n",
    "}     \n",
    "print(config[\"name\"])\n",
    "root=\"/beegfs/desy/user/\"+os.environ[\"USER\"]+\"/\"+config[\"name\"]+\"/\"+datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M-%S\")\n",
    "\n",
    "hyperopt=True\n",
    "\n",
    "# This function is a wrapper for the hyperparameter optimization module called ray \n",
    "# Its parameters hyperopt and load_ckpt are there for convenience\n",
    "# Config is the only relevant parameter as it sets the trainings hyperparameters\n",
    "# hyperopt:whether to optimizer hyper parameters - load_ckpt: path to checkpoint if used\n",
    "data_module = JetNetDataloader(config,config[\"batch_size\"]) #this loads the data\n",
    "\n",
    "model = TransGan(config,hyperopt) # the sets up the model,  config are hparams we want to optimize\n",
    "model.data_module=data_module\n",
    "# Callbacks to use during the training, we  checkpoint our models\n",
    "\n",
    "callbacks = [ModelCheckpoint(monitor=\"val_w1m\",save_top_k=2, filename='{epoch}-{val_fpnd:.2f}-{val_w1m:.4f}', dirpath=root,every_n_epochs=10) ]\n",
    "print(model.config)\n",
    "\n",
    "# if True:#load_ckpt:\n",
    "#     model = TransGan.load_from_checkpoint(\"/beegfs/desy/user/kaechben/Transflow_reloaded2/2022_08_08-18_02-08/epoch=239-val_logprob=0.47-val_w1m=0.0014.ckpt\")\n",
    "#     model.data_module=data_module\n",
    "#     print(\"loaded mass\",model.config[\"mass\"])\n",
    "\n",
    "# model.load_datamodule(data_module)\n",
    "\n",
    "# pl.seed_everything(model.config[\"seed\"], workers=True)\n",
    "\n",
    "\n",
    "# logger = TensorBoardLogger(root)\n",
    "# #log every n steps could be important as it decides how often it should log to tensorboard\n",
    "# # Also check val every n epochs, as validation checking takes some time\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1, logger=logger,  log_every_n_steps=100,  # auto_scale_batch_size=\"binsearch\",\n",
    "#                       max_epochs=config[\"max_epochs\"], callbacks=callbacks, progress_bar_refresh_rate=int(not hyperopt)*10,\n",
    "#                       check_val_every_n_epoch=5 ,num_sanity_val_steps=1,#gradient_clip_val=.02, gradient_clip_algorithm=\"norm\",\n",
    "#                      fast_dev_run=False,default_root_dir=root)\n",
    "# # This calls the fit function which trains the model\n",
    "\n",
    "# trainer.fit(model,datamodule=data_module )  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8335de33-288a-4393-b178-b947032c093f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Flow:\n\tUnexpected key(s) in state_dict: \"_transform._transforms.10.identity_features\", \"_transform._transforms.10.transform_features\", \"_transform._transforms.10.transform_net.initial_layer.weight\", \"_transform._transforms.10.transform_net.initial_layer.bias\", \"_transform._transforms.10.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.10.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.10.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.10.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.10.transform_net.final_layer.weight\", \"_transform._transforms.10.transform_net.final_layer.bias\", \"_transform._transforms.11.identity_features\", \"_transform._transforms.11.transform_features\", \"_transform._transforms.11.transform_net.initial_layer.weight\", \"_transform._transforms.11.transform_net.initial_layer.bias\", \"_transform._transforms.11.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.11.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.11.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.11.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.11.transform_net.final_layer.weight\", \"_transform._transforms.11.transform_net.final_layer.bias\", \"_transform._transforms.12.identity_features\", \"_transform._transforms.12.transform_features\", \"_transform._transforms.12.transform_net.initial_layer.weight\", \"_transform._transforms.12.transform_net.initial_layer.bias\", \"_transform._transforms.12.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.12.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.12.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.12.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.12.transform_net.final_layer.weight\", \"_transform._transforms.12.transform_net.final_layer.bias\", \"_transform._transforms.13.identity_features\", \"_transform._transforms.13.transform_features\", \"_transform._transforms.13.transform_net.initial_layer.weight\", \"_transform._transforms.13.transform_net.initial_layer.bias\", \"_transform._transforms.13.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.13.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.13.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.13.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.13.transform_net.final_layer.weight\", \"_transform._transforms.13.transform_net.final_layer.bias\", \"_transform._transforms.14.identity_features\", \"_transform._transforms.14.transform_features\", \"_transform._transforms.14.transform_net.initial_layer.weight\", \"_transform._transforms.14.transform_net.initial_layer.bias\", \"_transform._transforms.14.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.14.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.14.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.14.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.14.transform_net.final_layer.weight\", \"_transform._transforms.14.transform_net.final_layer.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch.save(model.flow.state_dict(), \"./flow.pt\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflows\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./flow.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Flow:\n\tUnexpected key(s) in state_dict: \"_transform._transforms.10.identity_features\", \"_transform._transforms.10.transform_features\", \"_transform._transforms.10.transform_net.initial_layer.weight\", \"_transform._transforms.10.transform_net.initial_layer.bias\", \"_transform._transforms.10.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.10.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.10.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.10.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.10.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.10.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.10.transform_net.final_layer.weight\", \"_transform._transforms.10.transform_net.final_layer.bias\", \"_transform._transforms.11.identity_features\", \"_transform._transforms.11.transform_features\", \"_transform._transforms.11.transform_net.initial_layer.weight\", \"_transform._transforms.11.transform_net.initial_layer.bias\", \"_transform._transforms.11.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.11.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.11.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.11.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.11.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.11.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.11.transform_net.final_layer.weight\", \"_transform._transforms.11.transform_net.final_layer.bias\", \"_transform._transforms.12.identity_features\", \"_transform._transforms.12.transform_features\", \"_transform._transforms.12.transform_net.initial_layer.weight\", \"_transform._transforms.12.transform_net.initial_layer.bias\", \"_transform._transforms.12.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.12.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.12.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.12.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.12.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.12.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.12.transform_net.final_layer.weight\", \"_transform._transforms.12.transform_net.final_layer.bias\", \"_transform._transforms.13.identity_features\", \"_transform._transforms.13.transform_features\", \"_transform._transforms.13.transform_net.initial_layer.weight\", \"_transform._transforms.13.transform_net.initial_layer.bias\", \"_transform._transforms.13.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.13.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.13.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.13.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.13.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.13.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.13.transform_net.final_layer.weight\", \"_transform._transforms.13.transform_net.final_layer.bias\", \"_transform._transforms.14.identity_features\", \"_transform._transforms.14.transform_features\", \"_transform._transforms.14.transform_net.initial_layer.weight\", \"_transform._transforms.14.transform_net.initial_layer.bias\", \"_transform._transforms.14.transform_net.blocks.0.context_layer.weight\", \"_transform._transforms.14.transform_net.blocks.0.context_layer.bias\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.0.weight\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.0.bias\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.1.weight\", \"_transform._transforms.14.transform_net.blocks.0.linear_layers.1.bias\", \"_transform._transforms.14.transform_net.blocks.1.context_layer.weight\", \"_transform._transforms.14.transform_net.blocks.1.context_layer.bias\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.0.weight\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.0.bias\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.1.weight\", \"_transform._transforms.14.transform_net.blocks.1.linear_layers.1.bias\", \"_transform._transforms.14.transform_net.final_layer.weight\", \"_transform._transforms.14.transform_net.final_layer.bias\". "
     ]
    }
   ],
   "source": [
    "# torch.save(model.flow.state_dict(), \"./flow.pt\")\n",
    "base.Flow(distribution=model.q_test, transform=model.flows).load_state_dict(torch.load(\"./flow.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d77a7ae7-5d0f-443f-b0f1-efec7f794016",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JetNetDataloader' object has no attribute 'scaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/JetNet_NF/LitJetNet/LitNF/lit_nf.py:435\u001b[0m, in \u001b[0;36mTransGan.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;124;03m'''This calculates some important metrics on the hold out set (checking for overtraining)'''\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m    436\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'JetNetDataloader' object has no attribute 'scaler'"
     ]
    }
   ],
   "source": [
    "model.validation_step(torch.ones(10,30,3).cuda(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75ff0ae9-845a-450b-8bd6-32667358b831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64950"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import mass\n",
    "del model\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d67c30-d4ca-486d-bd8f-b7b968a0b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004051065444946289\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=15, nhead=1).cuda()\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6).cuda()\n",
    "lin=nn.Sequential(*[nn.Linear(99,99) for i in range(6)]).cuda()\n",
    "import time\n",
    "error=[]\n",
    "for i in range(100):\n",
    "    start=time.time()\n",
    "    src = torch.rand(5000, 99).cuda()\n",
    "    out = lin(src).detach()\n",
    "    error.append(time.time()-start)\n",
    "    \n",
    "print(np.average(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764480a-86cd-499c-86ca-cb7b31c4ad2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetnet",
   "language": "python",
   "name": "jetnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
