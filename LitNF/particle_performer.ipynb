{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a88fd02-4ae2-4dfb-a320-55409876c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jetnet\n",
    "from jetnet_dataloader import JetNetDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8589fc-b9c1-4fca-89e4-b7a3358c07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaechben/JetNet_NF/LitJetNet/LitNF/jetnet_dataloader.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(self.data).float()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import yaml\n",
    "import os\n",
    "p=\"g\"\n",
    "best_hparam=\"bestever_{}/hparams.yaml\".format(p) #top\n",
    "# best_hparam=\"/beegfs/desy/user/kaechben/fix_mask_scan3/lightning_logs/version_180/hparams.yaml\" #top\n",
    "config={'batch_size': 3096, 'bins': 5, 'coupling_layers': 15, 'dropout': 0.2, 'freq': 5, 'heads': 4, 'hidden': 500, 'max_epochs': 128, \n",
    " 'max_steps': 100000, 'n_dim': 3, 'n_part': 30, 'name': 'jupyter', 'parton': 'g', 'seed': 69, }\n",
    "\n",
    "data_module = JetNetDataloader(config,) #this loads the data\n",
    "data_module.setup(\"train\")\n",
    "data=data_module.data\n",
    "mask=~(data[:,config[\"n_part\"]*config[\"n_dim\"]:].bool())\n",
    "\n",
    "data=data[:,:config[\"n_part\"]*config[\"n_dim\"]].reshape(len(data),config[\"n_part\"],config[\"n_dim\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b0e589-dc77-4b14-8370-1b6d231082a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from performer_pytorch import SelfAttention\n",
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from numpy import random\n",
    "@torch.jit.script\n",
    "def delta_phi(a, b):\n",
    "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_r2(eta1, phi1, eta2, phi2):\n",
    "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
    "\n",
    "\n",
    "def to_pt2(x, eps=1e-8):\n",
    "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        pt2 = pt2.clamp(min=eps)\n",
    "    return pt2\n",
    "\n",
    "\n",
    "def to_m2(x, eps=1e-8):\n",
    "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        m2 = m2.clamp(min=eps)\n",
    "    return m2\n",
    "\n",
    "\n",
    "def atan2(y, x):\n",
    "    sx = torch.sign(x)\n",
    "    sy = torch.sign(y)\n",
    "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
    "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
    "    return atan_part + pi_part\n",
    "\n",
    "\n",
    "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
    "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
    "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
    "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
    "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return torch.cat((pt, rapidity, phi), dim=1)\n",
    "    else:\n",
    "        m = torch.sqrt(to_m2(x, eps=eps))\n",
    "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
    "\n",
    "\n",
    "def boost(x, boostp4, eps=1e-8):\n",
    "    # boost x to the rest frame of boostp4\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
    "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
    "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
    "    gamma2 = (gamma - 1) / b2\n",
    "    gamma2.masked_fill_(b2 == 0, 0)\n",
    "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
    "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
    "    return v\n",
    "\n",
    "\n",
    "def p3_norm(p, eps=1e-8):\n",
    "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
    "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "\n",
    "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
    "    lndelta = torch.log(delta.clamp(min=eps))\n",
    "    if num_outputs == 1:\n",
    "        return lndelta\n",
    "\n",
    "    if num_outputs > 1:\n",
    "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
    "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
    "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
    "        outputs = [lnkt, lnz, lndelta]\n",
    "\n",
    "    if num_outputs > 3:\n",
    "        xij = xi + xj\n",
    "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
    "        outputs.append(lnm2)\n",
    "\n",
    "    if num_outputs > 4:\n",
    "        ei, ej = xi[:, 3:4], xj[:, 3:4]\n",
    "        emin = ((ei <= ej) * ei + (ei > ej) * ej) if for_onnx else torch.minimum(ei, ej)\n",
    "        lnet = torch.log((emin * delta).clamp(min=eps))\n",
    "        lnze = torch.log((emin / (ei + ej).clamp(min=eps)).clamp(min=eps))\n",
    "        outputs += [lnet, lnze]\n",
    "\n",
    "    if num_outputs > 6:\n",
    "        costheta = (p3_norm(xi, eps=eps) * p3_norm(xj, eps=eps)).sum(dim=1, keepdim=True)\n",
    "        sintheta = (1 - costheta**2).clamp(min=0, max=1).sqrt()\n",
    "        outputs += [costheta, sintheta]\n",
    "\n",
    "    assert(len(outputs) == num_outputs)\n",
    "    return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # From https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
    "        module_list = []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.LayerNorm(input_dim),\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.input_bn is not None:\n",
    "            # x: (batch, embed_dim, seq_len)\n",
    "            x = self.input_bn(x)\n",
    "            x = x.permute(2, 0, 1).contiguous()\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class PairEmbed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu', eps=1e-8, for_onnx=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.for_onnx = for_onnx\n",
    "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=input_dim, eps=eps, for_onnx=for_onnx)\n",
    "\n",
    "        module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.Conv1d(input_dim, dim, 1),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, v_dim, seq_len)\n",
    "        with torch.no_grad():\n",
    "            batch_size, _, seq_len = x.size()\n",
    "            if not self.for_onnx:\n",
    "                i, j = torch.tril_indices(seq_len, seq_len, device=x.device)\n",
    "                x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
    "                xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                xj = x[:, :, j, i]\n",
    "                x = self.pairwise_lv_fts(xi, xj)\n",
    "            else:\n",
    "                x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2)).view(batch_size, -1, seq_len * seq_len)\n",
    "\n",
    "        elements = self.embed(x)  # (batch, embed_dim, num_elements)\n",
    "        if not self.for_onnx:\n",
    "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=x.device)\n",
    "            y[:, :, i, j] = elements\n",
    "            y[:, :, j, i] = elements\n",
    "        else:\n",
    "            y = elements.view(batch_size, -1, seq_len, seq_len)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
    "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                 add_bias_kv=False, activation='gelu',# lin_attn=True,\n",
    "                 scale_fc=False, scale_attn=False, scale_heads=False, scale_resids=False, ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.ffn_dim = embed_dim * ffn_ratio\n",
    "\n",
    "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=attn_dropout,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "        )\n",
    "#         self.attn=SelfAttention(dim=embed_dim,heads=num_heads,nb_features=self.ffn_dim,causal=False)#\n",
    "        self.attn=nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=attn_dropout,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "        )\n",
    "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
    "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.act_dropout = nn.Dropout(activation_dropout)\n",
    "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
    "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
    "        #self.lin_attn=lin_attn\n",
    "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
    "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
    "\n",
    "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
    "            padding_mask (ByteTensor, optional): binary\n",
    "                ByteTensor of shape `(batch, seq_len)` where padding\n",
    "                elements are indicated by ``1``.\n",
    "        Returns:\n",
    "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
    "        \"\"\"\n",
    "\n",
    "        if x_cls is not None:\n",
    "            with torch.no_grad():\n",
    "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
    "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
    "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
    "            residual = x_cls\n",
    "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
    "            u = self.pre_attn_norm(u)\n",
    "           # if self.lin_attn:\n",
    "            #    x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0].unsqueeze(0) # (1, batch, embed_dim)\n",
    "#            else:\n",
    "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0] # (1, batch, embed_dim)\n",
    "        else:\n",
    "            residual = x\n",
    "            x = self.pre_attn_norm(x)\n",
    "#             if self.lin_attn: \n",
    "#                 x = self.attn(x, x, x, key_padding_mask=padding_mask) #[0]#(seq_len, batch, embed_dim)  attn_mask=attn_mask\n",
    "#             else:\n",
    "            x = self.attn(x, x, x, key_padding_mask=padding_mask)[0] # (1, batch, embed_dim)\n",
    "\n",
    "        if self.c_attn is not None:\n",
    "            tgt_len, bsz = x.size(0), x.size(1)\n",
    "            x = x.view(tgt_len, bsz, self.num_heads, self.head_dim)\n",
    "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
    "            x = x.reshape(tgt_len, bsz, self.embed_dim)\n",
    "        if self.post_attn_norm is not None:\n",
    "            x = self.post_attn_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.pre_fc_norm(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act_dropout(x)\n",
    "        if self.post_fc_norm is not None:\n",
    "            x = self.post_fc_norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.w_resid is not None:\n",
    "            residual = torch.mul(self.w_resid, residual)\n",
    "        x += residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParticleTransformerCritic(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes=1,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=3,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.trim = trim\n",
    "        self.for_inference = for_inference\n",
    "        self.use_amp = use_amp\n",
    "        self._counter = 0\n",
    "\n",
    "        default_cfg = dict(embed_dim=embed_dims[-1], num_heads=num_heads, ffn_ratio=4,\n",
    "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                           add_bias_kv=False, activation=activation,\n",
    "                           scale_fc=False, scale_attn=False, scale_heads=False, scale_resids=False)\n",
    "\n",
    "        cfg_block = copy.deepcopy(default_cfg)\n",
    "        if block_params is not None:\n",
    "            cfg_block.update(block_params)\n",
    "        \n",
    "\n",
    "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
    "        if cls_block_params is not None:\n",
    "            cfg_cls_block.update(cls_block_params)\n",
    "        \n",
    "\n",
    "        self.embed = Embed(input_dim, embed_dims, activation=activation)\n",
    "        self.pair_embed = PairEmbed(\n",
    "            pair_input_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
    "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim > 0 else None\n",
    "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
    "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dims[-1])\n",
    "\n",
    "        if fc_params is not None:\n",
    "            fcs = []\n",
    "            in_dim = embed_dims[-1]\n",
    "            for out_dim, drop_rate in fc_params:\n",
    "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
    "                in_dim = out_dim\n",
    "            fcs.append(nn.Linear(in_dim, num_classes))\n",
    "            self.fc = nn.Sequential(*fcs)\n",
    "        else:\n",
    "            self.fc = None\n",
    "\n",
    "        # init\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dims[-1]), requires_grad=True)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.sig=torch.nn.Sigmoid()\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token', }\n",
    "\n",
    "    def forward(self, x, v=None, mask=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if mask is None:\n",
    "                mask = torch.ones_like(x[:, :1])\n",
    "            mask = mask.bool()\n",
    "\n",
    "            if self.trim and not self.for_inference:\n",
    "                if self._counter < 5:\n",
    "                    self._counter += 1\n",
    "                else:\n",
    "                    if self.training:\n",
    "                        q = min(1, random.uniform(0.9, 1.02))\n",
    "                        maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "                        rand = torch.rand_like(mask.type_as(x))\n",
    "                        rand.masked_fill_(~mask, -1)\n",
    "                        perm = rand.argsort(dim=-1, descending=True)\n",
    "                        mask = torch.gather(mask, -1, perm)\n",
    "                        x = torch.gather(x, -1, perm.expand_as(x))\n",
    "                        if v is not None and self.pair_embed is not None:\n",
    "                            v = torch.gather(v, -1, perm.expand_as(v))\n",
    "                    else:\n",
    "                        maxlen = mask.sum(dim=-1).max()\n",
    "                    maxlen = max(maxlen, 1)\n",
    "                    if maxlen < mask.size(-1):\n",
    "                        mask = mask[:, :, :maxlen]\n",
    "                        x = x[:, :, :maxlen]\n",
    "                        if v is not None and self.pair_embed is not None:\n",
    "                            v = v[:, :, :maxlen]\n",
    "\n",
    "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            # input embedding\n",
    "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
    "            attn_mask = None\n",
    "            if v is not None and self.pair_embed is not None:\n",
    "                attn_mask = self.pair_embed(v).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
    "            # transform\n",
    "            for block in self.blocks:     \n",
    "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
    "            # extract class token\n",
    "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
    "            for block in self.cls_blocks:\n",
    "                \n",
    "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "            # fc\n",
    "            if self.fc is None:\n",
    "                return x_cls\n",
    "            output = self.fc(x_cls)\n",
    "            if self.for_inference:\n",
    "                output = self.sig(output)\n",
    "            # print('output:\\n', output)\n",
    "            return output\n",
    "\n",
    "class ParticleTransformerGen(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 out_dim=3,\n",
    "                 # network configurations\n",
    "                 fc_params=[],\n",
    "                 pair_input_dim=3,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.trim = trim\n",
    "        self.for_inference = for_inference\n",
    "        self.use_amp = use_amp\n",
    "        self._counter = 0\n",
    "\n",
    "        default_cfg = dict(embed_dim=embed_dims[-1], num_heads=num_heads, ffn_ratio=4,\n",
    "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                           add_bias_kv=False, activation=activation,\n",
    "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
    "\n",
    "        cfg_block = copy.deepcopy(default_cfg)\n",
    "        self.embed = Embed(input_dim, embed_dims, activation=activation)\n",
    "        self.pair_embed = PairEmbed(\n",
    "            pair_input_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
    "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim > 0 else None\n",
    "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dims[-1])\n",
    "        \n",
    "        fcs = []\n",
    "        in_dim = embed_dims[-1]\n",
    "        for out_dim, drop_rate in fc_params:\n",
    "            fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
    "            in_dim = out_dim\n",
    "        fcs.append(nn.Linear(in_dim, out_dim))\n",
    "        self.fc = nn.Sequential(*fcs)\n",
    "\n",
    "    def forward(self, x, v=None, mask=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if mask is None:\n",
    "                mask = torch.ones_like(x[:, :1])\n",
    "            mask = mask.bool()\n",
    "\n",
    "            # if self.trim and not self.for_inference:\n",
    "            #     if self._counter < 5:\n",
    "            #         self._counter += 1\n",
    "            #     else:\n",
    "            #         if self.training:\n",
    "            #             q = min(1, random.uniform(0.9, 1.02))\n",
    "            #             maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "            #             rand = torch.rand_like(mask.type_as(x))\n",
    "            #             rand.masked_fill_(~mask, -1)\n",
    "            #             perm = rand.argsort(dim=-1, descending=True)\n",
    "            #             mask = torch.gather(mask, -1, perm)\n",
    "            #             x = torch.gather(x, -1, perm.expand_as(x))\n",
    "            #             if v is not None and self.pair_embed is not None:\n",
    "            #                 v = torch.gather(v, -1, perm.expand_as(v))\n",
    "            #         else:\n",
    "            #             maxlen = mask.sum(dim=-1).max()\n",
    "            #         maxlen = max(maxlen, 1)\n",
    "            #         if maxlen < mask.size(-1):\n",
    "            #             mask = mask[:, :, :maxlen]\n",
    "            #             x = x[:, :, :maxlen]\n",
    "            #             if v is not None and self.pair_embed is not None:\n",
    "            #                 v = v[:, :, :maxlen]\n",
    "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            # input embedding\n",
    "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
    "            attn_mask = None\n",
    "            if v is not None and self.pair_embed is not None:\n",
    "                attn_mask = self.pair_embed(v).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
    "            # transform\n",
    "            for block in self.blocks:\n",
    "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
    "\n",
    "            output = self.fc(x)\n",
    "            # print('output:\\n', output)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d219f803-f5c6-4dd0-af2b-76e136a061b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from particle_transformer import ParticleTransformerCritic\n",
    "\n",
    "model=ParticleTransformerCritic(3)\n",
    "gen=ParticleTransformerGen(3)\n",
    "\n",
    "p=data_module.data[:,:90].reshape(-1,30,3)\n",
    "px=torch.cos(p[:,:,1])*p[:,:,2]\n",
    "py=torch.sin(p[:,:,1])*p[:,:,2]\n",
    "pz=torch.sinh(p[:,:,0])*p[:,:,2]\n",
    "E=torch.sqrt(px**2+py**2+pz**2)\n",
    "v=torch.stack((px,py,pz,E),dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867d4e10-48ee-44d6-b537-e213c0fc1d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a7d109-a360-4c78-a174-0b67b63dbc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.0381e+00,  4.9579e-01, -2.6674e+00],\n",
       "         [ 1.5417e-01,  4.9677e-01, -2.7089e+00],\n",
       "         [-1.0025e+00,  1.7906e+00, -2.2417e+00],\n",
       "         [ 1.5954e+00,  1.6986e+00, -4.2189e+00],\n",
       "         [ 1.4051e+00,  9.5534e-01, -2.4250e+00],\n",
       "         [-6.6820e-01,  2.3693e-01, -3.4033e-01],\n",
       "         [ 4.8175e-01,  1.0187e+00, -2.7466e+00],\n",
       "         [ 7.9907e-01, -7.5628e-01, -1.1550e+00],\n",
       "         [ 1.1858e+00,  2.1888e+00, -1.1491e+00],\n",
       "         [-1.2118e+00,  1.8390e+00, -1.2990e+00]],\n",
       "\n",
       "        [[ 1.7627e+00,  1.8285e+00, -2.4155e+00],\n",
       "         [ 6.4342e-01, -7.5569e-01, -1.0874e+00],\n",
       "         [-1.3481e+00,  1.8935e+00, -4.6243e-01],\n",
       "         [ 7.8466e-01,  8.1616e-01, -3.1923e+00],\n",
       "         [ 6.8084e-01,  3.4737e+00, -2.4908e+00],\n",
       "         [-6.5216e-01, -3.2817e-01, -7.0682e-01],\n",
       "         [ 6.2730e-01,  2.2780e+00, -3.0473e+00],\n",
       "         [-6.3555e-01,  3.9729e-01,  3.4236e-01],\n",
       "         [-5.6970e-01,  1.5619e+00, -8.9322e-01],\n",
       "         [-1.6324e+00,  2.5417e+00, -2.7437e+00]],\n",
       "\n",
       "        [[ 1.1588e+00,  1.2237e+00, -2.4521e+00],\n",
       "         [ 7.3429e-02,  8.8454e-01, -1.3320e+00],\n",
       "         [-7.2108e-01,  1.7624e+00, -1.0829e+00],\n",
       "         [ 8.5290e-01,  1.7295e+00, -3.1996e+00],\n",
       "         [ 1.1029e+00,  2.9375e+00, -2.9230e+00],\n",
       "         [-1.0846e+00,  8.4792e-01, -3.4338e+00],\n",
       "         [-1.8467e+00,  1.3747e+00, -1.9814e+00],\n",
       "         [ 6.9712e-01,  7.2366e-01, -3.4149e+00],\n",
       "         [-1.8536e-01,  1.5928e+00, -3.1303e+00],\n",
       "         [-1.7449e+00,  1.6749e+00, -8.6697e-01]],\n",
       "\n",
       "        [[ 1.8432e+00, -1.1872e-03, -1.1577e+00],\n",
       "         [ 7.0643e-02,  1.5961e+00, -2.1376e+00],\n",
       "         [-2.7300e-01,  1.4825e+00, -1.6131e+00],\n",
       "         [ 2.4981e+00,  1.5640e+00, -3.2696e+00],\n",
       "         [ 1.9098e+00,  1.9966e+00, -3.5200e+00],\n",
       "         [ 7.4938e-01,  7.5472e-01, -2.8607e+00],\n",
       "         [ 3.8536e-01,  2.2720e+00, -1.2606e+00],\n",
       "         [-5.8654e-01,  1.9496e-01, -2.2430e+00],\n",
       "         [-1.3991e+00,  2.7435e+00,  6.6644e-01],\n",
       "         [-1.4335e-01,  2.0474e+00, -1.9926e+00]],\n",
       "\n",
       "        [[ 2.9998e+00,  1.3836e+00, -2.7919e+00],\n",
       "         [ 1.7779e+00,  6.5790e-01, -1.0498e+00],\n",
       "         [ 4.5438e-01,  2.5781e+00, -1.3835e+00],\n",
       "         [ 4.5307e-01,  2.2201e+00, -2.7984e+00],\n",
       "         [ 8.0928e-01,  1.4498e+00, -6.4721e-01],\n",
       "         [-1.3373e+00,  1.2540e+00, -9.8098e-01],\n",
       "         [-4.2870e-01,  3.6134e+00, -2.2278e+00],\n",
       "         [-2.2828e-01,  8.5106e-01, -2.3125e-01],\n",
       "         [ 7.3647e-01,  1.9186e+00, -1.8863e+00],\n",
       "         [ 7.8885e-01,  2.3642e+00, -2.0584e+00]],\n",
       "\n",
       "        [[ 2.0610e+00,  9.4244e-01, -3.1630e+00],\n",
       "         [ 5.2931e-01,  2.6439e+00, -1.2927e+00],\n",
       "         [ 5.2066e-01,  1.5008e+00, -2.2695e+00],\n",
       "         [ 1.8438e+00,  2.7928e+00, -2.3665e+00],\n",
       "         [ 1.1342e+00,  3.1348e+00, -3.3251e+00],\n",
       "         [-3.5658e-01, -3.6182e-03, -2.2831e+00],\n",
       "         [-1.4301e+00,  1.8423e+00, -3.9953e+00],\n",
       "         [ 3.0822e-01,  7.7309e-01, -1.4491e+00],\n",
       "         [-8.0837e-01,  3.1405e+00, -2.8076e+00],\n",
       "         [-1.1537e+00,  2.3707e+00, -2.7879e+00]],\n",
       "\n",
       "        [[ 1.2718e+00,  4.5077e-01, -3.2827e+00],\n",
       "         [-3.5727e-01,  8.7142e-01, -1.3741e+00],\n",
       "         [-6.9278e-01,  2.0920e+00, -4.0740e-01],\n",
       "         [ 6.5619e-01,  6.7752e-01, -3.0845e+00],\n",
       "         [ 4.0331e-01,  1.7623e+00, -2.0350e+00],\n",
       "         [-6.2717e-01, -6.7402e-01, -5.2589e-01],\n",
       "         [-1.2517e+00,  3.1543e+00, -3.4095e+00],\n",
       "         [-7.7132e-01,  2.0256e+00, -8.7531e-01],\n",
       "         [ 7.6061e-02,  1.0830e+00, -4.9239e-01],\n",
       "         [-1.1641e-02,  1.7304e+00, -2.2050e+00]],\n",
       "\n",
       "        [[-3.1046e-02,  6.5381e-01, -4.1971e+00],\n",
       "         [-5.9641e-01,  2.7345e-01, -5.7268e-01],\n",
       "         [ 3.2748e-01,  1.3054e+00, -2.1680e+00],\n",
       "         [ 2.8496e+00,  1.2697e+00, -2.1111e+00],\n",
       "         [ 7.5275e-02,  3.0643e+00, -3.2441e+00],\n",
       "         [-1.8938e+00, -1.3228e-02, -6.6946e-01],\n",
       "         [-1.4637e+00,  8.7653e-01, -3.3996e+00],\n",
       "         [ 1.4365e+00, -9.4369e-02, -9.1498e-01],\n",
       "         [-1.3320e+00,  7.4755e-01, -2.1794e+00],\n",
       "         [-1.4192e+00,  2.8149e+00, -3.0531e+00]],\n",
       "\n",
       "        [[ 1.6303e+00,  1.3196e+00, -2.5485e+00],\n",
       "         [ 1.7109e-01,  6.2625e-01, -1.2991e+00],\n",
       "         [-7.1982e-01,  2.6831e+00, -2.0963e+00],\n",
       "         [-4.0016e-01,  2.1420e-01, -3.6551e+00],\n",
       "         [ 1.1557e+00,  2.2597e+00, -2.3865e+00],\n",
       "         [-1.8625e+00,  2.0305e+00, -2.3553e+00],\n",
       "         [ 5.5885e-01,  8.2519e-01, -4.0347e+00],\n",
       "         [-5.2000e-01,  3.4398e-01, -1.2633e+00],\n",
       "         [-2.6045e-01,  1.8426e+00, -2.6910e+00],\n",
       "         [-1.5401e+00,  3.7760e+00, -2.5890e+00]],\n",
       "\n",
       "        [[ 1.5499e+00,  1.0302e+00, -3.8915e+00],\n",
       "         [-1.7051e-01,  2.5113e+00, -1.3043e+00],\n",
       "         [-1.9377e+00,  2.1799e+00, -3.2198e+00],\n",
       "         [ 1.1122e+00,  2.1102e+00, -2.4401e+00],\n",
       "         [ 6.6264e-01,  3.0939e+00, -5.2556e-01],\n",
       "         [-1.5284e+00,  9.1159e-01, -2.3707e+00],\n",
       "         [ 2.7262e-02,  1.0904e+00, -2.4213e+00],\n",
       "         [ 3.3306e-01,  5.3643e-01, -1.1242e+00],\n",
       "         [-1.0796e+00,  6.9176e-01, -3.2132e+00],\n",
       "         [-5.2017e-01,  1.7434e+00, -1.1299e+00]],\n",
       "\n",
       "        [[ 2.4742e+00,  1.4604e+00, -2.8983e+00],\n",
       "         [ 7.8034e-02,  2.8776e+00, -3.0649e+00],\n",
       "         [-1.0486e+00,  2.1438e+00, -1.2913e+00],\n",
       "         [ 8.7130e-01,  2.0570e+00, -3.9074e+00],\n",
       "         [ 3.5433e-02,  2.7677e+00, -3.3114e+00],\n",
       "         [-1.0603e+00,  6.4840e-01, -2.1060e+00],\n",
       "         [-6.3016e-01,  1.5682e+00, -2.9097e+00],\n",
       "         [ 3.4528e-02,  1.7284e+00, -1.5840e+00],\n",
       "         [-2.0803e+00,  1.3798e+00, -1.2678e+00],\n",
       "         [-5.4576e-02,  2.3767e+00, -3.1785e+00]],\n",
       "\n",
       "        [[-8.5060e-01,  1.0449e+00, -2.1078e+00],\n",
       "         [-4.1331e-01,  1.1117e+00, -1.9319e+00],\n",
       "         [-1.7721e+00,  2.1202e+00, -2.1173e+00],\n",
       "         [ 5.9027e-01,  1.7107e+00, -2.2673e+00],\n",
       "         [-9.3736e-01,  1.4919e+00, -2.4673e+00],\n",
       "         [-2.0822e+00,  4.6372e-01, -1.5085e+00],\n",
       "         [-9.5918e-01,  2.0835e+00, -2.0508e+00],\n",
       "         [-1.3248e+00,  1.5646e+00, -6.7900e-01],\n",
       "         [-9.8716e-02,  1.8487e+00, -1.5252e+00],\n",
       "         [-8.5340e-01,  1.0617e+00, -1.5902e+00]],\n",
       "\n",
       "        [[ 8.4786e-01,  1.1886e+00, -3.4081e+00],\n",
       "         [-5.7594e-01,  8.9559e-01, -1.8601e+00],\n",
       "         [-8.6814e-01,  1.8292e+00, -9.5061e-01],\n",
       "         [ 9.7099e-01,  1.6738e+00, -1.5584e+00],\n",
       "         [-1.4346e+00,  1.2091e+00, -2.5206e+00],\n",
       "         [ 1.2655e+00, -1.2826e-01, -2.0944e+00],\n",
       "         [-1.1516e+00,  2.3653e+00, -2.8241e+00],\n",
       "         [ 1.4395e+00,  7.5892e-01, -2.1486e+00],\n",
       "         [-2.5943e-02,  1.3961e+00, -2.3121e+00],\n",
       "         [-1.4092e+00,  1.9497e+00, -2.9907e+00]],\n",
       "\n",
       "        [[ 2.4276e-01,  1.4879e+00, -3.0911e+00],\n",
       "         [-7.9540e-01,  1.2568e+00, -1.3769e+00],\n",
       "         [-4.6367e-01,  1.7129e+00, -1.0034e+00],\n",
       "         [ 2.5948e-01,  1.2408e+00, -2.4430e+00],\n",
       "         [-1.0171e+00,  3.0181e+00, -7.7754e-01],\n",
       "         [-2.5858e+00, -3.0247e-01, -1.4622e+00],\n",
       "         [-2.0061e+00,  1.6402e+00, -2.4800e+00],\n",
       "         [ 3.5689e-01,  2.0237e+00, -2.2922e+00],\n",
       "         [-1.0642e+00,  3.0834e+00, -2.6796e+00],\n",
       "         [-1.1156e+00,  1.7737e+00, -1.8670e+00]],\n",
       "\n",
       "        [[ 6.1878e-01,  1.5607e+00, -3.1420e+00],\n",
       "         [-7.2471e-01,  8.3104e-01, -8.0744e-01],\n",
       "         [ 1.8892e-01,  2.4623e+00, -2.0136e+00],\n",
       "         [ 1.2959e+00,  1.8795e+00, -3.6941e+00],\n",
       "         [ 7.5770e-01,  1.3398e+00, -1.5511e+00],\n",
       "         [ 2.1469e-01, -8.4383e-01, -1.2427e+00],\n",
       "         [-1.3235e+00,  2.1020e+00, -1.9582e+00],\n",
       "         [-5.2214e-01,  8.7850e-01, -1.0055e+00],\n",
       "         [-1.0151e+00,  1.6459e+00, -2.6381e+00],\n",
       "         [-1.3737e+00,  2.2613e+00, -1.5173e+00]],\n",
       "\n",
       "        [[ 2.6191e+00,  6.6277e-01, -2.4573e+00],\n",
       "         [-7.1342e-01,  7.4554e-01, -1.8212e+00],\n",
       "         [ 9.3150e-01,  1.6556e+00, -1.2356e+00],\n",
       "         [-3.4945e-01,  4.9239e-01, -5.6002e-01],\n",
       "         [ 7.8578e-01,  1.1685e+00, -1.9009e+00],\n",
       "         [-1.2887e-01,  9.2075e-01, -1.4664e+00],\n",
       "         [-1.2047e+00,  1.9458e+00, -2.6228e+00],\n",
       "         [-2.7274e-01, -3.2684e-01, -7.5232e-01],\n",
       "         [-1.4829e-01,  1.4817e+00, -2.3207e+00],\n",
       "         [ 8.0924e-01,  1.3406e+00, -1.9578e+00]],\n",
       "\n",
       "        [[ 1.6653e+00, -5.1933e-02, -1.5557e+00],\n",
       "         [-8.2649e-01,  2.0299e+00, -1.8072e+00],\n",
       "         [-1.0414e+00,  1.6644e+00, -1.4480e+00],\n",
       "         [ 2.0736e-01,  1.4675e+00, -1.0317e+00],\n",
       "         [ 1.2198e+00,  1.7481e+00, -3.3852e+00],\n",
       "         [-1.2566e+00,  5.0476e-01, -1.5314e+00],\n",
       "         [ 2.2515e-02,  1.4809e+00, -2.4677e+00],\n",
       "         [ 1.5164e-01, -1.0864e+00, -1.3042e+00],\n",
       "         [-7.5776e-01,  1.9003e+00, -1.6392e+00],\n",
       "         [-8.2358e-01,  1.4481e+00, -1.7551e+00]],\n",
       "\n",
       "        [[ 1.2258e+00,  1.9671e+00, -3.2104e+00],\n",
       "         [-1.5044e+00,  4.5749e-01, -8.0215e-01],\n",
       "         [-4.1930e-01,  4.1654e+00, -2.7581e+00],\n",
       "         [-1.1795e+00,  4.2099e-01, -2.2240e+00],\n",
       "         [-8.6852e-02,  1.8529e-01, -9.3524e-01],\n",
       "         [-6.2833e-01,  1.7399e+00, -2.3170e+00],\n",
       "         [-2.2546e+00,  3.8053e+00, -1.8576e+00],\n",
       "         [-9.8618e-01,  1.9162e+00, -2.7432e+00],\n",
       "         [-3.0352e-01,  2.4014e+00, -2.2981e+00],\n",
       "         [-5.9997e-01,  2.0822e+00, -1.1082e+00]],\n",
       "\n",
       "        [[ 1.5542e-01,  1.4037e+00, -2.6241e+00],\n",
       "         [-1.0787e+00,  8.2145e-01, -1.7773e+00],\n",
       "         [-7.2499e-02,  1.8453e+00, -3.0364e+00],\n",
       "         [ 6.7711e-02,  1.3722e+00, -2.9138e+00],\n",
       "         [ 1.1017e+00,  1.0970e+00, -2.1748e+00],\n",
       "         [-1.4944e+00,  5.6938e-01, -1.9585e+00],\n",
       "         [-3.0756e-01,  5.0795e+00, -3.2170e+00],\n",
       "         [ 1.9614e-01,  1.2135e-02, -1.5270e+00],\n",
       "         [ 2.2619e-01,  1.4915e+00, -3.9191e-01],\n",
       "         [-8.6169e-01,  2.2113e+00, -2.1646e+00]],\n",
       "\n",
       "        [[-2.1562e-01,  1.0773e+00, -4.9812e+00],\n",
       "         [ 3.1955e-01,  1.6269e+00, -3.0221e+00],\n",
       "         [-1.1973e+00,  8.1467e-01, -1.6087e+00],\n",
       "         [ 2.0889e-01,  1.3522e+00, -1.2983e+00],\n",
       "         [ 1.8765e-01,  1.7185e+00, -2.9560e+00],\n",
       "         [-3.6722e-01, -6.0882e-01, -1.0062e+00],\n",
       "         [ 7.3956e-01,  2.9540e+00, -2.3265e+00],\n",
       "         [-3.8126e-01,  7.9442e-01, -1.9274e+00],\n",
       "         [-3.2752e-02,  1.5822e+00, -1.8782e+00],\n",
       "         [-1.9956e+00,  3.2115e+00, -1.8530e+00]],\n",
       "\n",
       "        [[ 9.3542e-01,  2.4358e-02, -2.2201e+00],\n",
       "         [-8.6400e-01,  4.2132e-01, -1.9240e+00],\n",
       "         [-9.4308e-01,  7.5307e-01, -1.9451e+00],\n",
       "         [ 2.9065e+00,  9.6754e-01, -3.2353e+00],\n",
       "         [ 1.6301e-02,  3.1236e+00, -1.2765e+00],\n",
       "         [-3.5629e+00, -1.1070e-01, -2.8376e+00],\n",
       "         [ 2.1338e-01,  2.7008e+00, -2.8160e+00],\n",
       "         [-3.2335e-01,  7.5003e-01, -2.4263e+00],\n",
       "         [-2.0115e+00,  1.1908e+00, -9.6994e-01],\n",
       "         [-1.2225e+00,  5.7303e-01, -1.5789e+00]],\n",
       "\n",
       "        [[ 4.3750e-02,  1.9996e-01, -3.0012e+00],\n",
       "         [-6.3434e-01,  3.2778e-01, -6.7827e-01],\n",
       "         [-2.0500e+00,  3.2558e+00, -2.0592e+00],\n",
       "         [ 7.8871e-02,  1.1229e+00, -8.9027e-01],\n",
       "         [-8.2657e-01,  1.4630e+00, -1.9783e+00],\n",
       "         [-5.1963e-01,  3.0055e-01, -1.8929e+00],\n",
       "         [-1.4237e+00,  2.7979e+00, -2.4449e+00],\n",
       "         [-5.6479e-01,  8.2149e-01, -1.8481e+00],\n",
       "         [-7.6930e-01,  1.9490e+00, -1.2472e+00],\n",
       "         [-4.3035e-01,  1.2701e+00, -1.2910e+00]],\n",
       "\n",
       "        [[ 1.1921e+00,  2.3524e+00, -1.8446e+00],\n",
       "         [-7.0026e-01,  2.4040e+00, -1.9909e+00],\n",
       "         [-1.2005e+00,  2.3216e+00, -1.5166e+00],\n",
       "         [-1.7937e-01,  6.9640e-01, -9.9730e-01],\n",
       "         [ 9.6212e-01,  1.8444e+00, -1.9857e+00],\n",
       "         [-1.7935e+00,  4.4436e-01, -1.9827e+00],\n",
       "         [ 1.1154e-02,  2.5646e+00, -2.0757e+00],\n",
       "         [-2.9858e-01,  8.7765e-01, -5.1755e-01],\n",
       "         [ 1.0725e+00,  1.9311e+00, -2.2471e+00],\n",
       "         [-3.2415e-02,  2.9557e+00, -7.9504e-01]],\n",
       "\n",
       "        [[ 3.8493e-01, -8.1493e-01, -3.3386e+00],\n",
       "         [ 3.6575e-01,  1.4015e-01, -1.9447e+00],\n",
       "         [ 3.6050e-02,  2.6417e+00, -1.4304e+00],\n",
       "         [ 2.0253e+00,  2.8631e+00, -2.5479e+00],\n",
       "         [ 3.8783e-01,  2.8890e+00, -2.4195e+00],\n",
       "         [-1.4444e+00,  4.9267e-02, -1.8164e+00],\n",
       "         [ 3.9789e-01,  1.9399e+00, -2.2440e+00],\n",
       "         [-1.9359e-01,  1.7355e+00, -1.6283e+00],\n",
       "         [ 2.1589e-01,  2.1715e+00, -1.4254e+00],\n",
       "         [-5.6298e-01,  1.8827e+00, -2.8667e+00]],\n",
       "\n",
       "        [[ 2.7017e-01,  1.9660e+00, -3.3111e+00],\n",
       "         [-1.2887e+00,  1.3747e+00,  3.8878e-01],\n",
       "         [-1.9042e+00,  1.3294e+00, -8.0533e-01],\n",
       "         [ 8.2305e-01,  1.9540e+00, -3.5067e+00],\n",
       "         [-1.7126e+00,  3.0986e+00, -1.4017e+00],\n",
       "         [-1.8669e+00,  6.5265e-02, -3.4946e+00],\n",
       "         [-2.4563e+00,  2.3759e+00, -2.1779e+00],\n",
       "         [-1.8214e+00, -3.1392e-01, -3.0145e-01],\n",
       "         [-8.9930e-01,  1.7809e+00, -2.7547e+00],\n",
       "         [ 3.3280e-01,  2.7722e+00, -2.9674e-01]],\n",
       "\n",
       "        [[ 2.1310e+00,  3.8023e-01, -3.0516e+00],\n",
       "         [-6.9480e-01,  2.4170e-01, -1.2499e+00],\n",
       "         [ 1.3747e-02,  2.0142e+00, -3.2212e+00],\n",
       "         [-7.2177e-01,  5.0659e-01, -1.8357e+00],\n",
       "         [ 9.4853e-01,  1.6759e+00, -4.9546e-01],\n",
       "         [-7.4657e-01,  1.1820e+00, -2.2776e-02],\n",
       "         [-1.9864e+00,  2.4528e+00, -3.2406e+00],\n",
       "         [-2.0265e+00, -6.2663e-01, -1.1304e+00],\n",
       "         [ 2.7207e-01,  1.5652e+00, -1.0191e+00],\n",
       "         [-1.5481e+00,  1.9875e+00, -2.8478e+00]],\n",
       "\n",
       "        [[ 3.4319e-01,  8.8478e-01, -4.4325e+00],\n",
       "         [-6.8793e-01,  1.2492e+00, -7.2039e-01],\n",
       "         [-6.5385e-01,  2.1978e+00, -2.7794e+00],\n",
       "         [ 1.4652e+00,  2.8433e+00, -2.5803e+00],\n",
       "         [ 9.8520e-01,  1.1949e+00, -3.3412e+00],\n",
       "         [-9.8736e-01,  3.8145e-01, -3.3453e+00],\n",
       "         [-8.5293e-01,  2.4821e+00, -3.7152e+00],\n",
       "         [-8.6429e-01,  1.2532e+00, -1.8907e+00],\n",
       "         [-7.0736e-01, -4.7824e-01, -1.4309e+00],\n",
       "         [-1.6836e+00,  1.6633e+00, -1.5270e+00]],\n",
       "\n",
       "        [[ 8.1675e-01,  1.9959e-01, -1.5717e+00],\n",
       "         [-5.1920e-01,  1.2945e+00, -1.5478e+00],\n",
       "         [-1.4639e+00,  5.7407e-01, -2.8372e+00],\n",
       "         [-1.2164e-01,  3.5835e-01, -2.9833e+00],\n",
       "         [ 9.6936e-01,  2.7853e+00, -3.0124e+00],\n",
       "         [-1.1174e+00,  1.3065e+00, -9.8782e-01],\n",
       "         [-5.6294e-01,  1.9747e+00, -2.7440e+00],\n",
       "         [-1.3651e+00,  1.4307e+00, -1.1148e+00],\n",
       "         [-1.3195e+00,  3.2340e+00, -3.9966e+00],\n",
       "         [ 6.8322e-01,  1.5483e+00, -1.6322e+00]],\n",
       "\n",
       "        [[ 9.7648e-01,  3.9501e-01, -4.4559e+00],\n",
       "         [ 5.6440e-01,  3.6888e-01, -1.4488e+00],\n",
       "         [-9.0679e-01,  3.4869e-01, -1.1505e+00],\n",
       "         [ 4.3234e-01,  1.2213e+00, -1.8047e+00],\n",
       "         [ 7.5974e-01,  1.5110e+00, -1.6816e+00],\n",
       "         [-2.6894e+00, -6.6535e-01, -2.0428e+00],\n",
       "         [ 3.5178e-02,  2.5509e+00, -1.5378e+00],\n",
       "         [-7.3915e-01,  6.4838e-02, -1.8039e+00],\n",
       "         [-6.3951e-02,  2.7216e+00, -1.7976e+00],\n",
       "         [-1.7033e+00,  1.9772e+00, -3.5250e+00]],\n",
       "\n",
       "        [[ 1.7038e-01,  3.3196e-01, -3.1098e+00],\n",
       "         [-7.8362e-01,  1.2361e+00, -1.2300e+00],\n",
       "         [-8.4370e-01,  2.3468e+00, -5.5732e-01],\n",
       "         [ 5.6802e-01,  1.5986e+00, -4.1033e+00],\n",
       "         [ 2.1944e-01,  1.6430e+00, -1.8876e+00],\n",
       "         [-4.4586e-01, -2.3070e-01, -1.6106e+00],\n",
       "         [-2.3917e+00,  1.3748e+00, -1.1624e+00],\n",
       "         [ 1.0563e+00,  2.6318e-01, -1.6968e+00],\n",
       "         [-1.1078e+00,  2.0126e+00, -2.2654e+00],\n",
       "         [-4.7427e-01,  2.8903e+00, -1.2941e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(p[:10].permute(0,2,1),v[:10].permute(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86d0eba0-b492-402d-be2e-5897b818d365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d26af27b-7585-4270-b130-0c1b3b032220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating cov and mmd over 10 batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 19.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.558, 0.02297494809004871)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df40ec87-fcc5-47d9-a7a3-eed0e323bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5193/675825127.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "import jetnet\n",
    "\n",
    "class StandardScaler:\n",
    "    def __init__(self, mean=None, std=None, epsilon=1e-7):\n",
    "        \"\"\"Standard Scaler.\n",
    "        The class can be used to normalize PyTorch Tensors using native\n",
    "        functions. The module does not expect the tensors to be of any specific shape;\n",
    "         as long as the features are the last dimension in the tensor, the module\n",
    "        will work fine.\n",
    "        :param mean: The mean of the features. The property will be set after a call to fit.\n",
    "        :param std: The standard deviation of the features. The property will be set after a call to fit.\n",
    "        :param epsilon: Used to avoid a Division-By-Zero exception.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, values):\n",
    "        dims = list(range(values.dim() - 1))\n",
    "        self.mean = torch.mean(values, dim=dims)\n",
    "        self.std = torch.std(values, dim=dims)\n",
    "\n",
    "    def transform(self, values):\n",
    "        return (values - self.mean) / (self.std + self.epsilon)\n",
    "\n",
    "    def inverse_transform(self, values):\n",
    "        return (values * self.std) + self.mean\n",
    "\n",
    "    def fit_transform(self, values):\n",
    "        self.fit(values)\n",
    "        return self.transform(values)\n",
    "\n",
    "    def to(self, dev):\n",
    "        self.std = self.std.to(dev)\n",
    "        self.mean = self.mean.to(dev)\n",
    "        return self\n",
    "test_set, _ = jetnet.datasets.JetNet.getData(\n",
    "            jet_type=\"q\",\n",
    "            split=\"valid\",\n",
    "            num_particles=30,\n",
    "            data_dir=\"/beegfs/desy/user/kaechben/datasets\",\n",
    "        )\n",
    " \n",
    "data, _ = jetnet.datasets.JetNet.getData(\n",
    "            jet_type=\"q\",\n",
    "            split=\"valid\",\n",
    "            num_particles=30,\n",
    "            data_dir=\"/beegfs/desy/user/kaechben/datasets\",\n",
    "        )\n",
    "    \n",
    "\n",
    "test_set = torch.tensor(test_set)\n",
    "\n",
    "data = torch.tensor(data)\n",
    "data = torch.cat((data, test_set), dim=0)\n",
    "\n",
    "# masks=np.sum(data.values[:,np.arange(3,120,4)],axis=1)\n",
    "masks =data[:, :, -1].bool()\n",
    "data = data[:, :, :-1]\n",
    "n = masks.sum(axis=1)\n",
    "\n",
    "masks = ~masks\n",
    "data[masks, :] = (\n",
    "    torch.normal(mean=torch.zeros_like(data[masks, :]), std=1).abs() * 1e-7\n",
    ")\n",
    "\n",
    "# standard scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "data = data.reshape(len(data), 90)\n",
    "data = torch.tensor(\n",
    "    torch.hstack(\n",
    "        (data.reshape(len(data), 30 * 3), masks))\n",
    "    \n",
    ")\n",
    "# self.data, self.test_set = train_test_split(self.data.cpu().numpy(), test_size=0.3)\n",
    "test_set = data[-len(test_set) :].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "155e236a-32c1-4fe8-a9e7-1cf0925affe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating cov and mmd over 10 batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 18.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5389999999999999, 0.025488714683874404)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jetnet.evaluation.cov_mmd(scaler.inverse_transform(test_set[:,:90].reshape(-1,30,3)),scaler.inverse_transform(test_set[:,:90].reshape(-1,30,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df4beb-be9c-46d1-840e-ba2b65af87d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetnet2",
   "language": "python",
   "name": "jetnet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
