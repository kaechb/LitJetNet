{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b72771-2116-4468-b170-ca8f7fdfdf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thx max\n",
      "good boy\n"
     ]
    }
   ],
   "source": [
    "from jetnet_dataloader import JetNetDataloader,StandardScaler, QuantileTransformer\n",
    "# from helpers import mass\n",
    "from plotting import *\n",
    "from distutils.command.config import config\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import nflows as nf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from jetnet.evaluation import cov_mmd, fpnd, w1efp, w1m, w1p\n",
    "from nflows.flows import base\n",
    "from nflows.nn import nets\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.coupling import PiecewiseRationalQuadraticCouplingTransform\n",
    "from nflows.utils.torchutils import create_random_binary_mask\n",
    "from torch import nn\n",
    "from torch.nn import functional as FF\n",
    "from torch.nn.functional import leaky_relu, sigmoid\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from helpers import CosineWarmupScheduler, mass\n",
    "from plotting import *    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4692af6d-688d-45a7-b653-499d280c4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gen(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dim=3,\n",
    "        l_dim=10,\n",
    "        hidden=300,\n",
    "        num_layers=3,\n",
    "        num_heads=1,\n",
    "        n_part=5,\n",
    "        fc=False,\n",
    "        dropout=0.5,\n",
    "        no_hidden=True,\n",
    "        norm=False\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_nodes = hidden\n",
    "        self.n_dim = n_dim\n",
    "        self.l_dim = l_dim\n",
    "        self.n_part = n_part\n",
    "        self.no_hidden = no_hidden\n",
    "        self.fc = fc\n",
    "        if fc:\n",
    "            self.l_dim *= n_part\n",
    "            self.embbed_flat = nn.Linear(n_dim * n_part, l_dim)\n",
    "            self.flat_hidden = nn.Linear(l_dim, hidden)\n",
    "            self.flat_hidden2 = nn.Linear(hidden, hidden)\n",
    "            self.flat_hidden3 = nn.Linear(hidden, hidden)\n",
    "            self.flat_out = nn.Linear(hidden, n_dim * n_part)\n",
    "            self.out = nn.Linear(hidden, n_dim)\n",
    "            \n",
    "        else:\n",
    "            self.embbed = nn.Linear(n_dim, l_dim)\n",
    "            self.encoder = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=l_dim,\n",
    "                    nhead=num_heads,\n",
    "                    batch_first=True,\n",
    "                    norm_first=norm,\n",
    "                    dim_feedforward=hidden,\n",
    "                    dropout=dropout,\n",
    "                ),\n",
    "                num_layers=num_layers,\n",
    "            )\n",
    "            self.hidden = nn.Linear(l_dim, hidden)\n",
    "            self.hidden2 = nn.Linear(hidden, hidden)\n",
    "            self.hidden3 = nn.Linear(hidden, hidden)\n",
    "            self.dropout = nn.Dropout(dropout / 2)\n",
    "            self.out = nn.Linear(hidden, n_dim)\n",
    "            self.out2 = nn.Linear(l_dim, n_dim)\n",
    "\n",
    "            self.out_flat = nn.Linear(hidden, n_dim * n_part)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        if self.fc:\n",
    "            x = x.reshape(len(x), self.n_part * self.n_dim)\n",
    "            x = self.embbed_flat(x)\n",
    "            x = leaky_relu(self.flat_hidden(x))\n",
    "            #             x = self.dropout(x)\n",
    "            x = self.flat_out(x)\n",
    "            x = x.reshape(len(x), self.n_part, self.n_dim)\n",
    "        else:\n",
    "            x = self.embbed(x)\n",
    "            x = self.encoder(x, src_key_padding_mask=mask)\n",
    "            if not self.no_hidden == True:\n",
    "                x = leaky_relu(self.hidden(x))\n",
    "                x = self.dropout(x)\n",
    "                x = leaky_relu(self.hidden2(x))\n",
    "                x = self.dropout(x)\n",
    "                x = self.out(x)\n",
    "            elif self.no_hidden == \"more\":\n",
    "                x = leaky_relu(self.hidden(x))\n",
    "                x = self.dropout(x)\n",
    "                x = leaky_relu(self.hidden2(x))\n",
    "                x = self.dropout(x)\n",
    "                x = leaky_relu(self.hidden3(x))\n",
    "                x = self.dropout(x)\n",
    "\n",
    "            else:\n",
    "                x = leaky_relu(x)\n",
    "                x = self.out2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Disc(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dim=3,\n",
    "        l_dim=10,\n",
    "        hidden=300,\n",
    "        num_layers=3,\n",
    "        num_heads=1,\n",
    "        n_part=2,\n",
    "        fc=False,\n",
    "        dropout=0.5,\n",
    "        mass=False,\n",
    "        clf=False,\n",
    "        norm=False,\n",
    "        no_hidden=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes = hidden\n",
    "        self.n_dim = n_dim\n",
    "        #         l_dim=n_dim\n",
    "        self.l_dim = l_dim\n",
    "        self.n_part = n_part\n",
    "        self.fc = fc\n",
    "        self.clf = clf\n",
    "        self.no_hidden = no_hidden\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.use_mass=mass\n",
    "        self.bullshitbingo=False\n",
    "        self.layernorm=nn.LayerNorm(l_dim+mass)\n",
    "        if fc:\n",
    "            self.l_dim *= n_part\n",
    "            self.embbed_flat = nn.Linear(n_dim * n_part, l_dim)\n",
    "            self.flat_hidden = nn.Linear(l_dim, hidden)\n",
    "            self.flat_hidden2 = nn.Linear(hidden, hidden)\n",
    "            self.flat_hidden3 = nn.Linear(hidden, hidden)\n",
    "            self.out = nn.Linear(hidden, 1)\n",
    "        else:\n",
    "            self.embbed=nn.Linear(n_dim,l_dim)\n",
    "            self.encoder=nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.l_dim,nhead=num_heads,dim_feedforward=hidden,dropout=dropout,\n",
    "                                       norm_first=norm,activation=lambda x: leaky_relu(x,0.2),batch_first=True) ,num_layers=num_layers)\n",
    "            self.hidden=nn.Linear(l_dim+int(mass)-self.bullshitbingo,hidden)\n",
    "            self.hidden2=nn.Linear(hidden,hidden)\n",
    "            self.out=nn.Linear(hidden if not no_hidden else l_dim+mass,1)\n",
    "    \n",
    "    def forward(self, x, m=None, mask=None):\n",
    "        if self.fc == True:\n",
    "            x = x.reshape(len(x), self.n_dim * self.n_part)\n",
    "            x = self.embbed_flat(x)\n",
    "            x = leaky_relu(self.flat_hidden(x), 0.2)\n",
    "            x = leaky_relu(self.flat_hidden2(x), 0.2)\n",
    "#         elif self.bullshitbingo:\n",
    "#             x = torch.cat((m.repeat(3,1).reshape(len(m),1,3),x),axis=1)\n",
    "#             x = self.embbed(x)\n",
    "#             x = self.encoder(x)\n",
    "#             if self.clf:\n",
    "#                 x=torch.concat((torch.ones_like(x[:,0,:]).reshape(len(x),1,-1),x),axis=1)\n",
    "#                 x=self.encoder(x)\n",
    "#                 x=x[:,0,:]\n",
    "#             else:\n",
    "#                 x=self.encoder(x)\n",
    "#                 x=torch.mean(x,axis=1)\n",
    "#             if not self.no_hidden:\n",
    "#                 x=leaky_relu(self.hidden(x),0.2)\n",
    "#                 x=leaky_relu(self.hidden2(x),0.2)\n",
    "#             x=self.out(x)\n",
    "# #             x=self.sig(x)\n",
    "        else:\n",
    "            x = self.embbed(x)\n",
    "            if self.clf:   \n",
    "                x=torch.concat((torch.ones_like(x[:,0,:]).reshape(len(x),1,-1),x),axis=1)\n",
    "                x=self.encoder(x)\n",
    "                x=x[:,0,:]\n",
    "            else:\n",
    "                x=self.encoder(x)\n",
    "                x=torch.mean(x,axis=1)\n",
    "            if self.use_mass:\n",
    "                x=torch.concat((m.reshape(len(x),1),x),axis=1)\n",
    "                \n",
    "            if not self.no_hidden:\n",
    "                x=leaky_relu(self.hidden(x),0.2)\n",
    "#                 x=self.layernorm(x)\n",
    "#                 x=leaky_relu(self.hidden2(x),0.2)\n",
    "        x=self.out(x)\n",
    "#             x=self.sig(x)\n",
    "        return x\n",
    "\n",
    "def mass(data, mask=None, canonical=False):\n",
    "    if len(data.shape)==2:\n",
    "        if not mask==None:\n",
    "            data = data.reshape(len(data),data.shape[1]//3, 3) * mask.reshape(len(data), data.shape[1]//3, 1)\n",
    "        else:\n",
    "            data = data.reshape(len(data),data.shape[1]//3, 3)\n",
    "    if canonical:        \n",
    "        p = data\n",
    "        px = p[:, :, 0]\n",
    "        py = p[:, :, 1]\n",
    "        pz = p[:, :, 2]\n",
    "    else: \n",
    "        p = data\n",
    "        px = torch.cos(p[:, :, 1]) * p[:, :, 2]\n",
    "        py = torch.sin(p[:, :, 1]) * p[:, :, 2]\n",
    "        pz = torch.sinh(p[:, :, 0]) * p[:, :, 2]\n",
    "    \n",
    "    E = torch.sqrt(px**2 + py**2 + pz**2)\n",
    "    E = E.sum(axis=1) ** 2\n",
    "    p = px.sum(axis=1) ** 2 + py.sum(axis=1) ** 2 + pz.sum(axis=1) ** 2\n",
    "    m2 = E - p\n",
    "    return torch.sqrt(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292f9dd-3043-473c-8502-d350ca342c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "       def __init__(self, mu):\n",
    "           self.mu = mu\n",
    "           self.shadow = {}\n",
    "\n",
    "       def register(self, name, val):\n",
    "           self.shadow[name] = val.clone()\n",
    "\n",
    "       def __call__(self, name, x):\n",
    "           assert name in self.shadow\n",
    "           new_average = (1.0 - self.mu) * x + self.mu * self.shadow[name]\n",
    "           self.shadow[name] = new_average.clone()\n",
    "           return new_average\n",
    "\n",
    "   ema = EMA(0.999)\n",
    "   for name, param in model.named_parameters():\n",
    "       if param.requires_grad:\n",
    "           ema.register(name, param.data)\n",
    "\n",
    "  # in batch training loop\n",
    "  # for batch in batches:\n",
    "       optimizer.step()\n",
    "       for name, param in model.named_parameters():\n",
    "           if param.requires_grad:\n",
    "                param.data = ema(name, param.data)\n",
    "class TransGan(pl.LightningModule):\n",
    "    def create_resnet(self, in_features, out_features):\n",
    "        \"\"\"This is the network that outputs the parameters of the invertible transformation\n",
    "        The only arguments can be the in dimension and the out dimenson, the structure\n",
    "        of the network is defined over the config which is a class attribute\n",
    "        Context Features: Amount of features used to condition the flow - in our case\n",
    "        this is usually the mass\n",
    "        num_blocks: How many Resnet blocks should be used, one res net block is are 1 input+ 2 layers\n",
    "        and an additive skip connection from the first to the third\"\"\"\n",
    "        c = self.config[\"context_features\"]\n",
    "        return nets.ResidualNet(\n",
    "            in_features,\n",
    "            out_features,\n",
    "            hidden_features=self.config[\"network_nodes_nf\"],\n",
    "            context_features=c,\n",
    "            num_blocks=self.config[\"network_layers_nf\"],\n",
    "            activation=self.config[\"activation\"] if \"activation\" in self.config.keys() else FF.relu,\n",
    "            dropout_probability=self.config[\"dropout\"] if \"dropout\" in self.config.keys() else 0,\n",
    "            use_batch_norm=self.config[\"batchnorm\"] if \"batchnorm\" in self.config.keys() else 0,\n",
    "        )\n",
    "\n",
    "    def __init__(self, config, hyperopt, num_batches):\n",
    "        \"\"\"This initializes the model and its hyperparameters\"\"\"\n",
    "        super().__init__()\n",
    "        self.start = time.time()\n",
    "        # self.batch_size=batch_size\n",
    "        # print(batch_size)\n",
    "        self.config = config\n",
    "        self.automatic_optimization = False\n",
    "        # Loss function of the Normalizing flows\n",
    "        self.n_part = config[\"n_part\"]\n",
    "        self.n_dim = config[\"n_dim\"]\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.flows = []\n",
    "\n",
    "        self.num_batches = int(num_batches)\n",
    "        self.build_flow()\n",
    "        self.gen_net = Gen(\n",
    "            n_dim=self.n_dim,\n",
    "            hidden=config[\"hidden\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            no_hidden=config[\"no_hidden\"],\n",
    "            fc=config[\"fc\"],\n",
    "            n_part=config[\"n_part\"],\n",
    "            l_dim=config[\"l_dim\"],\n",
    "            num_heads=config[\"heads\"],\n",
    "            norm=config[\"norm\"]\n",
    "        ).cuda()\n",
    "        self.dis_net = Disc(\n",
    "            n_dim=self.n_dim,\n",
    "            hidden=config[\"hidden\"],\n",
    "            l_dim=config[\"l_dim\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            mass=config[\"mass\"],\n",
    "            num_heads=config[\"heads\"],\n",
    "            fc=config[\"fc\"],\n",
    "            n_part=config[\"n_part\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            clf=config[\"clf\"],\n",
    "            no_hidden=config[\"no_hidden\"],\n",
    "            norm=config[\"norm\"], \n",
    "        ).cuda()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fpnds = []\n",
    "        \n",
    "        for p in self.dis_net.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal_(p)\n",
    "        for p in self.gen_net.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal_(p)\n",
    "        \n",
    "        self.gen_net.out.weight.data.fill_(0)#uniform_(-0.01,0.01)\n",
    "        self.gen_net.out.bias.data.fill_(0)\n",
    "        self.nf_train = True\n",
    "        self.train_nf_epochs = config[\"max_epochs\"] // config[\"frac_pretrain\"]\n",
    "\n",
    "    def load_datamodule(self, data_module):\n",
    "        \"\"\"needed for lightning training to work, it just sets the dataloader for training and validation\"\"\"\n",
    "        self.data_module = data_module\n",
    "    \n",
    "    def plot_mass(self,m_f,m_t=None,postfix=\"\"):\n",
    "        fig=plt.figure()\n",
    "        m_f=m_f.cpu().detach().numpy()\n",
    "        if not m_t==None:\n",
    "            m_t=m_t.cpu().detach().numpy()\n",
    "            max_m=np.min([np.quantile(m_t,0.95),np.quantile(m_f,0.95)])\n",
    "            _,bins,_=plt.hist(m_t,bins=np.linspace(0,max_m,30),label=\"True\",histtype=\"step\")\n",
    "            plt.hist(m_f,bins=bins,label=\"Fake\",histtype=\"step\")\n",
    "        else:\n",
    "            plt.hist(m_f[m_f<np.quantile(m_f.cpu().numpy(),0.95)],bins=30,label=\"Fake\",histtype=\"step\")\n",
    "        plt.legend()\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.xlabel(\"Critic Score\")\n",
    "        self.logger.experiment.add_figure(\"train_mass\"+postfix, fig, global_step=self.current_epoch)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_class(self,pred_real,pred_fake,mask):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(pred_real.detach().cpu().numpy(), label=\"real\", bins=30, histtype=\"step\",density=1)\n",
    "        ax.hist(pred_fake.detach().cpu().numpy(), label=\"fake\", bins=30, histtype=\"step\",density=1)\n",
    "#         ax.hist(pred_real[mask.sum(1)].detach().cpu().numpy(), label=\"real<30\", bins=np.linspace(0, 1, 30) if not self.wgan else 30, histtype=\"step\",density=1)\n",
    "#         ax.hist(pred_fake[mask.sum(1)].detach().cpu().numpy(), label=\"fake<30\", bins=np.linspace(0, 1, 30) if not self.wgan else 30, histtype=\"step\",density=1)\n",
    "        ax.legend()\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.xlabel(\"Critic Score\")\n",
    "        self.logger.experiment.add_figure(\"class_train\", fig, global_step=self.current_epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def on_after_backward(self) -> None:\n",
    "        \"\"\"This is a genious little hook, sometimes my model dies, i have no clue why. This saves the training from crashing and continues\"\"\"\n",
    "        valid_gradients = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                valid_gradients = not (torch.isnan(param.grad).any() or torch.isinf(param.grad).any())\n",
    "                if not valid_gradients:\n",
    "                    break\n",
    "        if not valid_gradients:\n",
    "            \n",
    "            self.zero_grad()\n",
    "            self.counter += 1\n",
    "            if self.counter > 8:\n",
    "                raise ValueError(\"5 nangrads in a row\")\n",
    "        else:\n",
    "            self.counter = 0\n",
    "\n",
    "    def build_flow(self):\n",
    "        K = self.config[\"coupling_layers\"]\n",
    "        for i in range(K):\n",
    "            \"\"\"This creates the masks for the coupling layers, particle masks are masks\n",
    "            created such that each feature particle (eta,phi,pt) is masked together or not\"\"\"\n",
    "            mask = create_random_binary_mask(self.n_dim * self.n_part)\n",
    "            self.flows += [\n",
    "                PiecewiseRationalQuadraticCouplingTransform(\n",
    "                    mask=mask,\n",
    "                    transform_net_create_fn=self.create_resnet,\n",
    "                    tails=\"linear\",\n",
    "                    tail_bound=self.config[\"tail_bound\"],\n",
    "                    num_bins=self.config[\"bins\"],)]\n",
    "        self.q0 = nf.distributions.normal.StandardNormal([self.n_dim * self.n_part])\n",
    "        # Creates working flow model from the list of layer modules\n",
    "        self.flows = CompositeTransform(self.flows)\n",
    "        # Construct flow model\n",
    "        self.flow = base.Flow(distribution=self.q0, transform=self.flows)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.losses = []\n",
    "        # mlosses are initialized with None during the time it is not turned on, makes it easier to plot\n",
    "        opt_nf = torch.optim.AdamW(self.flow.parameters(), lr=self.config[\"lr_nf\"])\n",
    "        if self.config[\"opt\"] == \"Adam\":\n",
    "            opt_g = torch.optim.Adam(self.gen_net.parameters(), lr=self.config[\"lr_g\"], betas=(0, 0.99))\n",
    "            opt_d = torch.optim.Adam(self.dis_net.parameters(), lr=self.config[\"lr_d\"], betas=(0, 0.99))\n",
    "        elif self.config[\"opt\"] == \"AdamW\":\n",
    "            opt_g = torch.optim.AdamW(self.gen_net.parameters(), lr=self.config[\"lr_g\"], betas=(0, 0.99))\n",
    "            opt_d = torch.optim.AdamW(self.dis_net.parameters(), lr=self.config[\"lr_d\"], betas=(0, 0.99))\n",
    "        else:\n",
    "            opt_g = torch.optim.RMSprop(self.gen_net.parameters(), lr=self.config[\"lr_g\"])\n",
    "            opt_d = torch.optim.RMSprop(self.dis_net.parameters(), lr=self.config[\"lr_d\"])\n",
    "        if self.config[\"sched\"] == \"cosine\":\n",
    "            lr_scheduler_nf = CosineWarmupScheduler(opt_nf, warmup=1, max_iters=10000000 * self.config[\"freq\"])\n",
    "            max_iter_d = (self.config[\"max_epochs\"]) * self.num_batches\n",
    "            max_iter_g = (self.config[\"max_epochs\"] - self.train_nf_epochs//2) * self.num_batches // self.freq_d\n",
    "            lr_scheduler_d = CosineWarmupScheduler(opt_d, warmup=15 * self.num_batches, max_iters=max_iter_d)\n",
    "            lr_scheduler_g = CosineWarmupScheduler(opt_g, warmup=15 * self.num_batches // self.freq_d, max_iters=max_iter_g)\n",
    "        else: \n",
    "            lr_scheduler_nf = None\n",
    "            lr_scheduler_d = None\n",
    "            lr_scheduler_g = None\n",
    "        if self.config[\"sched\"] != None:\n",
    "            return [opt_nf, opt_d, opt_g], [lr_scheduler_nf, lr_scheduler_d, lr_scheduler_g]\n",
    "        else:\n",
    "            return [opt_nf, opt_d, opt_g]\n",
    "\n",
    "    def sample_n(self, mask):\n",
    "        #Samples a mask where the zero padded particles are True, rest False\n",
    "        mask_test = torch.ones_like(mask)\n",
    "        n, counts = np.unique(self.data_module.n, return_counts=True)\n",
    "        counts_prob = torch.tensor(counts / len(self.data_module.n) )\n",
    "        n_test=n[torch.multinomial(counts_prob,replacement=True,num_samples=(len(mask)))] \n",
    "        indices = torch.arange(self.n_part, device=mask.device)\n",
    "        mask_test = (indices.view(1, -1) < torch.tensor(n_test).view(-1, 1))      \n",
    "        mask_test=~mask_test.bool()\n",
    "        return (mask_test)\n",
    "\n",
    "    def scale(self, x,m): \n",
    "        x = x.reshape(len(x), self.n_part, self.n_dim)\n",
    "        # self.data_module.scaler = self.data_module.scaler.to(x.device)\n",
    "        for i in range(self.n_part):\n",
    "            if self.config[\"quantile\"]:\n",
    "                x[~m[:,i] ,i, 2] = torch.tensor(self.data_module.ptscalers[i].inverse_transform(x[~m[:,i] ,i, 2].cpu().numpy().reshape(-1,1)).reshape(-1)).to(x.device)\n",
    "                x[~m[:,i] ,i, :2] = self.data_module.scalers[i].inverse_transform(x[~m[:,i] ,i, :2].float())\n",
    "            else:\n",
    "                x[~m[:,i],i,:]= self.data_module.scalers[i].inverse_transform(x[~m[:,i],i,:])\n",
    "        return x\n",
    "\n",
    "    def sampleandscale(self, batch, mask, mask_test=None, scale=False):\n",
    "        \"\"\"This is a helper function that samples from the flow (i.e. generates a new sample)\n",
    "        and reverses the standard scaling that is done in the preprocessing. This allows to calculate the mass\n",
    "        on the generative sample and to compare to the simulated one, we need to inverse the scaling before calculating the mass\n",
    "        because calculating the mass is a non linear transformation and does not commute with the mass calculation\"\"\"\n",
    "        mask=mask.bool()\n",
    "        if mask_test!=None:\n",
    "            mask_test=mask_test.bool()\n",
    "        else:\n",
    "            mask_test=mask   \n",
    "        z = self.flow.sample(len(batch) if not self.config[\"context_features\"] else 1, context=None if not self.config[\"context_features\"]\n",
    "                            else (~mask_test).sum(1).float().reshape(-1,1)).reshape(len(batch), self.n_part, self.n_dim)\n",
    "        fake = z + self.gen_net(z, mask=mask_test)  # (1-self.alpha)*\n",
    "        fake = fake.reshape(len(batch), self.n_part, self.n_dim)\n",
    "        if scale:\n",
    "            fake_scaled, z_scaled, true = (self.scale(fake.detach().clone(),mask_test).to(batch.device),\n",
    "                                        self.scale(z.detach().clone(),mask_test).to(batch.device), \n",
    "                                        self.scale(batch.detach().clone(),mask).to(batch.device))\n",
    "            true = true * (~mask.reshape(len(batch), self.n_part, 1))\n",
    "            z_scaled = z_scaled * (~mask_test.reshape(len(batch), self.n_part, 1))\n",
    "            fake_scaled = fake_scaled * (~mask_test.reshape(len(batch), self.n_part, 1))\n",
    "            return fake.to(batch.device), fake_scaled, true, z_scaled\n",
    "        else:\n",
    "            return fake,z.detach()\n",
    "\n",
    "    def train_disc(self,  batch,mask,opt_d,sched_d=None):  \n",
    "        batch = batch.reshape(len(batch), self.n_part, self.n_dim)\n",
    "        fake = self.sampleandscale(batch, mask, scale=False)\n",
    "        fake = fake.detach()\n",
    "        if self.config[\"mass\"]:\n",
    "            m_t = mass(batch, mask=~mask, canonical=self.config[\"canonical\"])\n",
    "            m_f = mass(fake, mask=~mask, canonical=self.config[\"canonical\"])\n",
    "        pred_real = self.dis_net(batch, m=None if not self.config[\"mass\"] else m_t, mask=mask)\n",
    "        pred_fake = self.dis_net(fake, m=None if not self.config[\"mass\"] else m_f, mask=mask)\n",
    "        if self.config[\"loss\"]==\"lsgan\":            \n",
    "            target_real = torch.ones_like(pred_real)\n",
    "            target_fake = -torch.ones_like(pred_fake)\n",
    "            pred = torch.vstack((pred_real, pred_fake))\n",
    "            target = torch.vstack((target_real, target_fake))\n",
    "            d_loss = nn.MSELoss()(pred, target).mean()\n",
    "        elif self.config[\"loss\"]==\"wgan\":\n",
    "            d_loss=-torch.mean(pred_real)+torch.mean(pred_fake)\n",
    "        if self.config[\"gp\"]:\n",
    "            gp=self.compute_gradient_penalty(batch, fake, pred_real, pred_fake)\n",
    "            d_loss+=gp\n",
    "        self.log(\"d_loss\", d_loss, logger=True, prog_bar=True)\n",
    "        self.dis_net.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.zero_grad()\n",
    "        if self.current_epoch%10==0 and self.config[\"mass\"]:\n",
    "            self.plot_mass(m_t,m_f)\n",
    "        torch.nn.utils.clip_grad_norm_(self.dis_net.parameters(), 5.)\n",
    "        opt_d.step()\n",
    "        return pred_real,pred_fake\n",
    "\n",
    "    def train_gen(self,batch,mask,opt_g,sched_g=None):\n",
    "        fake,z = self.sampleandscale(batch, mask, scale=False)\n",
    "        m_f = mass(fake, mask=~mask)\n",
    "        pred_fake = self.dis_net(fake, None if not self.config[\"mass\"] else m_f, mask=mask)\n",
    "        if self.config[\"loss\"]==\"lsgan\":\n",
    "            target_real = torch.zeros_like(pred_fake)\n",
    "            g_loss = nn.MSELoss()((pred_fake.view(-1)), target_real.view(-1))  \n",
    "        if self.config[\"loss\"]==\"wganls\":\n",
    "            batch1, batch2 = batch[:self.config[\"batch_size\"]//2], batch[self.config[\"batch_size\"]:]\n",
    "            fake1, fake2 = fake[:self.config[\"batch_size\"]//2], fake[self.config[\"batch_size\"]:]\n",
    "            z1, z2 = z[:self.config[\"batch_size\"]//2], z[self.config[\"batch_size\"]//2:]\n",
    "            lz = torch.mean(torch.abs(fake2 - fake1)) / torch.mean(\n",
    "            torch.abs(z_random2 - z_random1))\n",
    "            eps = 1 * 1e-5\n",
    "            loss_lz = 1 / (lz + eps)\n",
    "            g_loss = -torch.mean(pred_fake) + loss_lz\n",
    "        self.gen_net.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        if self.global_step > 10:\n",
    "            opt_g.step()\n",
    "        else:\n",
    "            opt_g.zero_grad()\n",
    "        self.log(\"g_loss\", g_loss, logger=True, prog_bar=True)\n",
    "        if self.global_step == 3:\n",
    "            print(\"passed test gen\")\n",
    "        if self.current_epoch%10==0:\n",
    "            self.plot_mass(m_f,postfix=\"2\")\n",
    "        if False:\n",
    "            ema_nimg = 500 * 1000\n",
    "            cur_nimg = self.config[\"batch_size\"] * 1 * self.global_steps \n",
    "            ema_nimg = min(ema_nimg, cur_nimg * 0.01)\n",
    "            ema_beta = 0.5 ** (float(self.config[\"batch_size\"] * 1) / max(ema_nimg, 1e-8))\n",
    "        else:\n",
    "            ema_beta=0.9999\n",
    "            # moving average weight\n",
    "            for p, avg_p in zip(gen_net.parameters(), gen_avg_param):\n",
    "                cpu_p = deepcopy(p)\n",
    "                avg_p.mul_(ema_beta).add_(1. - ema_beta, cpu_p.cpu().data)\n",
    "                del cpu_p\n",
    "\n",
    "   \n",
    "    def _summary(self, temp):\n",
    "        self.summary_path = \"/beegfs/desy/user/{}/{}/summary.csv\".format(os.environ[\"USER\"], self.config[\"name\"])\n",
    "        if os.path.isfile(self.summary_path):\n",
    "            summary = pd.read_csv(self.summary_path).set_index([\"path_index\"])\n",
    "        else:\n",
    "            print(\"summary not found\")\n",
    "            summary = pd.DataFrame()\n",
    "        summary.loc[self.logger.log_dir, self.config.keys()] = self.config.values()\n",
    "        summary.loc[self.logger.log_dir, temp.keys()] = temp.values()\n",
    "        summary.loc[self.logger.log_dir, \"time\"] = time.time() - self.start\n",
    "        summary.to_csv(self.summary_path, index_label=[\"path_index\"])\n",
    "        return summary\n",
    "\n",
    "    def _results(self, temp):\n",
    "        self.metrics[\"step\"].append(self.current_epoch)\n",
    "        self.df = self.df.append(pd.DataFrame([temp], index=[self.current_epoch]))\n",
    "        self.df.to_csv(self.logger.log_dir + \"result.csv\", index_label=[\"index\"])\n",
    "    def compute_gradient_penalty(self, real, fake, pred_real, pred_fake):\n",
    "        k = 2\n",
    "        p = 6\n",
    "        real = real.reshape(len(real),self.n_part,self.n_dim)\n",
    "        fake = fake.reshape(len(real),self.n_part,self.n_dim)\n",
    "        fake = Variable(fake, requires_grad=True)\n",
    "        real = Variable(real, requires_grad=True)\n",
    "        m_t=mass(real.reshape(len(real),self.n_part*self.n_dim))\n",
    "        m_f=mass(fake.reshape(len(real),self.n_part*self.n_dim))\n",
    "        pred_real = self.dis_net(real, None if not self.config[\"mass\"] else m_t)\n",
    "        pred_fake = self.dis_net(fake, None if not self.config[\"mass\"] else m_f)\n",
    "        real_grad_out = Variable(torch.cuda.FloatTensor(real.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        real_grad = autograd.grad(pred_real, real, grad_outputs=real_grad_out, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "        fake_grad_out = Variable(torch.cuda.FloatTensor(fake.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake_grad = autograd.grad(pred_fake, fake, grad_outputs=fake_grad_out, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "        div_gp = torch.mean(real_grad_norm + fake_grad_norm) * k / 2\n",
    "        return div_gp\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"training loop of the model, here all the data is passed forward to a gaussian\n",
    "        This is the important part what is happening here. This is all the training we do\"\"\"\n",
    "        batch = batch.cuda()\n",
    "        mask = batch[:, self.n_part*self.n_dim:].bool()\n",
    "        batch = batch[:, :self.n_part*self.n_dim]\n",
    "        opt_nf, opt_d, opt_g = self.optimizers()\n",
    "        if self.config[\"sched\"]:\n",
    "            sched_nf, sched_d, sched_g = self.lr_schedulers()\n",
    "        # ### NF PART\n",
    "        if self.config[\"sched\"] != None:\n",
    "            self.log(\"lr_g\", sched_g.get_last_lr()[-1], logger=True, on_epoch=True)\n",
    "            self.log(\"lr_nf\", sched_nf.get_last_lr()[-1], logger=True, on_epoch=True)\n",
    "            self.log(\"lr_d\", sched_d.get_last_lr()[-1], logger=True, on_epoch=True)\n",
    "\n",
    "        # Only train nf for first few epochs\n",
    "        if self.current_epoch < self.train_nf:\n",
    "            if self.config[\"sched\"] != None:\n",
    "                sched_nf.step()\n",
    "            nf_loss = -self.flow.to(self.device).log_prob(batch,context=(~mask).sum(1).reshape(-1,1).float() if self.config[\"context_features\"] else None).mean()\n",
    "            nf_loss /= self.n_dim * self.n_part\n",
    "            self.flow.zero_grad()\n",
    "            self.manual_backward(nf_loss)\n",
    "            opt_nf.step()\n",
    "            self.log(\"logprob\", nf_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
    "\n",
    "        # GAN PART\n",
    "\n",
    "        if self.config[\"sched\"]:\n",
    "            sched_d.step()\n",
    "    \n",
    "        if self.current_epoch == self.train_nf//2:\n",
    "            print(\"start gan training\")\n",
    "        pred_real,pred_fake=self.train_disc(batch=batch,mask=mask,opt_d=opt_d,sched_d=sched_d if self.config[\"sched\"] else None )\n",
    "        if (self.current_epoch > self.train_nf//2  and self.global_step % self.freq_d < 2) or self.global_step <= 3:\n",
    "            \n",
    "            if self.config[\"sched\"]:\n",
    "                sched_g.step()\n",
    "            self.train_gen(batch=batch,mask=mask,opt_g=opt_g,sched_g=sched_g if self.config[\"sched\"] else None )\n",
    "        # Control plot train\n",
    "        if self.current_epoch % 5 == 0 and self.current_epoch > self.train_nf / 2:\n",
    "            self.plot_class(pred_real=pred_real,pred_fake=pred_fake,mask=mask)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"This calculates some important metrics on the hold out set (checking for overtraining)\"\"\"\n",
    "\n",
    "        mask = batch[:, self.n_part*self.n_dim:].cpu().bool()\n",
    "        mask_test = self.sample_n(mask).bool()\n",
    "        batch = batch[:, :self.n_part*self.n_dim].cpu()\n",
    "        self.dis_net.train()\n",
    "        self.gen_net.train()\n",
    "        self.flow.train()\n",
    "        # self.data_module.scaler.to(\"cpu\")\n",
    "        batch = batch.to(\"cpu\")\n",
    "        self.flow = self.flow.to(\"cpu\")\n",
    "        self.dis_net = self.dis_net.cpu()\n",
    "        self.gen_net = self.gen_net.cpu()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen, fake_scaled, true_scaled, z_scaled = self.sampleandscale(batch,mask, mask_test, scale=True)\n",
    "            if self.config[\"mass\"]:\n",
    "                m_t = mass(batch, mask=~mask, canonical=self.config[\"canonical\"])\n",
    "                m_f = mass(gen, mask=~mask_test, canonical=self.config[\"canonical\"])\n",
    "                self.plot_mass(m_t,m_f)\n",
    "                \n",
    "#                 fig=plt.figure()\n",
    "#                 max_m=np.min(np.quantile(m_t.numpy(),0.95),np.quantile(m_f.numpy(),0.95))\n",
    "#                 _,bins,_=plt.hist(m_t.detach().numpy(),bins=np.linspace(0,max_m,30),label=\"True\")\n",
    "#                 plt.hist(m_f.cpu().detach().numpy(),bins=bins,label=\"Fake\")\n",
    "#                 plt.legend()\n",
    "#                 self.logger.experiment.add_figure(\"train_mass\", fig, global_step=self.current_epoch)\n",
    "#                 plt.close()\n",
    "        pred_real = self.dis_net(batch.reshape(len(batch), self.n_part, self.n_dim), m=None if not self.config[\"mass\"] else m_t, mask=mask)\n",
    "        pred_fake = self.dis_net(gen, m=None if not self.config[\"mass\"] else m_f, mask=mask_test)\n",
    "      \n",
    "        target_real = torch.ones_like(pred_real)\n",
    "        target_fake = torch.zeros_like(pred_fake)\n",
    "        pred = torch.vstack((pred_real, pred_fake))\n",
    "        target = torch.vstack((target_real, target_fake))\n",
    "        d_loss = nn.MSELoss()(pred, target).mean()\n",
    "        g_loss = nn.MSELoss()((pred_fake.view(-1)), target_real.view(-1))\n",
    "        true_scaled=true_scaled.reshape(-1,self.n_part,self.n_dim)*(~mask).reshape(-1,self.n_part,1)\n",
    "        fake_scaled=fake_scaled.reshape(-1,self.n_part,3)*(~mask_test).reshape(-1,self.n_part,1)\n",
    "        z_scaled=(z_scaled.reshape(-1,self.n_part,3)*(~mask_test).reshape(-1,self.n_part,1)).reshape(-1,self.n_dim*self.n_part)\n",
    "        true_scaled, fake_scaled, z_scaled = (true_scaled.reshape(-1, self.n_dim*self.n_part), fake_scaled.reshape(-1, self.n_dim*self.n_part), z_scaled.reshape(-1, self.n_dim*self.n_part))\n",
    "        # Reverse Standard Scaling (this has nothing to do with flows, it is a standard preprocessing step)\n",
    "        m_t = mass(true_scaled, canonical=self.config[\"canonical\"], mask=~mask).cpu()\n",
    "        m_gen = mass(z_scaled, mask=~mask_test, canonical=self.config[\"canonical\"]).cpu()\n",
    "        m_c = mass(fake_scaled, mask=~mask_test, canonical=self.config[\"canonical\"]).cpu()\n",
    "        fake_scaled=fake_scaled.reshape(-1,self.n_part,self.n_dim)\n",
    "        true_scaled=true_scaled.reshape(-1,self.n_part,self.n_dim)\n",
    "        z_scaled=z_scaled.reshape(-1,self.n_part,self.n_dim)\n",
    "\n",
    "        for i in range(self.n_part):\n",
    "            # gen[gen[:,i]<0,i]=0\n",
    "            z_scaled[z_scaled[:, i,2] < 0,i, 2] = 0\n",
    "            fake_scaled[fake_scaled[:, i,2] < 0,i, 2] = 0\n",
    "            true_scaled[true_scaled[:, i,2] < 0,i, 2] = 0\n",
    "        # metrics\n",
    "\n",
    "        cov, mmd = cov_mmd(fake_scaled, true_scaled, use_tqdm=False)\n",
    "        nfcov, nfmmd = cov_mmd(z_scaled, true_scaled, use_tqdm=False)\n",
    "        try:\n",
    "            fpndnf = fpnd(z_scaled.numpy(), use_tqdm=False, jet_type=self.config[\"parton\"]) \n",
    "            fpndv = fpnd(fake_scaled.numpy(), use_tqdm=False, jet_type=self.config[\"parton\"])\n",
    "        except:\n",
    "            fpndv = 1000\n",
    "            fpndnf=1000\n",
    "        self.fpnds.append(fpndv)\n",
    "#         if (np.array(self.fpnds)[-10:] > 5).all() and self.current_epoch > 200:\n",
    "#             print(\"fpnd to high, stop training\")\n",
    "#             raise\n",
    "        w1m_ = w1m(fake_scaled.reshape(len(batch), self.n_part, self.n_dim), true_scaled.reshape(len(batch), self.n_part, self.n_dim))[0]\n",
    "        w1p_ = w1p(fake_scaled.reshape(len(batch), self.n_part, self.n_dim), true_scaled.reshape(len(batch), self.n_part, self.n_dim))[0]\n",
    "        w1efp_ = w1efp(fake_scaled.reshape(len(batch), self.n_part, self.n_dim), true_scaled.reshape(len(batch), self.n_part, self.n_dim))[0]\n",
    "\n",
    "        temp = {\n",
    "            \"val_fpnd\": fpndv,\n",
    "            \"val_fpnd_nf\": fpndnf,\n",
    "            \"val_mmd\": mmd,\n",
    "            \"val_cov\": cov,\n",
    "            \"val_nfcov\": nfcov,\n",
    "            \"val_w1m\": w1m_,\n",
    "            \"val_w1efp\": w1efp_,\n",
    "            \"val_w1p\": w1p_,\n",
    "            \"step\": self.global_step,\n",
    "            \"g_loss\": float(g_loss.numpy()),\n",
    "            \"d_loss\": float(d_loss.numpy()),\n",
    "        }\n",
    "        print(\"epoch {}: \".format(self.current_epoch), temp)\n",
    "        if self.hyperopt and self.global_step > 3:\n",
    "\n",
    "            # if self.current_epoch < self.train_nf:\n",
    "            with torch.no_grad():\n",
    "                logprob = -self.flow.log_prob(batch,context=(~mask).sum(1).reshape(-1,1).float() if self.config[\"context_features\"] else None).mean() / 90\n",
    "            self.log(\"val_logprob\", logprob, prog_bar=True, logger=True)\n",
    "            temp[\"val_logprob\"] = float(logprob.numpy())\n",
    "            try:\n",
    "                self._results(temp)\n",
    "            except:\n",
    "                print(\"error in results\")\n",
    "            self._summary(temp)\n",
    "\n",
    "        self.log(\"hp_metric\", w1m_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1m\", w1m_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1p\", w1p_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1efp\", w1efp_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_cov\", cov, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_nfcov\", nfcov, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_nfmmd\", nfmmd, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_fpnd\", fpndv, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_fpnd_nf\", fpndnf, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mmd\", mmd, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        true_scaled, fake_scaled, z_scaled = (true_scaled.reshape(-1,self.n_part*self.n_dim), fake_scaled.reshape(-1, self.n_part*self.n_dim), z_scaled.reshape(-1, self.n_part*self.n_dim))\n",
    "        self.plot = plotting(\n",
    "            model=self, gen=z_scaled, gen_corr=fake_scaled, true=true_scaled, config=self.config, step=self.global_step, logger=self.logger.experiment\n",
    "        )\n",
    "        try:\n",
    "            self.plot.plot_mass(m=m_gen.cpu().numpy(), m_t=m_t.cpu().numpy(), m_c=m_c.cpu().numpy(), save=True, bins=50, quantile=True, plot_vline=False)\n",
    "            self.plot.plot_class(pred_fake=pred_fake, pred_real=pred_real, bins=50, step=self.current_epoch)\n",
    "            # self.plot.plot_2d(save=True)\n",
    "        #             self.plot.var_part(true=true[:,:self.n_dim],gen=gen_corr[:,:self.n_dim],true_n=n_true,gen_n=n_gen_corr,m_true=m_t,m_gen=m_test ,save=True)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "        self.flow = self.flow.to(\"cuda\")\n",
    "        self.gen_net = self.gen_net.to(\"cuda\")\n",
    "        self.dis_net = self.dis_net.to(\"cuda\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetnet",
   "language": "python",
   "name": "jetnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
