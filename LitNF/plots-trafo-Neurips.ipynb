{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148e9d34-e26d-4a67-acf8-48081ff27e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thx max\n",
      "good boy\n"
     ]
    }
   ],
   "source": [
    "import torchinfo\n",
    "import torch\n",
    "from jetnet_dataloader import JetNetDataloader\n",
    "from lit_nf import TransGan\n",
    "from plotting import *\n",
    "import pandas as pd\n",
    "from jetnet.evaluation import w1p, w1efp, w1m, cov_mmd,fpnd\n",
    "from jetnet.datasets import JetNet\n",
    "from main import train\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import hist\n",
    "import mplhep as hep\n",
    "import torch\n",
    "import numpy as np\n",
    "import hist\n",
    "from hist import Hist\n",
    "import traceback\n",
    "from helpers import mass\n",
    "import pandas as pd\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "class plotting_paper():\n",
    "    '''This is a class that takes care of  plotting steps in the script,\n",
    "        It is initialized with the following arguments:\n",
    "        true=the simulated data, note that it needs to be scaled\n",
    "        gen= Generated data , needs to be scaled\n",
    "        step=The current step of the training, this is need for tensorboard\n",
    "        model=the model that is trained, a bit of an overkill as it is only used to access the losses\n",
    "        config=the config used for training\n",
    "        logger=The logger used for tensorboard logging'''\n",
    "    def __init__(self,true,gen,config,step,p,model=None,logger=None,weight=1):\n",
    "        self.config=model.config\n",
    "        self.n_dim=self.config[\"n_dim\"]\n",
    "        self.gen=gen\n",
    "        self.test_set=true\n",
    "        self.step=step\n",
    "        self.model=model\n",
    "        self.p=p\n",
    "\n",
    "        self.weight=weight\n",
    "        if logger is not None:\n",
    "            self.summary=logger\n",
    "    def plot_mass_only(self,m,m_t,bins=15):\n",
    "        fig,ax=plt.subplots(2,1,gridspec_kw={'height_ratios': [3, 1]},figsize=(6,8))\n",
    "        a=min(np.quantile(m_t,0.001),np.quantile(m,0.001))\n",
    "        b=max(np.quantile(m_t,0.999),np.quantile(m,0.999))\n",
    "        a=np.quantile(m_t,0.001)\n",
    "        b=np.quantile(m_t,0.999)\n",
    "        h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "        h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "        bins = h.axes[0].edges\n",
    "        h.fill(m)#,weight=1/self.weight)\n",
    "        h2.fill(m_t)\n",
    "            \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "\n",
    "        main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "            h2,\n",
    "            ax_dict={\"main_ax\":ax[0],\"ratio_ax\":ax[1]},\n",
    "            rp_ylabel=r\"Ratio\",\n",
    "            rp_num_label=\"Flow Generated\",\n",
    "            rp_denom_label=\"MC Simulatied\",\n",
    "            rp_uncert_draw_type=\"line\",  # line or bar\n",
    "        )\n",
    "        ax[0].set_xlabel(\"\")\n",
    "#                 if quantile and v==\"m\" and plot_vline:\n",
    "#                     ax[0,k].hist(m[m_t<np.quantile(m_t,0.1)],histtype='step',bins=bins,alpha=1,color=\"red\",label=\"10% quantile gen\",hatch=\"/\")\n",
    "#                     ax[0,k].vlines(np.quantile(m_t,0.1),0,np.max(h[:]),color=\"red\",label='10% quantile train')\n",
    "\n",
    "        ax[1].set_ylim(0.25,2)\n",
    "        ax[0].set_xlim(a,b)\n",
    "        ax[1].set_xlabel(\"$m_T$\",fontweight=\"bold\")\n",
    "        ax[1].set_xlim(a,b)\n",
    "        ax[0].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "        ax[1].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "  \n",
    "     \n",
    "#             print(\"added figure\")\n",
    "#             self.summary.close()\n",
    "\n",
    "        plt.savefig(\"{}_mass\".format(self.p))\n",
    "        plt.show()\n",
    "\n",
    "    def plot_marginals(self,ith=None,save=False,title=None):\n",
    "        #This plots the marginal distribution for simulation and generation\n",
    "        #Note that this is the data the model sees during training as input to model in the NF\n",
    "        #This is the distribution of one of [eta,phi,pt] of one particle of the n particles per jet: for example the pt of the 3rd particle\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        \n",
    "        plt.switch_backend('agg')\n",
    "        name,label=[\"eta\",\"phi\",\"pt\"],['${\\eta}^{rel}_{7}$',\"${\\phi}^{rel}_{7}$\",\"${p_T}^{rel}_{7}$\"]\n",
    "        fig,ax=plt.subplots(2,3,gridspec_kw={'height_ratios': [3, 1]},figsize=(18,6))\n",
    "        particles=range(self.n_dim) if not ith else [3*ith,3*ith+1,3*ith+2]\n",
    "        plt.suptitle(title,fontweight=\"bold\")\n",
    "        k=0\n",
    "        for i in particles:\n",
    "            if ith:\n",
    "                ax_temp=ax[:,k]\n",
    "            else:\n",
    "                fig,ax_temp=plt.subplots(2,1)\n",
    "            a=np.quantile(self.test_set[:,i].numpy(),0)\n",
    "            b=np.quantile(self.test_set[:,i].numpy(),1)\n",
    "\n",
    "            h=hist.Hist(hist.axis.Regular(15,a,b,label=label[i%3],underflow=False,overflow=False))\n",
    "            h2=hist.Hist(hist.axis.Regular(15,a,b,label=label[i%3],underflow=False,overflow=False))\n",
    "            h.fill(self.gen[:,i].numpy())\n",
    "            h2.fill(self.test_set[:,i].numpy())\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0,k] )\n",
    "       \n",
    "            main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "                h2,\n",
    "                ax_dict={\"main_ax\":ax_temp[0],\"ratio_ax\":ax_temp[1]},\n",
    "                rp_ylabel=r\"Ratio\",\n",
    "#                 rp_xlabel=label[i%3],\n",
    "                rp_num_label=\"Flow Generated\",\n",
    "                rp_denom_label=\"MC Simulated\",\n",
    "                rp_uncert_draw_type=\"line\",  # line or bar\n",
    "            )\n",
    "            \n",
    "            \n",
    "            ax_temp[0].set_xlabel(\"\")\n",
    "            ax_temp[1].set_ylim(0.25,2)\n",
    "            ax_temp[0].set_xlim(a,b)\n",
    "            ax_temp[1].set_xlim(a,b)\n",
    "            ax_temp[1].set_xlabel(label[i%3])\n",
    "            ax_temp[0].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "            ax_temp[1].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "            \n",
    "            \n",
    "            #plt.tight_layout(pad=2)\n",
    "            k+=1\n",
    "        if save:\n",
    "            self.summary.add_figure(\"jet{}_{}\".format(i//3+1,name[i%3]),fig,global_step=self.step)\n",
    "            self.summary.close()\n",
    "        else:\n",
    "            plt.savefig(\"{}_7thpart\".format(self.p))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def plot_2d(self,save=False):\n",
    "        #This creates a 2D histogram of the inclusive distribution for all 3 feature combinations\n",
    "        #Inclusive means that is the distribution of pt of all particles per jet and sample\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        data=self.test_set[:,:self.n_dim].reshape(-1,3).numpy()\n",
    "        gen=self.gen[:,:self.n_dim].reshape(-1,3).numpy()\n",
    "        labels=[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}_7$\",r\"$p_T^{rel}$\"]\n",
    "        names=[\"eta\",\"phi\",\"pt\"]\n",
    "        for index in [[0,1],[0,2],[1,2]]:\n",
    "            \n",
    "            fig,ax=plt.subplots(ncols=2,figsize=(16, 8))\n",
    "            _,x,y,_=ax[0].hist2d(data[:,index[0]],data[:,index[1]],bins=30)\n",
    "            #rebin to only take 5% to 95.0% of signal dis\n",
    "            a=np.quantile(x,0.05)\n",
    "            b=np.quantile(x,0.95)\n",
    "            x=np.linspace(a,b,len(x))\n",
    "            a=np.quantile(y,0.05)\n",
    "            b=np.quantile(y,0.95)\n",
    "            y=np.linspace(a,b,len(y))\n",
    "            if index[1]==2:\n",
    "                y=np.abs(y)+0.00001\n",
    "                y = np.logspace(np.log(y[0]),np.log(y[-1]),len(y))\n",
    "            ax[0].hist2d(data[:,index[0]],data[:,index[1]],bins=[x,y])\n",
    "            data[:,index[0]]=np.abs(data[:,index[0]])+0.00001\n",
    "            ax[1].hist2d(gen[:,index[0]],gen[:,index[1]],bins=[x,y])\n",
    "        \n",
    "        \n",
    "            plt.tight_layout(pad=2)\n",
    "            ax[0].set_xlabel( labels[index[0]],fontweight=\"bold\")\n",
    "            ax[0].set_ylabel( labels[index[1]],fontweight=\"bold\")\n",
    "            \n",
    "            ax[0].set_title(\"MC Simulated\")\n",
    "            ax[1].set_xlabel( labels[index[0]],fontweight=\"bold\")\n",
    "            ax[1].set_ylabel( labels[index[1]],fontweight=\"bold\")\n",
    "            \n",
    "            ax[1].set_title(\"Flow Generated\")\n",
    "           \n",
    "            if save:\n",
    "                self.summary.add_figure(\"2d{}-{}\".format(names[index[0]],names[index[1]]),fig,global_step=self.step)\n",
    "                \n",
    "                # self.summary.close()\n",
    "            else:\n",
    "                plt.savefig(\"{}_2dcorr{}{}\".format(self.p,names[index[0]],names[index[0]]))\n",
    "                plt.show()\n",
    "                \n",
    " \n",
    "        \n",
    "    def oversample(self,m,m_t,weight,save=False,quantile=False,bins=15,plot_vline=False,title=\"\"):\n",
    "        #This creates a histogram of the inclusive distributions and calculates the mass of each jet\n",
    "        #and creates a histogram of that\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        #if quantile, this also creates a histogram of a subsample of the generated data, \n",
    "        # where the mass used to condition the flow is in the first 10% percentile of the simulated mass dist\n",
    "        i=0\n",
    "        k=0\n",
    "        fig,ax=plt.subplots(2,4,gridspec_kw={'height_ratios': [3, 1]},figsize=(20,5))\n",
    "        plt.suptitle(title,fontweight=\"bold\")\n",
    "        for v,name in zip([\"eta\",\"phi\",\"pt\",\"m\"],[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}$\",r\"$p_T^{rel}$\",r\"$m^{rel}$\"]):\n",
    "            \n",
    "            if v!=\"m\":\n",
    "                a=min(np.quantile(self.gen[:,i],0.001),np.quantile(self.test_set[:,i],0.001))\n",
    "                b=max(np.quantile(self.gen[:,i],0.999),np.quantile(self.test_set[:,i],0.999))     \n",
    "                \n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h.fill(self.gen[:,i],weight=1/weight)\n",
    "                h2.fill(self.test_set[:,i])\n",
    "                i+=1\n",
    "            else:\n",
    "                a=min(np.quantile(m_t,0.001),np.quantile(m,0.001))\n",
    "                b=max(np.quantile(m_t,0.999),np.quantile(m,0.999))\n",
    "                a=np.quantile(m_t,0.001)\n",
    "                b=np.quantile(m_t,0.999)\n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                bins = h.axes[0].edges\n",
    "                h.fill(m,weight=1/weight)#,weight=1/self.weight)\n",
    "                h2.fill(m_t)\n",
    "            \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            try:\n",
    "                main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "                    h2,\n",
    "                    ax_dict={\"main_ax\":ax[0,k],\"ratio_ax\":ax[1,k]},\n",
    "                    rp_ylabel=r\"Ratio\",\n",
    "                    rp_num_label=\"Flow Generated\",\n",
    "                    rp_denom_label=\"MC Simulatied\",\n",
    "                    rp_uncert_draw_type=\"line\",  # line or bar\n",
    "                )\n",
    "                ax[0,k].set_xlabel(\"\")\n",
    "#                 if quantile and v==\"m\" and plot_vline:\n",
    "#                     ax[0,k].hist(m[m_t<np.quantile(m_t,0.1)],histtype='step',bins=bins,alpha=1,color=\"red\",label=\"10% quantile gen\",hatch=\"/\")\n",
    "#                     ax[0,k].vlines(np.quantile(m_t,0.1),0,np.max(h[:]),color=\"red\",label='10% quantile train')\n",
    "                    \n",
    "                ax[1,k].set_ylim(0.25,2)\n",
    "                ax[0,k].set_xlim(a,b)\n",
    "                ax[1,k].set_xlabel(name,fontweight=\"bold\")\n",
    "                ax[1,k].set_xlim(a,b)\n",
    "                ax[0,k].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "                ax[1,k].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "                \n",
    "#                 if plot_vline:\n",
    "#                        ax[0,k].legend([\"Generated\",\"Training\",\"10% quantile Gen\",\"10% quantile Sim\"] )\n",
    "#                 else:\n",
    "#                       ax[0,k].legend([\"Flow Generated\",\"MC Simulated\"] )\n",
    "            except:\n",
    "                print(\"mass plot failed reverting to simple plot mass bins\")\n",
    "                plt.close()\n",
    "                plt.figure()\n",
    "                _,b,_=plt.hist(m_t,15,label=\"MC Simulated\",alpha=0.5)\n",
    "                plt.hist(m,b,label=\"Flow Generated\",alpha=0.5)\n",
    "                plt.legend()  \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            \n",
    "#             plt.xlabel(name)\n",
    "            plt.tight_layout(pad=1)\n",
    "            k+=1\n",
    "        if save:\n",
    "            if v!=\"m\":\n",
    "                 self.summary.add_figure(\"inclusive\"+v,fig,self.step)\n",
    "            else:\n",
    "                self.summary.add_figure(\"jet_mass\",fig,self.step)\n",
    "#             print(\"added figure\")\n",
    "#             self.summary.close()\n",
    "        else:\n",
    "            plt.savefig(\"{}_oversample_{}\".format(self.p,v))\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "    def plot_mass(self,m,m_t,save=False,quantile=False,bins=15,plot_vline=False,title=\"\"):\n",
    "        #This creates a histogram of the inclusive distributions and calculates the mass of each jet\n",
    "        #and creates a histogram of that\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        #if quantile, this also creates a histogram of a subsample of the generated data, \n",
    "        # where the mass used to condition the flow is in the first 10% percentile of the simulated mass dist\n",
    "        i=0\n",
    "        k=0\n",
    "        fig,ax=plt.subplots(2,4,gridspec_kw={'height_ratios': [3, 1]},figsize=(24,6))\n",
    "        plt.suptitle(title,fontweight=\"bold\")\n",
    "        for v,name in zip([\"eta\",\"phi\",\"pt\",\"m\"],[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}$\",r\"$p_T^{rel}$\",r\"$m^{rel}$\"]):\n",
    "            \n",
    "            if v!=\"m\":\n",
    "                a=min(np.quantile(self.gen[:,i],0.001),np.quantile(self.test_set[:,i],0.001))\n",
    "                b=max(np.quantile(self.gen[:,i],0.999),np.quantile(self.test_set[:,i],0.999))     \n",
    "                \n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h.fill(self.gen[:,i])\n",
    "                h2.fill(self.test_set[:,i])\n",
    "                i+=1\n",
    "            else:\n",
    "                a=min(np.quantile(m_t,0.001),np.quantile(m,0.001))\n",
    "                b=max(np.quantile(m_t,0.999),np.quantile(m,0.999))\n",
    "                a=np.quantile(m_t,0.001)\n",
    "                b=np.quantile(m_t,0.999)\n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                bins = h.axes[0].edges\n",
    "                h.fill(m)#,weight=1/self.weight)\n",
    "                h2.fill(m_t)\n",
    "            \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            try:\n",
    "                main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "                    h2,\n",
    "                    ax_dict={\"main_ax\":ax[0,k],\"ratio_ax\":ax[1,k]},\n",
    "                    rp_ylabel=r\"Ratio\",\n",
    "                    rp_num_label=\"Flow Generated\",\n",
    "                    rp_denom_label=\"MC Simulatied\",\n",
    "                    rp_uncert_draw_type=\"line\",  # line or bar\n",
    "                )\n",
    "                ax[0,k].set_xlabel(\"\")\n",
    "#                 if quantile and v==\"m\" and plot_vline:\n",
    "#                     ax[0,k].hist(m[m_t<np.quantile(m_t,0.1)],histtype='step',bins=bins,alpha=1,color=\"red\",label=\"10% quantile gen\",hatch=\"/\")\n",
    "#                     ax[0,k].vlines(np.quantile(m_t,0.1),0,np.max(h[:]),color=\"red\",label='10% quantile train')\n",
    "                    \n",
    "                ax[1,k].set_ylim(0.25,2)\n",
    "                ax[0,k].set_xlim(a,b)\n",
    "                ax[1,k].set_xlabel(name,fontweight=\"bold\")\n",
    "                ax[1,k].set_xlim(a,b)\n",
    "                ax[0,k].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "                ax[1,k].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "                ax[0,k].legend(loc=\"upper left\")  \n",
    "#                 if plot_vline:\n",
    "#                        ax[0,k].legend([\"Generated\",\"Training\",\"10% quantile Gen\",\"10% quantile Sim\"] )\n",
    "#                 else:\n",
    "#                       ax[0,k].legend([\"Flow Generated\",\"MC Simulated\"] )\n",
    "            except:\n",
    "                print(\"mass plot failed reverting to simple plot mass bins\")\n",
    "                plt.close()\n",
    "                plt.figure()\n",
    "                _,b,_=plt.hist(m_t,15,label=\"MC Simulated\",alpha=0.5)\n",
    "                plt.hist(m,b,label=\"Flow Generated\",alpha=0.5)\n",
    "                plt.legend(loc=\"upper left\")  \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            \n",
    "#             plt.xlabel(name)\n",
    "            plt.tight_layout(pad=1)\n",
    "            \n",
    "\n",
    "            k+=1\n",
    "        if save:\n",
    "            if v!=\"m\":\n",
    "                 self.summary.add_figure(\"inclusive\"+v,fig,self.step)\n",
    "            else:\n",
    "                self.summary.add_figure(\"jet_mass\",fig,self.step)\n",
    "#             print(\"added figure\")\n",
    "#             self.summary.close()\n",
    "        else:\n",
    "            plt.savefig(\"{}_inclusive_{}\".format(self.p,v))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def losses(self,save=False):\n",
    "        '''This plots the different losses vs epochs'''\n",
    "        fig=plt.figure()\n",
    "        hep.cms.label(\"Private Work\",data=None,lumi=None,year=None)\n",
    "        plt.xlabel('step')\n",
    "        plt.ylabel('loss')\n",
    "        ln1=plt.plot(self.model.logprobs,label='log$(p_{gauss}(x_{data}))$')\n",
    "        if \"calc_massloss\" in self.config.keys() and self.config[\"calc_massloss\"]:\n",
    "            plt.twinx()\n",
    "            ln2=plt.plot(self.model.mlosses,label=r'mass mse $\\times$ {}'.format(self.config[\"lambda\"]),color='orange')\n",
    "            plt.ylabel(\"MSE\")\n",
    "            plt.yscale(\"log\")\n",
    "            ln1+=ln2\n",
    "        labs=[l.get_label() for l in ln1]\n",
    "        plt.legend(ln1,labs)\n",
    "        plt.tight_layout(pad=2)\n",
    "        if save:\n",
    "            self.summary.add_figure(\"losses\",fig,self.step)\n",
    "#             self.summary.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "   \n",
    "\n",
    "    def plot_correlations(self,save=True):\n",
    "        #Plots correlations between all particles for i=0 eta,i=1 phi,i=2 pt\n",
    "        self.plot_corr(i=0,save=save)\n",
    "        self.plot_corr(i=1,save=save)\n",
    "        self.plot_corr(i=2,save=save)\n",
    "\n",
    "    def plot_corr(self,i=0,names=[\"$\\eta^{rel}$\",\"$\\phi^{rel}$\",\"$p_T$\"],save=True):\n",
    "        if i==2:\n",
    "            c=1\n",
    "        else:\n",
    "            c=1\n",
    "        df_g=pd.DataFrame(self.gen.reshape(-1,90).detach().numpy()[:,range(i,90,3)])\n",
    "        df_h=pd.DataFrame(self.test_set.reshape(-1,90).detach().numpy()[:,range(i,90,3)])\n",
    "        fig,ax=plt.subplots(ncols=2,figsize=(20,10))\n",
    "        corr_g = ax[0].matshow(df_g.corr())\n",
    "        corr_g.set_clim(-c,c)\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar=fig.colorbar(corr_g,cax=cax)\n",
    "        corr_h = ax[1].matshow(df_h.corr())\n",
    "        corr_h.set_clim(-c,c)\n",
    "        divider = make_axes_locatable(ax[1])\n",
    "        cax2 = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar=fig.colorbar(corr_h,cax=cax2)\n",
    "        plt.suptitle(\"{} Correlation between Particles\".format(names[i]),fontweight=\"bold\")\n",
    "        ax[0].set_title(\"Flow Generated\",fontweight=\"bold\")\n",
    "        ax[1].set_title(\"MC Simulated\",fontweight=\"bold\")\n",
    "        ax[0].set_xlabel(\"Particles\",fontweight=\"bold\")\n",
    "        ax[0].set_ylabel(\"Particles\",fontweight=\"bold\")\n",
    "        ax[1].set_xlabel(\"Particles\",fontweight=\"bold\")\n",
    "        ax[1].set_ylabel(\"Particles\",fontweight=\"bold\")\n",
    "        title=[\"corr_eta\",\"corr_phi\",\"corr_pt\"]\n",
    "        if save:\n",
    "                \n",
    "                self.summary.add_figure(title[i],fig,self.step)    \n",
    "    #             self.summary.close()\n",
    "        else:\n",
    "                plt.savefig(\"{}_{}\".format(self.p,title[i]))\n",
    "                plt.show()\n",
    "\n",
    "    def var_part(self,true,gen,true_n,gen_n,m_true,m_gen,form=2,save=True):\n",
    "        labels=[\"$\\eta^{rel}$\",\"$\\phi^{rel}$\",\"$p^{rel}_T$\",\"$m^{rel}$\"]\n",
    "        names=[\"eta\",\"phi\",\"pt\",\"m\"]\n",
    "        n,counts=torch.unique(true_n,return_counts=True)\n",
    "        for j in range(4):\n",
    "            fig,ax=plt.subplots(ncols=2,nrows=2,figsize=(15,15))\n",
    "\n",
    "            k=-1\n",
    "            ntemp=n[-form**2:]\n",
    "\n",
    "            \n",
    "            for i in list(ntemp)[::-1]: \n",
    "                k+=1\n",
    "                i=int(i)\n",
    "\n",
    "                if names[j]!=\"m\":\n",
    "                    a=np.quantile(self.test_set[true_n.reshape(-1)==i,:].reshape(-1,3)[:,j],0.001)\n",
    "                    b=np.quantile(self.test_set[true_n.reshape(-1)==i,:].reshape(-1,3)[:,j],0.999)    \n",
    "                    h=hist.Hist(hist.axis.Regular(15,a,b))\n",
    "                    h2=hist.Hist(hist.axis.Regular(15,a,b))\n",
    "                    bins = h.axes[0].edges\n",
    "\n",
    "                    ax[k//form,k%form].legend()\n",
    "                    h.fill(self.gen[gen_n.reshape(-1)==i,:].reshape(-1,3)[:,j])\n",
    "                    h2.fill(self.test_set[true_n.reshape(-1)==i,:].reshape(-1,3)[:,j])\n",
    "                    \n",
    "                else:\n",
    "                    a=np.quantile(m_true[true_n.reshape(-1)==i],0.001)\n",
    "                    b=np.quantile(m_gen[gen_n.reshape(-1)==i],0.999)\n",
    "  \n",
    "                    h=hist.Hist(hist.axis.Regular(15,a,b,label=labels[j]))\n",
    "                    h2=hist.Hist(hist.axis.Regular(15,a,b,label=labels[j]))\n",
    "                    bins = h.axes[0].edges\n",
    "                    h.fill(m_gen[gen_n.reshape(-1)==i])\n",
    "                    h2.fill(m_true[true_n.reshape(-1)==i])\n",
    "                    \n",
    "\n",
    "                h.plot1d(    ax=ax[k//2,k%2],label=\"Flow Simulated\")  # line or bar)\n",
    "                h2.plot1d(    ax=ax[k//2,k%2],label=\"MC Generated\")  # line or bar)\n",
    "                ax[k//2,k%2].set_title(\"{} Distribution for jets with {} particles\".format(labels[j],i))\n",
    "\n",
    "                ax[k//2,k%2].set_xlabel(labels[j])\n",
    "                ax[k//2,k%2].set_ylabel(\"Counts\",fontweight=\"bold\")\n",
    "                ax[k//2,k%2].set_xlim(a,b)\n",
    "                ax[k//2,k%2].legend()\n",
    "                #plt.tight_layout(pad=2)\n",
    "\n",
    "            if save:\n",
    "                self.summary.add_figure(\"jet{}_{}\".format(i//3+1,names[i%3]),fig,global_step=self.step)\n",
    "                self.summary.close()\n",
    "            else:\n",
    "                plt.savefig(\"jet{}_{}\".format(self.p,i//3+1,names[j]))\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eeec8f7-62f4-4d96-877e-32bb9f53b79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'autoreg': False, 'context_features': 0, 'network_layers': 3, 'network_layers_nf': 2, 'network_nodes_nf': 256, 'batch_size': 1024, 'coupling_layers': 15, 'lr': 0.001, 'batchnorm': False, 'bins': 5, 'tail_bound': 6, 'limit': 150000, 'n_dim': 3, 'dropout': 0.2, 'canonical': False, 'max_steps': 100000, 'lambda': 1, 'name': 'Transflow_best', 'disc': False, 'variable': 1, 'parton': 't', 'wgan': False, 'corr': True, 'num_layers': 4, 'freq': 6, 'n_part': 30, 'fc': False, 'hidden': 500, 'heads': 4, 'l_dim': 100, 'lr_g': 0.0004327405312571664, 'lr_d': 0.0004327405312571664, 'lr_nf': 0.000722, 'sched': 'cosine2', 'opt': 'RMSprop', 'max_epochs': 3200, 'mass': True, 'no_hidden': False, 'clf': True, 'val_check': 50, 'frac_pretrain': 80, 'seed': 69, 'quantile': False}, 'hyperopt': True, 'num_batches': 112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/nn/init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/home/kaechben/JetNet_NF/LitJetNet/LitNF/lit_nf.py:270: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(p)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "best_hparam=\"/beegfs/desy/user/kaechben/Transflow_best/lightning_logs/version_72/hparams.yaml\"\n",
    "with open(best_hparam, 'r') as stream:\n",
    "        config=yaml.load(stream,Loader=yaml.Loader)\n",
    "        print(config)\n",
    "        config=config[\"config\"]\n",
    "data_module = JetNetDataloader(config,) #this loads the data\n",
    "data_module.setup(\"train\")\n",
    "# data=data_module.scaler.inverse_transform(data_module.data[:,:90]).reshape(-1,30,3)\n",
    "model = TransGan(config,data_module.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1367d3e-324f-4da8-ac20-69ea5ec385a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "TransGan                                                               --\n",
       "├─StandardNormal: 1-1                                                  --\n",
       "├─CompositeTransform: 1-2                                              --\n",
       "│    └─ModuleList: 2-1                                                 --\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-1           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-2           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-3           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-4           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-5           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-6           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-7           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-8           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-9           437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-10          437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-11          437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-12          437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-13          437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-14          437,366\n",
       "│    │    └─PiecewiseRationalQuadraticCouplingTransform: 3-15          437,366\n",
       "├─Flow: 1-3                                                            6,560,490\n",
       "│    └─CompositeTransform: 2-2                                         (recursive)\n",
       "│    │    └─ModuleList: 3-16                                           (recursive)\n",
       "│    └─StandardNormal: 2-3                                             --\n",
       "│    └─Identity: 2-4                                                   --\n",
       "├─Gen: 1-4                                                             --\n",
       "│    └─Linear: 2-5                                                     400\n",
       "│    └─TransformerEncoder: 2-6                                         --\n",
       "│    │    └─ModuleList: 3-17                                           565,600\n",
       "│    └─Linear: 2-7                                                     50,500\n",
       "│    └─Linear: 2-8                                                     250,500\n",
       "│    └─Linear: 2-9                                                     250,500\n",
       "│    └─Dropout: 2-10                                                   --\n",
       "│    └─Linear: 2-11                                                    1,503\n",
       "│    └─Linear: 2-12                                                    303\n",
       "│    └─Linear: 2-13                                                    45,090\n",
       "├─Disc: 1-5                                                            --\n",
       "│    └─Linear: 2-14                                                    400\n",
       "│    └─TransformerEncoder: 2-15                                        --\n",
       "│    │    └─ModuleList: 3-18                                           565,600\n",
       "│    └─Linear: 2-16                                                    102,000\n",
       "│    └─Linear: 2-17                                                    500,500\n",
       "│    └─Linear: 2-18                                                    501\n",
       "├─Sigmoid: 1-6                                                         --\n",
       "===============================================================================================\n",
       "Total params: 8,893,887\n",
       "Trainable params: 8,893,887\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = model\n",
    "batch_size = 16\n",
    "summary(model,depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78435b12-78a4-4ada-aa8e-351284ff58eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08110811392111827\n",
      "5.563642510858557\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model=\"/beegfs/desy/user/kaechben/Transflow_best/epoch=1749-val_fpnd=0.08-val_w1m=0.0008.ckpt\"\n",
    "batch=data_module.test_set\n",
    "model=model.load_from_checkpoint(best_model)\n",
    "model.load_datamodule(data_module)\n",
    "mask = batch[:50000, 90:].cpu().bool()\n",
    "batch = batch[:50000, :90].cpu()\n",
    "mask_test = model.sample_n(mask).bool()\n",
    "# mask_test=mask\n",
    "\n",
    "model.flow.train()\n",
    "model.dis_net.train()\n",
    "model.gen_net.train()\n",
    "model.data_module.scaler.to(\"cpu\")\n",
    "batch = batch.to(\"cpu\")\n",
    "model.flow = model.flow.to(\"cpu\")\n",
    "model.dis_net = model.dis_net.cpu()\n",
    "model.gen_net = model.gen_net.cpu()\n",
    "with torch.no_grad():\n",
    "#for some wierd reason its better with mask==None???\n",
    "    gen, true, z, fake_scaled, true_scaled, z_scaled = model.sampleandscale(batch, mask=mask_test,scale=True)\n",
    "    if model.config[\"mass\"]:\n",
    "        m_t = mass(batch.reshape(len(batch), model.n_part * model.n_dim), model.config[\"canonical\"])\n",
    "        m_f = mass(gen.reshape(len(batch), model.n_part * model.n_dim), model.config[\"canonical\"])\n",
    "\n",
    "    \n",
    "    # Reverse Standard Scaling (this has nothing to do with flows, it is a standard preprocessing step)\n",
    "    \n",
    "    true_scaled, fake_scaled, z_scaled = (true_scaled.reshape(-1, 90), fake_scaled.reshape(-1, 90), z_scaled.reshape(-1, 90))\n",
    "    for i in range(30):\n",
    "        i = 2 + 3 * i\n",
    "        z_scaled[z_scaled[:,i]<0,i]=0\n",
    "        fake_scaled[fake_scaled[:, i] < 0, i] = 0\n",
    "        true_scaled[true_scaled[:, i] < 0, i] = 0\n",
    "    # Some metrics we track\n",
    "    cov, mmd = cov_mmd(fake_scaled.reshape(-1, model.n_part, model.n_dim), true_scaled.reshape(-1, model.n_part, model.n_dim), use_tqdm=False)\n",
    "    try:\n",
    "        fpndv = fpnd(fake_scaled.reshape(-1, model.n_part, model.n_dim).numpy(), use_tqdm=False, jet_type=model.config[\"parton\"])\n",
    "        print(fpndv)\n",
    "    except:\n",
    "        fpndv = 1000\n",
    "    w1m_ = w1m(fake_scaled.reshape(len(batch), model.n_part, model.n_dim), true_scaled.reshape(len(batch), model.n_part, model.n_dim))\n",
    "    w1p_ = w1p(fake_scaled.reshape(len(batch), model.n_part, model.n_dim), true_scaled.reshape(len(batch), model.n_part, model.n_dim))\n",
    "    w1efp_ = w1efp(fake_scaled.reshape(len(batch), model.n_part, model.n_dim), true_scaled.reshape(len(batch), model.n_part, model.n_dim))\n",
    "\n",
    "\n",
    "    covz, mmdz = cov_mmd(z_scaled.reshape(-1, model.n_part, model.n_dim), true_scaled.reshape(-1, model.n_part, model.n_dim), use_tqdm=False)\n",
    "    try:\n",
    "        fpndvz = fpnd(z_scaled.reshape(-1, model.n_part, model.n_dim).numpy(), use_tqdm=False, jet_type=model.config[\"parton\"])\n",
    "        print(fpndvz)\n",
    "    except:\n",
    "        fpndv = 1000\n",
    "    w1m_z = w1m(z_scaled.reshape(len(batch), model.n_part, model.n_dim), true_scaled.reshape(len(batch), model.n_part, model.n_dim))\n",
    "    w1p_z = w1p(z_scaled.reshape(len(batch), model.n_part, model.n_dim), true_scaled.reshape(len(batch), model.n_part, model.n_dim))\n",
    "    w1efp_z = w1efp(z_scaled.reshape(len(batch), model.n_part, model.n_dim), true_scaled.reshape(len(batch), model.n_part, model.n_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7d40131-c09d-4067-b3cb-e66155b5db55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask=torch.triu(torch.ones(30,30,device=x.device))==0\n",
    "attention_mask.transpose(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4061c-fbb8-41f5-aad3-976d2a465885",
   "metadata": {},
   "source": [
    "# Measuring timing for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862a5fc0-1a09-4265-973e-e5967af13352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_mask = (torch.triu(torch.ones((3, 3))) == 0)\n",
    "att_mask = att_mask.transpose(0, 1).bool()\n",
    "att_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28bec0b-e7e4-4083-a9d1-bf3e355098a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.586864471435547e-06\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "batch=data_module.test_set\n",
    "model=model.load_from_checkpoint(best_model)\n",
    "model.load_datamodule(data_module)\n",
    "mask = batch[:, 90:].cpu().bool()\n",
    "batch = batch[:, :90].cpu()\n",
    "original_batch=batch\n",
    "model.flow.train()\n",
    "model.dis_net.train()\n",
    "model.gen_net.train()\n",
    "model.data_module.scaler.to(\"cuda\")\n",
    "batch = original_batch[:50000].to(\"cuda\")\n",
    "model.flow = model.flow.to(\"cuda\")\n",
    "model.dis_net = model.dis_net.cuda()\n",
    "model.gen_net = model.gen_net.cuda()\n",
    "start=time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "#for some wierd reason its better with mask==None???\n",
    "    mask_test = model.sample_n(mask[:50000]).bool().cuda()\n",
    "    gen, true, z, fake_scaled, true_scaled, z_scaled = model.sampleandscale(batch, mask=mask_test,scale=True)\n",
    "print((time.time()-start)/50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4451942-8bbd-4270-9d58-accd079e1bb8",
   "metadata": {},
   "source": [
    "# Print table for paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd555d0-107c-4502-a1c7-8c7f0a4a05c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    w1m_           w1p_      w1efp_   pmm   pmp   pme     cov  \\\n",
      "model                                                                           \n",
      "VNF        $4.8 \\pm 0.2$  $2.5 \\pm 0.3$  $13 \\pm 1$  0.21  0.32  1.43  $0.56$   \n",
      "TF         $2.6 \\pm 0.1$  $4.3 \\pm 0.4$  $12 \\pm 1$  0.11  0.43  1.22  $0.49$   \n",
      "MP-MP      $0.6 \\pm 0.2$  $2.3 \\pm 0.3$   $2 \\pm 1$  0.20  0.30  1.00  $0.57$   \n",
      "MP_LFC-MP  $0.9 \\pm 0.3$  $2.2 \\pm 0.7$   $2 \\pm 1$  0.30  0.70  1.00  $0.56$   \n",
      "\n",
      "            fpndv      mmd parton      model  \n",
      "model                                         \n",
      "VNF        $5.60$  $0.071$      t        VNF  \n",
      "TF         $4.46$  $0.084$      t         TF  \n",
      "MP-MP      $0.37$  $0.071$      t      MP-MP  \n",
      "MP_LFC-MP  $0.93$  $0.073$      t  MP_LFC-MP  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26200/200639810.py:74: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  for col in print_table.drop(\"model\",1).columns:\n",
      "/tmp/ipykernel_26200/200639810.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  temp_index=temp[col].str.replace(\"$\",\"\").str.split(\"\\\\\").str[0].astype(float)\n",
      "/tmp/ipykernel_26200/200639810.py:81: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  temp.loc[mins,col]=\"$\\mathbf{\"+temp.loc[mins,col].astype(str).str.replace(\"$\",\"\")+\"}$\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{7}{*}{Top Quark} & \n",
      "MP-MP & $\\mathbf{0.6 \\pm 0.2}$ &$2.3 \\pm 0.3$ &$\\mathbf{2 \\pm 1}$ & $\\mathbf{0.37}$ & $\\mathbf{0.57}$ & $\\mathbf{0.071}$ \\\\&\n",
      "\n",
      "MP\\_LFC-MP &$0.9 \\pm 0.3$ & $\\mathbf{2.2 \\pm 0.7}$ &$\\mathbf{2 \\pm 1}$ &$0.93$ &$0.56$ &$0.073$ \\\\&\n",
      "VNF &$4.8 \\pm 0.2$ &$2.5 \\pm 0.3$ &$13 \\pm 1$ &$5.60$ &$0.56$ & $\\mathbf{0.071}$ \\\\&\n",
      " TF &$2.6 \\pm 0.1$ &$4.3 \\pm 0.4$ &$12 \\pm 1$ &$4.46$ &$0.49$ &$0.084$ \\\\\\cline{1-8}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26200/200639810.py:84: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  text=temp.to_latex(index=False,escape=False)\n"
     ]
    }
   ],
   "source": [
    "def format_mean_sd(mean, sd):\n",
    "    \"\"\"round mean and standard deviation to most significant digit of sd and apply latex formatting\"\"\"\n",
    "    decimals = -int(np.floor(np.log10(sd)))\n",
    "    decimals -= int((sd * 10 ** decimals) >= 9.5)\n",
    "\n",
    "    if decimals < 0:\n",
    "        ten_to = 10 ** (-decimals)\n",
    "        if mean > ten_to:\n",
    "            mean = ten_to * (mean // ten_to)\n",
    "        else:\n",
    "            mean_ten_to = 10 ** np.floor(np.log10(mean))\n",
    "            mean = mean_ten_to * (mean // mean_ten_to)\n",
    "        sd = ten_to * (sd // ten_to)\n",
    "        decimals = 0\n",
    "\n",
    "    if mean >= 1e3 and sd >= 1e3:\n",
    "        mean = np.round(mean * 1e-3)\n",
    "        sd = np.round(sd * 1e-3)\n",
    "        return f\"${mean:.{decimals}f}$k $\\\\pm {sd:.{decimals}f}$k\"\n",
    "    else:\n",
    "        return f\"${mean:.{decimals}f} \\\\pm {sd:.{decimals}f}$\"\n",
    "\n",
    "# print_table.index=[\"t\"]#\"q\"\n",
    "# print_table[\"model\"]=[\"t\"]#\"q\"\n",
    "cols=[\"w1m_\",\"w1p_\",\"w1efp_\",\"pmm\",\"pmp\",\"pme\",\"cov\",\"fpndv\",\"mmd\",\"model\"]\n",
    "\n",
    "print_table=pd.DataFrame([[w1m_z[0],w1p_z[0],w1efp_z[0],w1m_z[1],w1p_z[1]\n",
    "                          ,w1efp_z[1],covz,fpndvz,mmdz,\"VNF\"],[w1m_[0],w1p_[0],w1efp_[0],w1m_[1],w1p_[1]\n",
    "                          ,w1efp_[1],cov,fpndv,mmd,\"TF\",]],columns=cols).set_index(\"model\",drop=False)\n",
    "\n",
    "\n",
    "print_table.loc[:,\"w1m_\"]*=1000\n",
    "print_table.loc[:,\"w1p_\"]*=1000\n",
    "print_table.loc[:,\"w1efp_\"]*=100000\n",
    "print_table.loc[:,\"pmm\"]*=1000\n",
    "print_table.loc[:,\"pmp\"]*=1000\n",
    "print_table.loc[:,\"pme\"]*=100000\n",
    "\n",
    "\n",
    "print_table.loc[\"MP-MP-t\",:]=np.array([0.6,2.3,2,.2,.3,1,0.57,0.37,0.071,\"MP-MP\"])\n",
    "print_table.loc[\"MPLFC-MP-t\",:]=np.array([0.9,2.2,2,.3,.7,1,0.56,0.93,0.073,\"MP_LFC-MP\"])\n",
    "print_table.loc[:,\"w1m_\"]=print_table.apply(lambda x:format_mean_sd(float(x[\"w1m_\"]),float(x[\"pmm\"])),axis=1)\n",
    "print_table.loc[:,\"w1efp_\"]=print_table.apply(lambda x:format_mean_sd(float(x[\"w1efp_\"]),float(x[\"pme\"])),axis=1)\n",
    "print_table.loc[:,\"w1p_\"]=print_table.apply(lambda x:format_mean_sd(float(x[\"w1p_\"]),float(x[\"pmp\"])),axis=1)\n",
    "\n",
    "\n",
    "print_table.loc[:,\"cov\"]=print_table.loc[:,\"cov\"].astype(float).map('{:.2f}'.format)\n",
    "print_table.loc[:,\"fpndv\"]=print_table.loc[:,\"fpndv\"].astype(float).map('{:.2f}'.format)\n",
    "print_table.loc[:,\"mmd\"]=print_table.loc[:,\"mmd\"].astype(float).map('{:.3f}'.format)\n",
    "print_table.loc[:,\"pmm\"]=print_table.loc[:,\"pmm\"].astype(float).map('{:,.2f}'.format)\n",
    "\n",
    "print_table.loc[:,\"pmp\"]=print_table.loc[:,\"pmp\"].astype(float).map('{:,.2f}'.format)\n",
    "\n",
    "print_table.loc[:,\"pme\"]=print_table.loc[:,\"pme\"].astype(float).map('{:,.2f}'.format)\n",
    "print_table.loc[:,\"parton\"]=\"t\"\n",
    "# print_table.loc[:,\"val_w1m\"]=\"$\"+print_table[\"val_w1m\"].map(str)+\"\\pm\"+print_table[\"pmm\"].map(str)+\"$\"\n",
    "# print_table.loc[:,\"val_w1p\"]=\"$\"+print_table[\"val_w1p\"].map(str)+\"\\pm\"+print_table[\"pmp\"].map(str)+\"$\"\n",
    "# print_table.loc[:,\"val_w1efp\"]=\"$\"+print_table[\"val_w1efp\"].map(str)+\"\\pm\"+print_table[\"pme\"].map(str)+\"$\"\n",
    "print_table.loc[:,\"cov\"]=\"$\"+print_table[\"cov\"].map(str)+\"$\"\n",
    "print_table.loc[:,\"fpndv\"]=\"$\"+print_table[\"fpndv\"].map(str)+\"$\"\n",
    "print_table.loc[:,\"mmd\"]=\"$\"+print_table[\"mmd\"].map(str)+\"$\"\n",
    "\n",
    "# print_table.loc[:,\"model\"]=print_table[\"model\"].str.replace(\"c0\",\"VNF\").str.replace(\"cc\",\"NFCC\").str.replace(\"c\",\"NFC\").str.replace(\"1\",\"\\ (m)\").str.replace(\"2\",\"\\ (m,n)\").str.replace(\"q\",\"\").str.replace(\"g\",\"\").str.replace(\"t\",\"\")\n",
    "index=[\"MP-MP\",\"MP_LFC-MP\",\"VNF\",\"TF\"]#\"MP-MP-q\",\"MPLFC-MP-q\",\"q\"\n",
    "print_table=print_table.set_index(\"model\")\n",
    "print_table[\"model\"]=print_table.index\n",
    "print(print_table)\n",
    "print_table=print_table.loc[index,:]\n",
    "final_table=pd.DataFrame()\n",
    "tex=\"\"\n",
    "for p in [\"t\"]:#\"q\",\n",
    "    temp=print_table[print_table[\"parton\"]==p]\n",
    "\n",
    "    for col in print_table.drop(\"model\",1).columns:\n",
    "        \n",
    "        if col not in [\"w1m_\",\"w1p_\",\"w1efp_\",\"fpndv\",\"cov\",\"mmd\" ]:\n",
    "            continue\n",
    "        \n",
    "        temp_index=temp[col].str.replace(\"$\",\"\").str.split(\"\\\\\").str[0].astype(float)\n",
    "        mins=temp_index==temp_index.min() if col!=\"cov\" else temp_index==temp_index.max()\n",
    "        temp.loc[mins,col]=\"$\\mathbf{\"+temp.loc[mins,col].astype(str).str.replace(\"$\",\"\")+\"}$\"\n",
    "    temp=temp[[\"model\",\"w1m_\",\"w1p_\",\"w1efp_\",\"fpndv\",\"cov\",\"mmd\"]]\n",
    "    temp.columns=[\"model\",\"$W_1^M (\\times 10^{-3})$\",\"$W_1^P (\\times 10^{-3})$\",\"$W_1^{EFP}(\\times 10^{-5})$\",\"FPND\",r\"COV $\\uparrow$\",\"MMD\"]\n",
    "    text=temp.to_latex(index=False,escape=False)\n",
    "    parton=\"Gluon\" if p==\"g\" else \"Light Quark\" if p==\"q\" else \"Top Quark\"\n",
    "    tex+=\"\\multirow{7}{*}{\"+parton+\"} & \"+text.split(\"MMD \\\\\\\\\")[1].split(\"\\\\bottomrule\")[0].replace(\"\\\\\\\\\",\"\\\\\\\\&\").replace(\"\\\\midrule\",\"\").replace(\"MP_LFC\",\"MP\\_LFC\").replace(\"  \",\"\")[:-2]+\"\\cline{1-8}\" \n",
    "    tex+=\"\\n\"\n",
    "print(tex)\n",
    "\n",
    "    #     final_table=final_table.append(temp)\n",
    "\n",
    "# print(final_table.to_latex(index=False,escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6ea3ec-5c98-4228-8a69-d1eb5c845a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m m_c \u001b[38;5;241m=\u001b[39m mass(fake_scaled[:, : model\u001b[38;5;241m.\u001b[39mn_dim \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mn_part], model\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcanonical\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      7\u001b[0m plot\u001b[38;5;241m=\u001b[39mplotting_paper(true_scaled,fake_scaled,config,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m,model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mplot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_mass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mquantile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mplot_vline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mplotting_paper.plot_mass\u001b[0;34m(self, m, m_t, save, quantile, bins, plot_vline, title)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v,name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m],[\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124meta^\u001b[39m\u001b[38;5;132;01m{rel}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mphi^\u001b[39m\u001b[38;5;132;01m{rel}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$p_T^\u001b[39m\u001b[38;5;132;01m{rel}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$m^\u001b[39m\u001b[38;5;132;01m{rel}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 286\u001b[0m         a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m,np\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set[:,i],\u001b[38;5;241m0.001\u001b[39m))\n\u001b[1;32m    287\u001b[0m         b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(np\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen[:,i],\u001b[38;5;241m0.999\u001b[39m),np\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set[:,i],\u001b[38;5;241m0.999\u001b[39m))     \n\u001b[1;32m    289\u001b[0m         h\u001b[38;5;241m=\u001b[39mhist\u001b[38;5;241m.\u001b[39mHist(hist\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mRegular(bins,a,b))\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mquantile\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/numpy/lib/function_base.py:3979\u001b[0m, in \u001b[0;36mquantile\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[1;32m   3978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantiles must be in the range [0, 1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/numpy/lib/function_base.py:3986\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a, q, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, overwrite_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3984\u001b[0m                         interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3985\u001b[0m     \u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3986\u001b[0m     r, k \u001b[38;5;241m=\u001b[39m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3987\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3988\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n\u001b[1;32m   3990\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mreshape(q\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m+\u001b[39m k)\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/numpy/lib/function_base.py:3539\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ureduce\u001b[39m(a, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3514\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3515\u001b[0m \u001b[38;5;124;03m    Internal Function.\u001b[39;00m\n\u001b[1;32m   3516\u001b[0m \u001b[38;5;124;03m    Call `func` with `a` as first argument swapping the axes to use extended\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3537\u001b[0m \n\u001b[1;32m   3538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3539\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3540\u001b[0m     axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/_tensor.py:732\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x432 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_t = mass(\n",
    "        true_scaled[:, : model.n_dim * model.n_part].to(model.device),\n",
    "        model.config[\"canonical\"],\n",
    "    ).cpu()\n",
    "m_gen = mass(z_scaled[:, : model.n_dim * model.n_part], model.config[\"canonical\"]).cpu()\n",
    "m_c = mass(fake_scaled[:, : model.n_dim * model.n_part], model.config[\"canonical\"]).cpu()\n",
    "plot=plotting_paper(true_scaled,fake_scaled,config,0,\"t\",model=model)\n",
    "plot.plot_mass(m_c,m_t,save=False,quantile=False,bins=50,plot_vline=False,title=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b88a6f-5acd-43b8-a738-394dc709acdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(30,30))-torch.diag(torch.ones(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f4fbd-daae-4697-a5ab-25140eca95c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetnet",
   "language": "python",
   "name": "jetnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
