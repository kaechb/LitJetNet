{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6770c4a2-d076-42d5-aaed-1864016ce6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thx max\n",
      "good boy\n"
     ]
    }
   ],
   "source": [
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from jetnet_dataloader import JetNetDataloader\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import traceback\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import OneCycleLR,ReduceLROnPlateau,ExponentialLR\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as FF\n",
    "import numpy as np\n",
    "from jetnet.evaluation import w1p, w1efp, w1m, cov_mmd,fpnd\n",
    "import mplhep as hep\n",
    "import hist\n",
    "from hist import Hist\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from collections import OrderedDict\n",
    "from ray import tune\n",
    "from helpers import *\n",
    "from plotting import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as FF\n",
    "import numpy as np\n",
    "from jetnet.evaluation import w1p, w1efp, w1m, cov_mmd,fpnd\n",
    "import mplhep as hep\n",
    "import hist\n",
    "from hist import Hist\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.autograd import Variable\n",
    "from helpers import *\n",
    "from plotting import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import datetime\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608734f7-eafc-473d-b5fb-639dcc966efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1953e-11, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The Box-Cox transformation can only be applied to strictly positive data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 126>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set),drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m3\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_part\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m30\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparton\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m150000\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m512\u001b[39m}\n\u001b[0;32m--> 126\u001b[0m \u001b[43mJetNetDataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mJetNetDataloader.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[:,:,:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptscaler\u001b[38;5;241m=\u001b[39mPowerTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox-cox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[:,:,:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[:,:,:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[:,:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptscaler\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[:,:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:3045\u001b[0m, in \u001b[0;36mPowerTransformer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate the optimal parameter lambda for each feature.\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m \n\u001b[1;32m   3029\u001b[0m \u001b[38;5;124;03m    The optimal lambda parameter for minimizing skewness is estimated on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;124;03m        Fitted transformer.\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:3068\u001b[0m, in \u001b[0;36mPowerTransformer._fit\u001b[0;34m(self, X, y, force_transform)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3068\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_positive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3070\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_transform:  \u001b[38;5;66;03m# if call from fit()\u001b[39;00m\n\u001b[1;32m   3071\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# force copy so that fit does not change X inplace\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:3299\u001b[0m, in \u001b[0;36mPowerTransformer._check_input\u001b[0;34m(self, X, in_fit, check_positive, check_shape, check_method)\u001b[0m\n\u001b[1;32m   3297\u001b[0m     np\u001b[38;5;241m.\u001b[39mwarnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll-NaN (slice|axis) encountered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_positive \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox-cox\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnanmin(X) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Box-Cox transformation can only be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3301\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplied to strictly positive data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3302\u001b[0m         )\n\u001b[1;32m   3304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_shape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas_):\n\u001b[1;32m   3305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput data has a different number of features \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan fitting data. Should have \u001b[39m\u001b[38;5;132;01m{n}\u001b[39;00m\u001b[38;5;124m, data has \u001b[39m\u001b[38;5;132;01m{m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   3308\u001b[0m             n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas_), m\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3309\u001b[0m         )\n\u001b[1;32m   3310\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The Box-Cox transformation can only be applied to strictly positive data"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from nflows.distributions.base import Distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from helpers import *\n",
    "\n",
    "\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self, mean=None, std=None, epsilon=1e-7):\n",
    "        \"\"\"Standard Scaler.\n",
    "        The class can be used to normalize PyTorch Tensors using native functions. The module does not expect the\n",
    "        tensors to be of any specific shape; as long as the features are the last dimension in the tensor, the module\n",
    "        will work fine.\n",
    "        :param mean: The mean of the features. The property will be set after a call to fit.\n",
    "        :param std: The standard deviation of the features. The property will be set after a call to fit.\n",
    "        :param epsilon: Used to avoid a Division-By-Zero exception.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, values):\n",
    "        dims = list(range(values.dim() - 1))\n",
    "        self.mean = torch.mean(values, dim=dims)\n",
    "        self.std = torch.std(values, dim=dims)\n",
    "\n",
    "    def transform(self, values):\n",
    "        return (values - self.mean) / (self.std + self.epsilon)\n",
    "    def inverse_transform(self,values):\n",
    "        return (values *self.std)+self.mean\n",
    "    def fit_transform(self, values):\n",
    "        self.fit(values)\n",
    "        return self.transform(values)\n",
    "    def to(self,dev):\n",
    "        self.std=self.std.to(dev)\n",
    "        self.mean=self.mean.to(dev)\n",
    "        return self\n",
    "  \n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class JetNetDataloader(pl.LightningDataModule):\n",
    "    '''This is more or less standard boilerplate coded that builds the data loader of the training\n",
    "       one thing to note is the custom standard scaler that works on tensors\n",
    "       Currently only jets with 30 particles are used but this maybe changes soon'''\n",
    "    def __init__(self,config): \n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.n_dim=config[\"n_dim\"]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        self.batch_size=config[\"batch_size\"]\n",
    "    def setup(self,stage):\n",
    "    # This just sets up the dataloader, nothing particularly important. it reads in a csv, calculates mass and reads out the number particles per jet\n",
    "    # And adds it to the dataset as variable. The only important thing is that we add noise to zero padded jets\n",
    "        data_dir=os.environ[\"HOME\"]+\"/JetNet_NF/train_{}_jets.csv\".format(self.config[\"parton\"])\n",
    "        data=pd.read_csv(data_dir,sep=\" \",header=None)\n",
    "        jets=[]\n",
    "        df=pd.DataFrame()\n",
    "        limit=int(self.config[\"limit\"]*1.1)\n",
    "\n",
    "            #masks=np.sum(data.values[:,np.arange(3,120,4)],axis=1)\n",
    "        masks=data.values[:,np.arange(3,120,4)][:self.config[\"limit\"]]\n",
    "        df=data.drop(np.arange(3,120,4),axis=1)[:self.config[\"limit\"]]\n",
    "\n",
    "        #stacking together differnet samples with different number particles per jet\n",
    "    \n",
    "      \n",
    "        # calculating mass per jet\n",
    "#         self.m=mass(self.data[:,:self.n_dim]).reshape(-1,1)  \n",
    "      # Adding noise to zero padded jets.\n",
    "        z=torch.tensor(df.values).reshape(len(df),30,3)\n",
    "        m=(torch.tensor(masks).reshape(len(df),30)).bool()\n",
    "\n",
    "\n",
    "        self.data=z\n",
    "        self.data[~m,:]=torch.normal(mean=torch.zeros_like(self.data[~m,:]),std=1).abs()*1e-7\n",
    "        print(self.data.reshape(-1,30,3)[:,:,2].min())\n",
    "        #standard scaling \n",
    "        self.scaler=StandardScaler()\n",
    "\n",
    "#         self.data=torch.hstack((self.data,self.m))        \n",
    "        self.scaler.fit(self.data[:,:,:2])\n",
    "        self.ptscaler=PowerTransformer(\"box-cox\")\n",
    "        self.data[:,:,:2]=self.scaler.transform(self.data[:,:,:2])\n",
    "        \n",
    "        self.data[:,:,2]=torch.tensor(self.ptscaler.transform(self.data[:,:,2].numpy()))\n",
    "        print(self.ptscaler.inverse_transform(self.data[:,:,2]).min())\n",
    "        print(self.ptscaler.inverse_transform(np.zeros((10,30))+self.data[:10,:,2].numpy().min()*0.9))\n",
    "        #         self.min_m=self.scaler.transform(torch.zeros((1,self.n_dim+1)))[0,-1]\n",
    "# #         self.data=torch.hstack((self.data,self.n))\n",
    "        \n",
    "#         #calculating mass dist in different bins, this is needed for the testcase where we need to generate the conditoon\n",
    "#         if self.config[\"variable\"]:\n",
    "#             self.mdists={}\n",
    "#             for i in torch.unique(self.n):\n",
    "#                 self.mdists[int(i)]=F(self.data[self.n[:,0]==i,-2])    \n",
    "        self.data,self.test_set=train_test_split(self.data.cpu().numpy(),test_size=0.3)\n",
    "        \n",
    "#         self.n_train=self.data[:,-1]\n",
    "#         self.n_test=self.test_set[:,-1]\n",
    "        \n",
    "            \n",
    "        self.test_set=torch.tensor(self.test_set).float()\n",
    "        self.data=torch.tensor(self.data).float()\n",
    "        self.num_batches=len(self.data)//self.config[\"batch_size\"]\n",
    "#         assert self.data.shape[1]==92\n",
    "        assert (torch.isnan(self.data)).sum()==0\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data, batch_size=self.batch_size,drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=len(self.test_set),drop_last=True)\n",
    "config={\"n_dim\":3,\"n_part\":30,\"parton\":\"t\",\"limit\":150000,\"batch_size\":512}\n",
    "JetNetDataloader(config).setup(stage=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35e07c5-db38-4f56-8224-525b7aa873a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import hist\n",
    "import mplhep as hep\n",
    "import torch\n",
    "import numpy as np\n",
    "import hist\n",
    "from hist import Hist\n",
    "import traceback\n",
    "from helpers import mass\n",
    "import pandas as pd\n",
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "class plotting():\n",
    "    '''This is a class that takes care of  plotting steps in the script,\n",
    "        It is initialized with the following arguments:\n",
    "        true=the simulated data, note that it needs to be scaled\n",
    "        gen= Generated data , needs to be scaled\n",
    "        step=The current step of the training, this is need for tensorboard\n",
    "        model=the model that is trained, a bit of an overkill as it is only used to access the losses\n",
    "        config=the config used for training\n",
    "        logger=The logger used for tensorboard logging'''\n",
    "    def __init__(self,true,gen,step,model=None,logger=None,weight=1):\n",
    "\n",
    "        self.n_dim=90\n",
    "        self.gen=gen\n",
    "        self.test_set=true\n",
    "        self.step=step\n",
    "        self.model=model\n",
    "\n",
    "        self.weight=weight\n",
    "        if logger is not None:\n",
    "            self.summary=logger\n",
    "    def plot_mass_only(self,m,m_t,bins=15):\n",
    "        fig,ax=plt.subplots(2,1,gridspec_kw={'height_ratios': [3, 1]},figsize=(6,8))\n",
    "        a=min(np.quantile(m_t,0.001),np.quantile(m,0.001))\n",
    "        b=max(np.quantile(m_t,0.999),np.quantile(m,0.999))\n",
    "        a=np.quantile(m_t,0.001)\n",
    "        b=np.quantile(m_t,0.999)\n",
    "        h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "        h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "        bins = h.axes[0].edges\n",
    "        h.fill(m)#,weight=1/self.weight)\n",
    "        h2.fill(m_t)\n",
    "            \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "\n",
    "        main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "            h2,\n",
    "            ax_dict={\"main_ax\":ax[0],\"ratio_ax\":ax[1]},\n",
    "            rp_ylabel=r\"Ratio\",\n",
    "            rp_num_label=\"Flow Generated\",\n",
    "            rp_denom_label=\"MC Simulatied\",\n",
    "            rp_uncert_draw_type=\"line\",  # line or bar\n",
    "        )\n",
    "        ax[0].set_xlabel(\"\")\n",
    "#                 if quantile and v==\"m\" and plot_vline:\n",
    "#                     ax[0,k].hist(m[m_t<np.quantile(m_t,0.1)],histtype='step',bins=bins,alpha=1,color=\"red\",label=\"10% quantile gen\",hatch=\"/\")\n",
    "#                     ax[0,k].vlines(np.quantile(m_t,0.1),0,np.max(h[:]),color=\"red\",label='10% quantile train')\n",
    "\n",
    "        ax[1].set_ylim(0.25,2)\n",
    "        ax[0].set_xlim(a,b)\n",
    "        ax[1].set_xlabel(\"$m_T$\",fontweight=\"bold\")\n",
    "        ax[1].set_xlim(a,b)\n",
    "        ax[0].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "        ax[1].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "  \n",
    "     \n",
    "#             print(\"added figure\")\n",
    "#             self.summary.close()\n",
    "\n",
    "        plt.savefig(\"{}{}{}_mass\".format(self.typ,self.c,self.p))\n",
    "        plt.show()\n",
    "\n",
    "    def plot_marginals(self,ith=None,save=True,title=None):\n",
    "        #This plots the marginal distribution for simulation and generation\n",
    "        #Note that this is the data the model sees during training as input to model in the NF\n",
    "        #This is the distribution of one of [eta,phi,pt] of one particle of the n particles per jet: for example the pt of the 3rd particle\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        \n",
    "        plt.switch_backend('agg')\n",
    "        name,label=[\"eta\",\"phi\",\"pt\"],['${\\eta}^{rel}_{7}$',\"${\\phi}^{rel}_{7}$\",\"${p_T}^{rel}_{7}$\"]\n",
    "        fig,ax=plt.subplots(2,3,gridspec_kw={'height_ratios': [3, 1]},figsize=(18,6))\n",
    "        particles=range(self.n_dim) if not ith else [3*ith,3*ith+1,3*ith+2]\n",
    "        plt.suptitle(title,fontweight=\"bold\")\n",
    "        k=0\n",
    "        for i in particles:\n",
    "            if ith:\n",
    "                ax_temp=ax[:,k]\n",
    "            else:\n",
    "                fig,ax_temp=plt.subplots(2,1)\n",
    "            a=np.quantile(self.test_set[:,i].numpy(),0)\n",
    "            b=np.quantile(self.test_set[:,i].numpy(),1)\n",
    "\n",
    "            h=hist.Hist(hist.axis.Regular(15,a,b,label=label[i%3],underflow=False,overflow=False))\n",
    "            h2=hist.Hist(hist.axis.Regular(15,a,b,label=label[i%3],underflow=False,overflow=False))\n",
    "            h.fill(self.gen[:,i].numpy())\n",
    "            h2.fill(self.test_set[:,i].numpy())\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0,k] )\n",
    "       \n",
    "            main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "                h2,\n",
    "                ax_dict={\"main_ax\":ax_temp[0],\"ratio_ax\":ax_temp[1]},\n",
    "                rp_ylabel=r\"Ratio\",\n",
    "#                 rp_xlabel=label[i%3],\n",
    "                rp_num_label=\"Flow Generated\",\n",
    "                rp_denom_label=\"MC Simulated\",\n",
    "                rp_uncert_draw_type=\"line\",  # line or bar\n",
    "            )\n",
    "            \n",
    "            \n",
    "            ax_temp[0].set_xlabel(\"\")\n",
    "            ax_temp[1].set_ylim(0.25,2)\n",
    "            ax_temp[0].set_xlim(a,b)\n",
    "            ax_temp[1].set_xlim(a,b)\n",
    "            ax_temp[1].set_xlabel(label[i%3])\n",
    "            ax_temp[0].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "            ax_temp[1].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "            \n",
    "            \n",
    "            #plt.tight_layout(pad=2)\n",
    "            k+=1\n",
    "        if save:\n",
    "            self.summary.add_figure(\"jet{}_{}\".format(i//3+1,name[i%3]),fig,global_step=self.step)\n",
    "            self.summary.close()\n",
    "        else:\n",
    "            plt.savefig(\"{}{}{}_7thpart\".format(self.typ,self.c,self.p))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def plot_2d(self,save=True):\n",
    "        #This creates a 2D histogram of the inclusive distribution for all 3 feature combinations\n",
    "        #Inclusive means that is the distribution of pt of all particles per jet and sample\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        data=self.test_set[:,:self.n_dim].reshape(-1,3).numpy()\n",
    "        gen=self.gen[:,:self.n_dim].reshape(-1,3).numpy()\n",
    "        labels=[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}_7$\",r\"$p_T^{rel}$\"]\n",
    "        names=[\"eta\",\"phi\",\"pt\"]\n",
    "        for index in [[0,1],[0,2],[1,2]]:\n",
    "            \n",
    "            fig,ax=plt.subplots(ncols=2,figsize=(16, 8))\n",
    "            _,x,y,_=ax[0].hist2d(data[:,index[0]],data[:,index[1]],bins=30)\n",
    "            #rebin to only take 5% to 95.0% of signal dis\n",
    "            a=np.quantile(x,0.05)\n",
    "            b=np.quantile(x,0.95)\n",
    "            x=np.linspace(a,b,len(x))\n",
    "            a=np.quantile(y,0.05)\n",
    "            b=np.quantile(y,0.95)\n",
    "            y=np.linspace(a,b,len(y))\n",
    "            if index[1]==2:\n",
    "                y=np.abs(y)+0.00001\n",
    "                y = np.logspace(np.log(y[0]),np.log(y[-1]),len(y))\n",
    "            ax[0].hist2d(data[:,index[0]],data[:,index[1]],bins=[x,y])\n",
    "            data[:,index[0]]=np.abs(data[:,index[0]])+0.00001\n",
    "            ax[1].hist2d(gen[:,index[0]],gen[:,index[1]],bins=[x,y])\n",
    "        \n",
    "        \n",
    "            plt.tight_layout(pad=2)\n",
    "            ax[0].set_xlabel( labels[index[0]],fontweight=\"bold\")\n",
    "            ax[0].set_ylabel( labels[index[1]],fontweight=\"bold\")\n",
    "            \n",
    "            ax[0].set_title(\"MC Simulated\")\n",
    "            ax[1].set_xlabel( labels[index[0]],fontweight=\"bold\")\n",
    "            ax[1].set_ylabel( labels[index[1]],fontweight=\"bold\")\n",
    "            \n",
    "            ax[1].set_title(\"Flow Generated\")\n",
    "           \n",
    "            if save:\n",
    "                self.summary.add_figure(\"2d{}-{}\".format(names[index[0]],names[index[1]]),fig,global_step=self.step)\n",
    "                \n",
    "                # self.summary.close()\n",
    "            else:\n",
    "                plt.savefig(\"{}{}{}_2dcorr{}{}\".format(self.typ,self.c,self.p,names[index[0]],names[index[0]]))\n",
    "                plt.show()\n",
    "                \n",
    " \n",
    "        \n",
    "    def oversample(self,m,m_t,weight,save=True,quantile=False,bins=15,plot_vline=False,title=\"\"):\n",
    "        #This creates a histogram of the inclusive distributions and calculates the mass of each jet\n",
    "        #and creates a histogram of that\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        #if quantile, this also creates a histogram of a subsample of the generated data, \n",
    "        # where the mass used to condition the flow is in the first 10% percentile of the simulated mass dist\n",
    "        i=0\n",
    "        k=0\n",
    "        fig,ax=plt.subplots(2,4,gridspec_kw={'height_ratios': [3, 1]},figsize=(20,5))\n",
    "        plt.suptitle(title,fontweight=\"bold\")\n",
    "        for v,name in zip([\"eta\",\"phi\",\"pt\",\"m\"],[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}$\",r\"$p_T^{rel}$\",r\"$m^{rel}$\"]):\n",
    "            \n",
    "            if v!=\"m\":\n",
    "                a=min(np.quantile(self.gen[:,i],0.001),np.quantile(self.test_set[:,i],0.001))\n",
    "                b=max(np.quantile(self.gen[:,i],0.999),np.quantile(self.test_set[:,i],0.999))     \n",
    "                \n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h.fill(self.gen[:,i],weight=1/weight)\n",
    "                h2.fill(self.test_set[:,i])\n",
    "                i+=1\n",
    "            else:\n",
    "                a=min(np.quantile(m_t,0.001),np.quantile(m,0.001))\n",
    "                b=max(np.quantile(m_t,0.999),np.quantile(m,0.999))\n",
    "                a=np.quantile(m_t,0.001)\n",
    "                b=np.quantile(m_t,0.999)\n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                bins = h.axes[0].edges\n",
    "                h.fill(m,weight=1/weight)#,weight=1/self.weight)\n",
    "                h2.fill(m_t)\n",
    "            \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            try:\n",
    "                main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "                    h2,\n",
    "                    ax_dict={\"main_ax\":ax[0,k],\"ratio_ax\":ax[1,k]},\n",
    "                    rp_ylabel=r\"Ratio\",\n",
    "                    rp_num_label=\"Flow Generated\",\n",
    "                    rp_denom_label=\"MC Simulatied\",\n",
    "                    rp_uncert_draw_type=\"line\",  # line or bar\n",
    "                )\n",
    "                ax[0,k].set_xlabel(\"\")\n",
    "#                 if quantile and v==\"m\" and plot_vline:\n",
    "#                     ax[0,k].hist(m[m_t<np.quantile(m_t,0.1)],histtype='step',bins=bins,alpha=1,color=\"red\",label=\"10% quantile gen\",hatch=\"/\")\n",
    "#                     ax[0,k].vlines(np.quantile(m_t,0.1),0,np.max(h[:]),color=\"red\",label='10% quantile train')\n",
    "                    \n",
    "                ax[1,k].set_ylim(0.25,2)\n",
    "                ax[0,k].set_xlim(a,b)\n",
    "                ax[1,k].set_xlabel(name,fontweight=\"bold\")\n",
    "                ax[1,k].set_xlim(a,b)\n",
    "                ax[0,k].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "                ax[1,k].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "                \n",
    "#                 if plot_vline:\n",
    "#                        ax[0,k].legend([\"Generated\",\"Training\",\"10% quantile Gen\",\"10% quantile Sim\"] )\n",
    "#                 else:\n",
    "#                       ax[0,k].legend([\"Flow Generated\",\"MC Simulated\"] )\n",
    "            except:\n",
    "                print(\"mass plot failed reverting to simple plot mass bins\")\n",
    "                plt.close()\n",
    "                plt.figure()\n",
    "                _,b,_=plt.hist(m_t,15,label=\"MC Simulated\",alpha=0.5)\n",
    "                plt.hist(m,b,label=\"Flow Generated\",alpha=0.5)\n",
    "                plt.legend()  \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            \n",
    "#             plt.xlabel(name)\n",
    "            plt.tight_layout(pad=1)\n",
    "            k+=1\n",
    "        if save:\n",
    "            if v!=\"m\":\n",
    "                 self.summary.add_figure(\"inclusive\"+v,fig,self.step)\n",
    "            else:\n",
    "                self.summary.add_figure(\"jet_mass\",fig,self.step)\n",
    "#             print(\"added figure\")\n",
    "#             self.summary.close()\n",
    "        else:\n",
    "            plt.savefig(\"{}{}{}_oversample_{}\".format(self.typ,self.c,self.p,v))\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "    def plot_mass(self,m,m_t,save=True,quantile=False,bins=15,plot_vline=False,title=\"\"):\n",
    "        #This creates a histogram of the inclusive distributions and calculates the mass of each jet\n",
    "        #and creates a histogram of that\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        #if quantile, this also creates a histogram of a subsample of the generated data, \n",
    "        # where the mass used to condition the flow is in the first 10% percentile of the simulated mass dist\n",
    "        i=0\n",
    "        k=0\n",
    "        fig,ax=plt.subplots(2,4,gridspec_kw={'height_ratios': [3, 1]},figsize=(20,5))\n",
    "        plt.suptitle(title,fontweight=\"bold\")\n",
    "        for v,name in zip([\"eta\",\"phi\",\"pt\",\"m\"],[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}$\",r\"$p_T^{rel}$\",r\"$m^{rel}$\"]):\n",
    "            \n",
    "            if v!=\"m\":\n",
    "                a=min(np.quantile(self.gen[:,i],0.001),np.quantile(self.test_set[:,i],0.001))\n",
    "                b=max(np.quantile(self.gen[:,i],0.999),np.quantile(self.test_set[:,i],0.999))     \n",
    "                \n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h.fill(self.gen[:,i],weight=1/self.weight)\n",
    "                h2.fill(self.test_set[:,i])\n",
    "                i+=1\n",
    "            else:\n",
    "                a=min(np.quantile(m_t,0.001),np.quantile(m,0.001))\n",
    "                b=max(np.quantile(m_t,0.999),np.quantile(m,0.999))\n",
    "                a=np.quantile(m_t,0.001)\n",
    "                b=np.quantile(m_t,0.999)\n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                bins = h.axes[0].edges\n",
    "                h.fill(m)#,weight=1/self.weight)\n",
    "                h2.fill(m_t)\n",
    "            \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            try:\n",
    "                main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "                    h2,\n",
    "                    ax_dict={\"main_ax\":ax[0,k],\"ratio_ax\":ax[1,k]},\n",
    "                    rp_ylabel=r\"Ratio\",\n",
    "                    rp_num_label=\"Flow Generated\",\n",
    "                    rp_denom_label=\"MC Simulatied\",\n",
    "                    rp_uncert_draw_type=\"line\",  # line or bar\n",
    "                )\n",
    "                ax[0,k].set_xlabel(\"\")\n",
    "#                 if quantile and v==\"m\" and plot_vline:\n",
    "#                     ax[0,k].hist(m[m_t<np.quantile(m_t,0.1)],histtype='step',bins=bins,alpha=1,color=\"red\",label=\"10% quantile gen\",hatch=\"/\")\n",
    "#                     ax[0,k].vlines(np.quantile(m_t,0.1),0,np.max(h[:]),color=\"red\",label='10% quantile train')\n",
    "                    \n",
    "                ax[1,k].set_ylim(0.25,2)\n",
    "                ax[0,k].set_xlim(a,b)\n",
    "                ax[1,k].set_xlabel(name,fontweight=\"bold\")\n",
    "                ax[1,k].set_xlim(a,b)\n",
    "                ax[0,k].set_ylabel(\"Counts\",fontweight=\"bold\" )\n",
    "                ax[1,k].set_ylabel(\"Ratio\",fontweight=\"bold\")\n",
    "                \n",
    "#                 if plot_vline:\n",
    "#                        ax[0,k].legend([\"Generated\",\"Training\",\"10% quantile Gen\",\"10% quantile Sim\"] )\n",
    "#                 else:\n",
    "#                       ax[0,k].legend([\"Flow Generated\",\"MC Simulated\"] )\n",
    "            except:\n",
    "                print(\"mass plot failed reverting to simple plot mass bins\")\n",
    "                plt.close()\n",
    "                plt.figure()\n",
    "                _,b,_=plt.hist(m_t,15,label=\"MC Simulated\",alpha=0.5)\n",
    "                plt.hist(m,b,label=\"Flow Generated\",alpha=0.5)\n",
    "                plt.legend()  \n",
    "            #hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            \n",
    "#             plt.xlabel(name)\n",
    "            plt.tight_layout(pad=1)\n",
    "            k+=1\n",
    "        if save:\n",
    "            if v!=\"m\":\n",
    "                 self.summary.add_figure(\"inclusive\"+v,fig,self.step)\n",
    "            else:\n",
    "                self.summary.add_figure(\"jet_mass\",fig,self.step)\n",
    "#             print(\"added figure\")\n",
    "#             self.summary.close()\n",
    "        else:\n",
    "            plt.savefig(\"inclusive_{}\".format(v))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#     def losses(self,save=False):\n",
    "#         '''This plots the different losses vs epochs'''\n",
    "#         fig=plt.figure()\n",
    "#         hep.cms.label(\"Private Work\",data=None,lumi=None,year=None)\n",
    "#         plt.xlabel('step')\n",
    "#         plt.ylabel('loss')\n",
    "#         ln1=plt.plot(self.model.logprobs,label='log$(p_{gauss}(x_{data}))$')\n",
    "#         if \"calc_massloss\" in self.config.keys() and self.config[\"calc_massloss\"]:\n",
    "#             plt.twinx()\n",
    "#             ln2=plt.plot(self.model.mlosses,label=r'mass mse $\\times$ {}'.format(self.config[\"lambda\"]),color='orange')\n",
    "#             plt.ylabel(\"MSE\")\n",
    "#             plt.yscale(\"log\")\n",
    "#             ln1+=ln2\n",
    "#         labs=[l.get_label() for l in ln1]\n",
    "#         plt.legend(ln1,labs)\n",
    "#         plt.tight_layout(pad=2)\n",
    "#         if save:\n",
    "#             self.summary.add_figure(\"losses\",fig,self.step)\n",
    "# #             self.summary.close()\n",
    "#         else:\n",
    "#             plt.show()\n",
    "   \n",
    "\n",
    "    def plot_correlations(self,save=True):\n",
    "        #Plots correlations between all particles for i=0 eta,i=1 phi,i=2 pt\n",
    "        self.plot_corr(i=0,save=save)\n",
    "        self.plot_corr(i=1,save=save)\n",
    "        self.plot_corr(i=2,save=save)\n",
    "\n",
    "    def plot_corr(self,i=0,names=[\"$\\eta^{rel}$\",\"$\\phi^{rel}$\",\"$p_T$\"],save=True):\n",
    "        if i==2:\n",
    "            c=1\n",
    "        else:\n",
    "            c=0.3\n",
    "        df_g=pd.DataFrame(self.gen[:,:self.n_dim].detach().numpy()[:,range(i,90,3)])\n",
    "        df_h=pd.DataFrame(self.test_set[:,:self.n_dim].detach().numpy()[:,range(i,90,3)])\n",
    "        fig,ax=plt.subplots(ncols=2,figsize=(20,10))\n",
    "        corr_g = ax[0].matshow(df_g.corr())\n",
    "        corr_g.set_clim(-c,c)\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar=fig.colorbar(corr_g,cax=cax)\n",
    "        corr_h = ax[1].matshow(df_h.corr())\n",
    "        corr_h.set_clim(-c,c)\n",
    "        divider = make_axes_locatable(ax[1])\n",
    "        cax2 = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar=fig.colorbar(corr_h,cax=cax2)\n",
    "        plt.suptitle(\"{} Correlation between Particles\".format(names[i]),fontweight=\"bold\")\n",
    "        ax[0].set_title(\"Flow Generated\",fontweight=\"bold\")\n",
    "        ax[1].set_title(\"MC Simulated\",fontweight=\"bold\")\n",
    "        ax[0].set_xlabel(\"Particles\",fontweight=\"bold\")\n",
    "        ax[0].set_ylabel(\"Particles\",fontweight=\"bold\")\n",
    "        ax[1].set_xlabel(\"Particles\",fontweight=\"bold\")\n",
    "        ax[1].set_ylabel(\"Particles\",fontweight=\"bold\")\n",
    "        title=[\"corr_eta\",\"corr_phi\",\"corr_pt\"]\n",
    "        if save:\n",
    "                \n",
    "                self.summary.add_figure(title[i],fig,self.step)    \n",
    "    #             self.summary.close()\n",
    "        else:\n",
    "                #plt.savefig(\"corr_{}\".format(title[i]))\n",
    "                plt.show()\n",
    "\n",
    "    def var_part(self,true,gen,true_n,gen_n,m_true,m_gen,form=2,save=True):\n",
    "        labels=[\"$\\eta^{rel}$\",\"$\\phi^{rel}$\",\"$p^{rel}_T$\",\"$m^{rel}$\"]\n",
    "        names=[\"eta\",\"phi\",\"pt\",\"m\"]\n",
    "        n,counts=torch.unique(true_n,return_counts=True)\n",
    "        for j in range(4):\n",
    "            fig,ax=plt.subplots(ncols=2,nrows=2,figsize=(15,15))\n",
    "\n",
    "            k=-1\n",
    "            ntemp=n[-form**2:]\n",
    "\n",
    "            \n",
    "            for i in list(ntemp)[::-1]: \n",
    "                k+=1\n",
    "                i=int(i)\n",
    "\n",
    "                if names[j]!=\"m\":\n",
    "                    a=np.quantile(self.test_set[true_n.reshape(-1)==i,:].reshape(-1,3)[:,j],0.001)\n",
    "                    b=np.quantile(self.test_set[true_n.reshape(-1)==i,:].reshape(-1,3)[:,j],0.999)    \n",
    "                    h=hist.Hist(hist.axis.Regular(15,a,b))\n",
    "                    h2=hist.Hist(hist.axis.Regular(15,a,b))\n",
    "                    bins = h.axes[0].edges\n",
    "\n",
    "                    ax[k//form,k%form].legend()\n",
    "                    h.fill(self.gen[gen_n.reshape(-1)==i,:].reshape(-1,3)[:,j])\n",
    "                    h2.fill(self.test_set[true_n.reshape(-1)==i,:].reshape(-1,3)[:,j])\n",
    "                    \n",
    "                else:\n",
    "                    a=np.quantile(m_true[true_n.reshape(-1)==i],0.001)\n",
    "                    b=np.quantile(m_gen[gen_n.reshape(-1)==i],0.999)\n",
    "  \n",
    "                    h=hist.Hist(hist.axis.Regular(15,a,b,label=labels[j]))\n",
    "                    h2=hist.Hist(hist.axis.Regular(15,a,b,label=labels[j]))\n",
    "                    bins = h.axes[0].edges\n",
    "                    h.fill(m_gen[gen_n.reshape(-1)==i])\n",
    "                    h2.fill(m_true[true_n.reshape(-1)==i])\n",
    "                    \n",
    "\n",
    "                h.plot1d(    ax=ax[k//2,k%2],label=\"Flow Simulated\")  # line or bar)\n",
    "                h2.plot1d(    ax=ax[k//2,k%2],label=\"MC Generated\")  # line or bar)\n",
    "                ax[k//2,k%2].set_title(\"{} Distribution for jets with {} particles\".format(labels[j],i))\n",
    "\n",
    "                ax[k//2,k%2].set_xlabel(labels[j])\n",
    "                ax[k//2,k%2].set_ylabel(\"Counts\",fontweight=\"bold\")\n",
    "                ax[k//2,k%2].set_xlim(a,b)\n",
    "                ax[k//2,k%2].legend()\n",
    "                #plt.tight_layout(pad=2)\n",
    "\n",
    "            if save:\n",
    "                self.summary.add_figure(\"jet{}_{}\".format(i//3+1,names[i%3]),fig,global_step=self.step)\n",
    "                self.summary.close()\n",
    "            else:\n",
    "                plt.savefig(\"{}{}{}jet{}_{}\".format(self.typ,self.c,self.p,i//3+1,names[j]))\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e941da-a4e5-4983-8b90-f1ab444e8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self, mean=None, std=None, epsilon=1e-7):\n",
    "        \"\"\"Standard Scaler.\n",
    "        The class can be used to normalize PyTorch Tensors using native functions. The module does not expect the\n",
    "        tensors to be of any specific shape; as long as the features are the last dimension in the tensor, the module\n",
    "        will work fine.\n",
    "        :param mean: The mean of the features. The property will be set after a call to fit.\n",
    "        :param std: The standard deviation of the features. The property will be set after a call to fit.\n",
    "        :param epsilon: Used to avoid a Division-By-Zero exception.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, values):\n",
    "        dims = list(range(values.dim() - 1))\n",
    "        self.mean = torch.mean(values, dim=dims)\n",
    "        self.std = torch.std(values, dim=dims)\n",
    "\n",
    "    def transform(self, values):\n",
    "        return (values - self.mean) / (self.std + self.epsilon)\n",
    "    def inverse_transform(self,values):\n",
    "        return (values *self.std)+self.mean\n",
    "    def fit_transform(self, values):\n",
    "        self.fit(values)\n",
    "        return self.transform(values)\n",
    "    def to(self,dev):\n",
    "        self.std=self.std.to(dev)\n",
    "        self.mean=self.mean.to(dev)\n",
    "        return self\n",
    "  \n",
    "\n",
    "    \n",
    "class JetNetDataloader(pl.LightningDataModule):\n",
    "    '''This is more or less standard boilerplate coded that builds the data loader of the training\n",
    "       one thing to note is the custom standard scaler that works on tensors\n",
    "       Currently only jets with 30 particles are used but this maybe changes soon'''\n",
    "    def __init__(self,batch_size,parton): \n",
    "        super().__init__()\n",
    "        self.parton=parton\n",
    "        self.batch_size=batch_size\n",
    "    def setup(self,stage):\n",
    "    # This just sets up the dataloader, nothing particularly important. it reads in a csv, calculates mass and reads out the number particles per jet\n",
    "    # And adds it to the dataset as variable. The only important thing is that we add noise to zero padded jets\n",
    "        data_dir=os.environ[\"HOME\"]+\"/JetNet_NF/train_{}_jets.csv\".format(self.parton)\n",
    "        data=pd.read_csv(data_dir,sep=\" \",header=None)\n",
    "        jets=[]\n",
    "        \n",
    "        for njets in range(1,31):\n",
    "            masks=np.sum(data.values[:,np.arange(3,120,4)],axis=1)\n",
    "            df=data.loc[masks==njets,:]\n",
    "            df=df.drop(np.arange(3,120,4),axis=1)\n",
    "            df[\"n\"]=njets\n",
    "            if len(df)>100:\n",
    "                jets.append(df)\n",
    "        #stacking together differnet samples with different number particles per jet\n",
    "        self.n=torch.empty((0,1))\n",
    "        \n",
    "        self.data=torch.empty((0,90))\n",
    "        self.masks=torch.empty((0,30))\n",
    "        for i in range(len(jets)):\n",
    "            \n",
    "            x=torch.tensor(jets[i].drop([\"n\"],axis=1).values).float()\n",
    "            n=torch.tensor(jets[i][\"n\"].values).int()\n",
    "            \n",
    "            m=torch.ones((len(jets[i]),30)).float()\n",
    "            m[:,n[0]+1:]=0\n",
    "                         \n",
    "            self.data=torch.vstack((self.data,x))\n",
    "            self.n=torch.vstack((self.n.reshape(-1,1),n.reshape(-1,1)))        \n",
    "            self.masks=torch.vstack((self.masks,m))\n",
    "            \n",
    "#         if self.config[\"canonical\"]:\n",
    "#             self.data=preprocess(self.data)        \n",
    "        # calculating mass per jet\n",
    "        self.m=mass(self.data).reshape(-1,1) \n",
    "        # Adding noise to zero padded jets.\n",
    "        for i in torch.unique(self.n):\n",
    "            i=int(i)\n",
    "            self.data[self.n.flatten()==i,3*i:90]=torch.normal(mean=torch.zeros_like(self.data[self.n.flatten()==i,3*i:90]),std=1).abs()*1e-7\n",
    "        #standard scaling \n",
    "        shitscaling=False\n",
    "        if shitscaling:\n",
    "            self.scaler=StandardScaler()\n",
    "            self.scaler2=PowerTransformer(method='box-cox')\n",
    "\n",
    "    #         self.data=torch.hstack((self.data,self.m))        \n",
    "\n",
    "            self.scaler.fit(self.data.reshape(-1,30,3)[:,:,:2].reshape(-1,60))\n",
    "            self.scaler2.fit(self.data.reshape(-1,30,3)[:,:,2].reshape(-1,30))\n",
    "            self.data=self.data.reshape(-1,30,3)\n",
    "            self.data[:,:,:2]=self.scaler.transform(self.data[:,:,:2].reshape(-1,60)).reshape(-1,30,2)\n",
    "            self.data[:,:,2]=torch.tensor(self.scaler2.transform(self.data[:,:,2].reshape(-1,30))).reshape(-1,30)\n",
    "#         self.min_m=self.scaler.transform(torch.zeros((1,91)))[0,-1]\n",
    "#         self.data=torch.hstack((self.data,self.n))\n",
    "        else:\n",
    "            self.scaler=StandardScaler()\n",
    "            self.scaler.fit(self.data)\n",
    "            self.data=self.scaler.transform(self.data)\n",
    "        self.data=self.data.reshape(-1,90)\n",
    "        \n",
    "        #calculating mass dist in different bins, this is needed for the testcase where we need to generate the conditoon\n",
    "#         if self.config[\"variable\"]:\n",
    "#             self.mdists={}\n",
    "#             for i in torch.unique(self.n):\n",
    "#                 self.mdists[int(i)]=F(self.data[self.n[:,0]==i,-2])    \n",
    "        self.data,self.test_set,self.mask_data,self.mask_test=train_test_split(self.data.cpu().numpy(),self.masks.cpu().numpy(),test_size=0.3)\n",
    "        \n",
    "        self.n_train=self.data[:,-1]\n",
    "        self.n_test=self.test_set[:,-1]\n",
    "        self.test_set=torch.tensor(self.test_set).float()\n",
    "        self.data=torch.tensor(self.data).float()\n",
    "        self.data=torch.hstack((self.data,torch.tensor(self.mask_data)))\n",
    "        self.test_set=torch.hstack((self.test_set,torch.tensor(self.mask_test)))\n",
    "#         assert self.data.shape[1]==92\n",
    "#         assert (torch.isnan(self.data)).sum()==0\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=len(self.test_set))\n",
    "JetNetDataloader(1024,\"t\").setup(\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba1bee9-03a0-4fb9-8f86-e8d1fbb64d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "# class chamfer_3DFunction(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, xyz1, xyz2):\n",
    "#         batchsize, n, _ = xyz1.size()\n",
    "#         _, m, _ = xyz2.size()\n",
    "#         device = xyz1.device\n",
    "\n",
    "#         dist1 = torch.zeros(batchsize, n)\n",
    "#         dist2 = torch.zeros(batchsize, m)\n",
    "\n",
    "#         idx1 = torch.zeros(batchsize, n).type(torch.IntTensor)\n",
    "#         idx2 = torch.zeros(batchsize, m).type(torch.IntTensor)\n",
    "\n",
    "#         dist1 = dist1.to(device)\n",
    "#         dist2 = dist2.to(device)\n",
    "#         idx1 = idx1.to(device)\n",
    "#         idx2 = idx2.to(device)\n",
    "#         torch.cuda.set_device(device)\n",
    "\n",
    "#         chamfer_3D.forward(xyz1, xyz2, dist1, dist2, idx1, idx2)\n",
    "#         ctx.save_for_backward(xyz1, xyz2, idx1, idx2)\n",
    "#         return dist1, dist2, idx1, idx2\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, graddist1, graddist2, gradidx1, gradidx2):\n",
    "#         xyz1, xyz2, idx1, idx2 = ctx.saved_tensors\n",
    "#         graddist1 = graddist1.contiguous()\n",
    "#         graddist2 = graddist2.contiguous()\n",
    "#         device = graddist1.device\n",
    "\n",
    "#         gradxyz1 = torch.zeros(xyz1.size())\n",
    "#         gradxyz2 = torch.zeros(xyz2.size())\n",
    "\n",
    "#         gradxyz1 = gradxyz1.to(device)\n",
    "#         gradxyz2 = gradxyz2.to(device)\n",
    "#         chamfer_3D.backward(\n",
    "#             xyz1, xyz2, gradxyz1, gradxyz2, graddist1, graddist2, idx1, idx2\n",
    "#         )\n",
    "#         return gradxyz1, gradxyz2\n",
    "# class cd(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(cd, self).__init__()\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         input1 = input1.contiguous()\n",
    "#         input2 = input2.contiguous()\n",
    "#         return chamfer_3DFunction.apply(input1, input2)\n",
    "def calc_dcd(x, gt, alpha=1000, n_lambda=1, return_raw=False, non_reg=False):\n",
    "    x = x.float()\n",
    "    gt = gt.float()\n",
    "    batch_size, n_x, _ = x.shape\n",
    "    batch_size, n_gt, _ = gt.shape\n",
    "    assert x.shape[0] == gt.shape[0]\n",
    "\n",
    "    if non_reg:\n",
    "        frac_12 = max(1, n_x / n_gt)\n",
    "        frac_21 = max(1, n_gt / n_x)\n",
    "    else:\n",
    "        frac_12 = n_x / n_gt\n",
    "        frac_21 = n_gt / n_x\n",
    "\n",
    "    cd_p, cd_t, dist1, dist2, idx1, idx2 = calc_cd(x, gt, return_raw=True)\n",
    "    # dist1 (batch_size, n_gt): a gt point finds its nearest neighbour x' in x;\n",
    "    # idx1  (batch_size, n_gt): the idx of x' \\in [0, n_x-1]\n",
    "    # dist2 and idx2: vice versa\n",
    "    exp_dist1, exp_dist2 = torch.exp(-dist1 * alpha), torch.exp(-dist2 * alpha)\n",
    "\n",
    "    count1 = torch.zeros_like(idx2)\n",
    "    count1.scatter_add_(1, idx1.long(), torch.ones_like(idx1))\n",
    "    weight1 = count1.gather(1, idx1.long()).float().detach() ** n_lambda\n",
    "    weight1 = (weight1 + 1e-6) ** (-1) * frac_21\n",
    "    loss1 = (1 - exp_dist1 * weight1).mean(dim=1)\n",
    "\n",
    "    count2 = torch.zeros_like(idx1)\n",
    "    count2.scatter_add_(1, idx2.long(), torch.ones_like(idx2))\n",
    "    weight2 = count2.gather(1, idx2.long()).float().detach() ** n_lambda\n",
    "    weight2 = (weight2 + 1e-6) ** (-1) * frac_12\n",
    "    loss2 = (1 - exp_dist2 * weight2).mean(dim=1)\n",
    "\n",
    "    loss = (loss1 + loss2) / 2\n",
    "\n",
    "    res = [loss, cd_p, cd_t]\n",
    "    if return_raw:\n",
    "        res.extend([dist1, dist2, idx1, idx2])\n",
    "\n",
    "    return res\n",
    "\n",
    "def calc_cd(output, gt, calc_f1=False, return_raw=False, normalize=False, separate=False):\n",
    "    # cham_loss = dist_chamfer_3D.chamfer_3DDist()\n",
    "#     cham_loss = distChamfer(output,gt)\n",
    "    dist1, dist2, idx1, idx2 = distChamfer(gt, output)\n",
    "    cd_p = (torch.sqrt(dist1).mean(1) + torch.sqrt(dist2).mean(1)) / 2\n",
    "    cd_t = (dist1.mean(1) + dist2.mean(1))\n",
    "\n",
    "    if separate:\n",
    "        res = [torch.cat([torch.sqrt(dist1).mean(1).unsqueeze(0), torch.sqrt(dist2).mean(1).unsqueeze(0)]),\n",
    "               torch.cat([dist1.mean(1).unsqueeze(0),dist2.mean(1).unsqueeze(0)])]\n",
    "    else:\n",
    "        res = [cd_p, cd_t]\n",
    "    if calc_f1:\n",
    "        f1, _, _ = fscore(dist1, dist2)\n",
    "        res.append(f1)\n",
    "    if return_raw:\n",
    "        res.extend([dist1, dist2, idx1, idx2])\n",
    "    return res\n",
    "\n",
    "def pairwise_dist(x, y):\n",
    "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "    rx = xx.diag().unsqueeze(0).expand_as(xx)\n",
    "    ry = yy.diag().unsqueeze(0).expand_as(yy)\n",
    "    P = rx.t() + ry - 2 * zz\n",
    "    return P\n",
    "\n",
    "\n",
    "def NN_loss(x, y, dim=0):\n",
    "    dist = pairwise_dist(x, y)\n",
    "    values, indices = dist.min(dim=dim)\n",
    "    return values.mean()\n",
    "\n",
    "\n",
    "def distChamfer(a, b):\n",
    "    \"\"\"\n",
    "    :param a: Pointclouds Batch x nul_points x dim\n",
    "    :param b:  Pointclouds Batch x nul_points x dim\n",
    "    :return:\n",
    "    -closest point on b of points from a\n",
    "    -closest point on a of points from b\n",
    "    -idx of closest point on b of points from a\n",
    "    -idx of closest point on a of points from b\n",
    "    Works for pointcloud of any dimension\n",
    "    \"\"\"\n",
    "    x, y = a.double(), b.double()\n",
    "    bs, num_points_x, points_dim = x.size()\n",
    "    bs, num_points_y, points_dim = y.size()\n",
    "\n",
    "    xx = torch.pow(x, 2).sum(2)\n",
    "    yy = torch.pow(y, 2).sum(2)\n",
    "    zz = torch.bmm(x, y.transpose(2, 1))\n",
    "    rx = xx.unsqueeze(1).expand(bs, num_points_y, num_points_x)  # Diagonal elements xx\n",
    "    ry = yy.unsqueeze(1).expand(bs, num_points_x, num_points_y)  # Diagonal elements yy\n",
    "    P = rx.transpose(2, 1) + ry - 2 * zz\n",
    "    return (\n",
    "        torch.min(P, 2)[0].float(),\n",
    "        torch.min(P, 1)[0].float(),\n",
    "        torch.min(P, 2)[1].int(),\n",
    "        torch.min(P, 1)[1].int(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09aec8a0-d0ab-4c49-8e0c-a28ec64d049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcd(x, gt, alpha=1, lpnorm=2.0, n_lambda=1, non_reg=False):\n",
    "    x = x.float()\n",
    "    gt = gt.float()\n",
    "    batch_size, n_x, _ = x.shape\n",
    "    batch_size, n_gt, _ = gt.shape\n",
    "    assert x.shape[0] == gt.shape[0]\n",
    "\n",
    "    if non_reg:\n",
    "        frac_12 = max(1, n_x / n_gt)\n",
    "        frac_21 = max(1, n_gt / n_x)\n",
    "    else:\n",
    "        frac_12 = n_x / n_gt\n",
    "        frac_21 = n_gt / n_x\n",
    "\n",
    "    dist1, dist2, idx1, idx2 = distChamfer(x, gt, lpnorm=lpnorm)\n",
    "    # dist1 (batch_size, n_gt): a gt point finds its nearest neighbour x' in x;\n",
    "    # idx1  (batch_size, n_gt): the idx of x' \\in [0, n_x-1]\n",
    "    # dist2 and idx2: vice versa\n",
    "    exp_dist1, exp_dist2 = torch.exp(-dist1 * alpha), torch.exp(-dist2 * alpha)\n",
    "\n",
    "    count1 = torch.zeros_like(idx2)\n",
    "    count1.scatter_add_(1, idx1.long(), torch.ones_like(idx1))\n",
    "    weight1 = count1.gather(1, idx1.long()).float().detach() ** n_lambda\n",
    "    weight1 = (weight1 + 1e-6) ** (-1) * frac_21\n",
    "    loss1 = (1 - exp_dist1 * weight1).mean(dim=1)\n",
    "\n",
    "    count2 = torch.zeros_like(idx1)\n",
    "    count2.scatter_add_(1, idx2.long(), torch.ones_like(idx2))\n",
    "    weight2 = count2.gather(1, idx2.long()).float().detach() ** n_lambda\n",
    "    weight2 = (weight2 + 1e-6) ** (-1) * frac_12\n",
    "    loss2 = (1 - exp_dist2 * weight2).mean(dim=1)\n",
    "\n",
    "    return (loss1 + loss2) / 2\n",
    "\n",
    "\n",
    "def cd(output, gt, lpnorm=2.0):\n",
    "    dist1, dist2, _, _ = distChamfer2(gt, output, lpnorm=lpnorm)\n",
    "    cd_p = (dist1.mean(1) + dist2.mean(1)) / 2\n",
    "    return cd_p\n",
    "#     cd_p = (torch.sqrt(dist1).mean(1) + torch.sqrt(dist2).mean(1)) / 2\n",
    "#     cd_t = dist1.mean(1) + dist2.mean(1)\n",
    "\n",
    "    # , calc_f1=False, separate=False\n",
    "#     if separate:\n",
    "#         res = [\n",
    "#             torch.cat(\n",
    "#                 [\n",
    "#                     torch.sqrt(dist1).mean(1).unsqueeze(0),\n",
    "#                     torch.sqrt(dist2).mean(1).unsqueeze(0),\n",
    "#                 ]\n",
    "#             ),\n",
    "#             torch.cat([dist1.mean(1).unsqueeze(0), dist2.mean(1).unsqueeze(0)]),\n",
    "#         ]\n",
    "#     else:\n",
    "#         res = [cd_p, cd_t]\n",
    "#     if calc_f1:\n",
    "#         f1, _, _ = fscore(dist1, dist2)\n",
    "#         res.append(f1)\n",
    "#     return res\n",
    "\n",
    "\n",
    "def distChamfer2(a, b, lpnorm=2):\n",
    "    \"\"\"\n",
    "    :param a: Pointclouds Batch x nul_points x dim\n",
    "    :param b:  Pointclouds Batch x nul_points x dim\n",
    "    :return:\n",
    "    -closest point on b of points from a\n",
    "    -closest point on a of points from b\n",
    "    -idx of closest point on b of points from a\n",
    "    -idx of closest point on a of points from b\n",
    "    Works for pointcloud of any dimension\n",
    "    \"\"\"\n",
    "    # original implementation equivalent to\n",
    "    # P = torch.pow(torch.cdist(a, b, p=lpnorm), lpnorm)\n",
    "    P = torch.cdist(a, b, p=lpnorm)\n",
    "    return (\n",
    "        torch.min(P, 2)[0],\n",
    "        torch.min(P, 1)[0],\n",
    "        torch.min(P, 2)[1].int(),\n",
    "        torch.min(P, 1)[1].int(),\n",
    "    )\n",
    "\n",
    "\n",
    "def fscore(dist1, dist2, threshold=0.0001):\n",
    "    \"\"\"\n",
    "    Calculates the F-score between two point clouds with the corresponding threshold value.\n",
    "    :param dist1: Batch, N-Points\n",
    "    :param dist2: Batch, N-Points\n",
    "    :param th: float\n",
    "    :return: fscore, precision, recall\n",
    "    \"\"\"\n",
    "    # NB : In this depo, dist1 and dist2 are squared pointcloud\n",
    "    # euclidean distances, so you should adapt the threshold accordingly.\n",
    "    precision_1 = torch.mean((dist1 < threshold).float(), dim=1)\n",
    "    precision_2 = torch.mean((dist2 < threshold).float(), dim=1)\n",
    "    fscore = 2 * precision_1 * precision_2 / (precision_1 + precision_2)\n",
    "    fscore[torch.isnan(fscore)] = 0\n",
    "    return fscore, precision_1, precision_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e6f95c-5bcf-4065-bb55-3c146e46a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention,TransformerEncoder,TransformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47a1197-d9a8-4ae1-86c0-f9c025ed4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn1 = MultiheadAttention(dim_feedforward, dim_feedforward, num_heads)\n",
    "#         self.self_attn2 = MultiheadAttention(dim_feedforward, dim_feedforward, num_heads)\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(dim_feedforward, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "        \n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(dim_feedforward)\n",
    "        self.norm2 = nn.LayerNorm(dim_feedforward)\n",
    "        self.norm3 = nn.LayerNorm(dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, z,e_out,src_mask=None,trg_mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn1(z,z,z, mask=trg_mask)\n",
    "        z = z + self.dropout(attn_out)\n",
    "        z = self.norm1(z)\n",
    "#         z = z+self.self_attn2(z,e_out,e_out,src_mask)\n",
    "#         z = self.norm2(z)\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(z)\n",
    "#         z = z + self.dropout(linear_out)\n",
    "        x = self.norm2(z)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers,input_dim, num_heads, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderBlock(input_dim, num_heads, dim_feedforward) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, z,e_outputs,src_mask=None,trg_mask=None):\n",
    "        for l in self.layers:\n",
    "            z = l(z,e_outputs,src_mask,trg_mask)\n",
    "        return z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35b516d-dcfd-4bf0-a467-7b20c313e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from mp import *\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "bce = torch.nn.BCELoss()\n",
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "class TransformerPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, lr, warmup,max_iters, dropout=0.0, input_dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Hidden dimensionality of the input\n",
    "            model_dim - Hidden dimensionality to use inside the Transformer\n",
    "            num_classes - Number of classes to predict per sequence element\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - Number of encoder blocks to use.\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout - Dropout to apply inside the model\n",
    "            input_dropout - Dropout to apply on the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "        self.loss=lambda x,y: (x-y)**2\n",
    "        self.metrics={\"val_w1p\":[],\"val_w1m\":[],\"val_w1efp\":[],\"val_cov\":[],\"val_mmd\":[],\"val_fpnd\":[],\"val_logprob\":[],\"step\":[]}\n",
    "\n",
    "    def _create_model(self):\n",
    "        self.encoder=TransformerEncoderLayer(d_model=3,nhead=3,dim_feedforward=128,activation=\"relu\")\n",
    "        self.encoder=TransformerEncoder(self.encoder,num_layers=6)\n",
    "        self.disc=MPDiscriminator(num_particles=30,input_node_size=2)\n",
    "#  \n",
    "    def to(self,dev):\n",
    "#         self.input_net=self.input_net.to(dev)\n",
    "\n",
    "#         self.mu=self.mu.to(dev)\n",
    "#         self.sigma=self.sigma.to(dev)\n",
    "#         self.decoder=self.decoder.to(dev)\n",
    "        self.encoder=self.encoder.to(dev)\n",
    "        self.disc=self.disc.to(dev)\n",
    "#         self.N.loc = self.N.loc.to(dev) # hack to get sampling on the GPU\n",
    "#         self.N.scale = self.N.scale.to(dev)\n",
    "        \n",
    "    def calc_G_loss(self,fake_outputs):\n",
    "        \"\"\"Calculates generator loss for the different possible loss functions\"\"\"\n",
    "        Y_real = torch.ones(fake_outputs.shape[0], 1, device=fake_outputs.device)\n",
    "        loss=\"og\"\n",
    "        if loss == \"og\":\n",
    "            G_loss = bce(fake_outputs, Y_real)\n",
    "        elif loss == \"ls\":\n",
    "            G_loss = mse(fake_outputs, Y_real)\n",
    "        elif loss == \"w\" or loss == \"hinge\":\n",
    "            G_loss = -fake_outputs.mean()\n",
    "\n",
    "        return G_loss\n",
    "\n",
    "    \n",
    "    def calc_D_loss(self,\n",
    "        real_outputs,\n",
    "        fake_outputs,\n",
    "        gp_lambda=0,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        calculates discriminator loss for the different possible loss functions\n",
    "        + optionally applying label smoothing, label flipping, or a gradient penalty\n",
    "        returns individual loss contributions as well for evaluation and plotting\n",
    "        \"\"\"\n",
    "\n",
    "        Y_real = torch.ones(len(real_outputs), 1).to(self.device)\n",
    "        Y_fake = torch.zeros(len(fake_outputs), 1).to(self.device)\n",
    "\n",
    "        loss=\"w\" \n",
    "        if loss == \"og\":\n",
    "            D_real_loss = bce(real_outputs, Y_real)\n",
    "            D_fake_loss = bce(fake_outputs, Y_fake)\n",
    "        elif loss == \"ls\":\n",
    "            D_real_loss = mse(real_outputs, Y_real)\n",
    "            D_fake_loss = mse(fake_outputs, Y_fake)\n",
    "        elif loss == \"w\":\n",
    "            D_real_loss = -real_outputs.mean()\n",
    "            D_fake_loss = fake_outputs.mean()\n",
    "\n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "        if gp_lambda:\n",
    "            gp = gradient_penalty(gp_lambda, D, data, gen_data, run_batch_size, device, model)\n",
    "            gpitem = gp.item()\n",
    "            D_loss += gp\n",
    "        else:\n",
    "            gpitem = None\n",
    "\n",
    "        return (\n",
    "            D_loss\n",
    "    #         {\n",
    "    #             \"Dr\": D_real_loss.item(),\n",
    "    #             \"Df\": D_fake_loss.item(),\n",
    "    #             \"gp\": gpitem,\n",
    "    #             \"D\": D_real_loss.item() + D_fake_loss.item(),\n",
    "    #         },\n",
    "        )\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.disc.parameters(), lr=self.hparams.lr)\n",
    "        optimizer_g = torch.optim.Adam(self.encoder.parameters(), lr=self.hparams.lr)\n",
    "        # Apply lr scheduler per step\n",
    "        lr_scheduler = CosineWarmupScheduler(optimizer,\n",
    "                                             warmup=self.hparams.warmup,\n",
    "                                             max_iters=self.hparams.max_iters)\n",
    "        return optimizer,optimizer_g#],[{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
    "\n",
    "\n",
    "\n",
    "    def load_datamodule(self,data_module):\n",
    "        '''needed for lightning training to work, it just sets the dataloader for training and validation'''\n",
    "        self.data_module=data_module   \n",
    "\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx,optimizer_idx):\n",
    "        self.to(\"cuda\")\n",
    "        d_opt, _opt = self.optimizers()\n",
    "        batch,mask=batch[:,:90],batch[:,-30:]\n",
    "        batch=batch[:,:90].reshape(-1,30,3)\n",
    "        gen=self.encoder(batch)#,z,trg_mask=None\n",
    "\n",
    "        y_real=self.disc(batch)\n",
    "        y_gen=self.disc(gen)\n",
    "        if optimizer_idx==0:\n",
    "\n",
    "            l_d=self.calc_D_loss(y_real,y_gen)\n",
    "            print(l_d)\n",
    "            return l_d\n",
    "#         l=l.mean()#+self.kl\n",
    "        if optimizer_idx==1:\n",
    "            l_g=self.calc_G_loss(y_gen)\n",
    "            print(l_g)\n",
    "            return l_g\n",
    "#         self.log(\"loss\",l,logger=True,on_step=True,on_epoch=True)\n",
    "       # self.log(\"kl\",l,logger=True,on_step=True,on_epoch=True)\n",
    "        return OrderedDict({\"loss\":l})\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''This calculates some important metrics on the hold out set (checking for overtraining)'''\n",
    "        self.data_module.scaler.to(\"cpu\")  \n",
    "        self.to(\"cpu\")\n",
    "        batch,mask=batch[:,:90],batch[:,-30:]\n",
    "        batch=batch.to(\"cpu\")\n",
    "        mask=mask.to(\"cpu\")\n",
    "        mask,trg_mask=self.create_masks(mask,mask)\n",
    "        mask=mask.unsqueeze(1).unsqueeze(1)\n",
    "        trg_mask=trg_mask.unsqueeze(1).unsqueeze(1)\n",
    "        z=torch.randn((len(batch),30,3))\n",
    "        self.n_dim=90\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test=self.forward(batch[:,:90].reshape(-1,30,3)).reshape(-1,90)#,mask\n",
    "            loss=self.loss(test.reshape(-1,30,3),batch.reshape(-1,30,3))\n",
    "            loss=loss[0].mean()+loss[1].mean()\n",
    "        self.log(\"val_loss\",loss,logger=True)\n",
    "        assert (test==test).all()\n",
    "        # Reverse Standard Scaling (this has nothing to do with flows, it is a standard preprocessing step)\n",
    "        shitscaling=False\n",
    "        if shitscaling:\n",
    "            test=test.reshape(-1,30,3)\n",
    "\n",
    "            test[:,:,-1]=torch.tensor(self.data_module.scaler2.inverse_transform(test.reshape(-1,30,3)[:,:,2].reshape(-1,30)).reshape(-1,30))\n",
    "            test[:,:,:2]=self.data_module.scaler.inverse_transform(test.reshape(-1,30,3)[:,:,:2].reshape(-1,60)).reshape(-1,30,2)\n",
    "        # gen=self.data_module.scaler.inverse_transform(gen)\n",
    "            test=test.reshape(-1,90)\n",
    "            true=batch.reshape(-1,30,3)\n",
    "            true[:,:,-1]=torch.tensor(self.data_module.scaler2.inverse_transform(true.reshape(-1,30,3)[:,:,2].reshape(-1,30)).reshape(-1,30))\n",
    "            true[:,:,:2]=self.data_module.scaler.inverse_transform(true.reshape(-1,30,3)[:,:,:2].reshape(-1,60)).reshape(-1,30,2)\n",
    "            true=true.reshape(-1,90)\n",
    "        else:\n",
    "            test=self.data_module.scaler.inverse_transform(test)\n",
    "            true=self.data_module.scaler.inverse_transform(batch)\n",
    "            \n",
    "            \n",
    "        #         true=self.data_module.scaler2.inverse_transform(batch.numpy())[:,:90]\n",
    "        # We overwrite in cases where n is smaller 30 the particles after n with 0\n",
    "        # if self.config[\"context_features\"]>1:\n",
    "        #     for i in torch.unique(batch[:,-1]):\n",
    "        #         i=int(i)\n",
    "        #         gen[c[:,-1]==i,3*i:]=0\n",
    "        #         test[c_test[:,-1]==i,3*i:-1]=0\n",
    "        #This is just a nice check to see whether we overtrain\n",
    "        m_t=mass(true[:,:self.n_dim].to(self.device)).cpu()\n",
    "        # m_gen=mass(gen[:,:self.n_dim],self.config[\"canonical\"]).cpu()\n",
    "        m_test=mass(test[:,:self.n_dim]).cpu()\n",
    "        # gen=torch.column_stack((gen[:,:90],m_gen))\n",
    "        test=torch.column_stack((test[:,:90],m_test))       \n",
    "        # Again checking for overtraining\n",
    "        mse=FF.mse_loss(m_t,m_test).detach()\n",
    "       \n",
    "        # For one metric the pt needs to always be bigger or equal 0, so we overwrite the cases where it isnt (its not physical possible to ahve pt smaller 0)\n",
    "        for i in range(30):\n",
    "            i=2+3*i\n",
    "            # gen[gen[:,i]<0,i]=0\n",
    "            test[test[:,i]<0,i]=0\n",
    "            true[true[:,i]<0,i]=0\n",
    "          #Some metrics we track\n",
    "        cov,mmd=cov_mmd(test[:,:self.n_dim].reshape(-1,self.n_dim//3,3),true[:,:self.n_dim].reshape(-1,self.n_dim//3,3),use_tqdm=False)\n",
    "\n",
    "        fpndv=fpnd(test[:,:self.n_dim].reshape(-1,self.n_dim//3,3).numpy(),use_tqdm=False,jet_type=\"t\")\n",
    "\n",
    "        self.metrics[\"val_fpnd\"].append(fpndv)\n",
    "        self.metrics[\"val_mmd\"].append(mmd)\n",
    "        self.metrics[\"val_cov\"].append(cov)\n",
    "        self.metrics[\"val_w1p\"].append(w1p(test[:,:self.n_dim].reshape(-1,self.n_dim//3,3),true[:,:self.n_dim].reshape(-1,self.n_dim//3,3)))\n",
    "        self.metrics[\"val_w1m\"].append(w1m(test[:,:self.n_dim].reshape(-1,self.n_dim//3,3),true[:,:self.n_dim].reshape(-1,self.n_dim//3,3)))\n",
    "        try:\n",
    "            self.metrics[\"val_w1efp\"].append(w1efp(test[:,:self.n_dim].reshape(-1,self.n_dim//3,3),true[:,:self.n_dim].reshape(-1,self.n_dim//3,3)))\n",
    "        except:\n",
    "            self.metrics[\"val_w1efp\"]=None\n",
    "        self.log(\"val_w1m\",self.metrics[\"val_w1m\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1p\",self.metrics[\"val_w1p\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1efp\",self.metrics[\"val_w1efp\"][-1][0],on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_cov\",cov,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_fpnd\",fpndv,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mmd\",mmd,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mse\",mse,prog_bar=True,logger=True,on_step=False, on_epoch=True)\n",
    "        # This part here adds the plots to tensorboard\n",
    "        self.plot=plotting(model=self,gen=test[:,:self.n_dim],true=true[:,:self.n_dim],step=self.global_step,logger=self.logger.experiment)\n",
    "        \n",
    "        try:\n",
    "            self.plot.plot_mass(m_test.cpu().numpy(),m_t.cpu().numpy(),save=True,bins=15,quantile=True,plot_vline=False)\n",
    "#             self.plot.plot_marginals(save=True)\n",
    "#             self.plot.plot_2d(save=True)\n",
    "#             self.plot.losses(save=True)\n",
    "#             self.plot.var_part(true=true[:,:self.n_dim],gen=test[:,:self.n_dim],true_n=n_true,gen_n=n_test,m_true=m_t,m_gen=m_test ,save=True)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            \n",
    "# #         self.flow.to(\"cuda\")\n",
    "#         self.plot.plot_correlations()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea594cc-780f-497a-8440-99f173805c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder | TransformerEncoder | 5.8 K \n",
      "1 | disc    | MPDiscriminator    | 355 K \n",
      "-----------------------------------------------\n",
      "360 K     Trainable params\n",
      "0         Non-trainable params\n",
      "360 K     Total params\n",
      "1.444     Total estimated model params size (MB)\n",
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b144cd6df634395844e6aa263f5e057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0648, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [32,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [35,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [38,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [43,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [45,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [49,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [50,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [51,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [55,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [56,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [66,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [76,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [78,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [81,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [88,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [92,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [17,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    807\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    808\u001b[0m )\n\u001b[0;32m--> 809\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1234\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     87\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m _get_active_optimizers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madvance\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_position\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;66;03m# would be skipped otherwise\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:369\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTPUAccelerator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_native_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAMPType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNATIVE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1593\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1593\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1625\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mOverride this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03meach optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \n\u001b[1;32m   1624\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1625\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:193\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:160\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/optim/adam.py:100\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 100\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:145\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mThe closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mconsistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 134\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:427\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1763\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:333\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mTransformerPredictor.training_step\u001b[0;34m(self, batch, batch_idx, optimizer_idx)\u001b[0m\n\u001b[1;32m    140\u001b[0m l_g\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_G_loss(y_gen)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml_g\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m l_g\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/_tensor.py:305\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/_tensor_str.py:434\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/_tensor_str.py:409\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/_tensor_str.py:264\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/_tensor_str.py:100\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,logger\u001b[38;5;241m=\u001b[39mlogger,   \u001b[38;5;66;03m# auto_scale_batch_size=\"binsearch\",\u001b[39;00m\n\u001b[1;32m     18\u001b[0m                       max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, num_sanity_val_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     19\u001b[0m                       check_val_every_n_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,track_grad_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     20\u001b[0m                      fast_dev_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# This calls the fit function which trains the model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 768\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:736\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mreconciliate_processes(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exception)\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1298\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;124;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;124;03mCallback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_dispatch(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_loop\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py:93\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mteardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# GPU teardown\u001b[39;00m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:444\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mteardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;124;03m\"\"\"This method is called to teardown the training process.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    It is the right place to release memory and free other resources.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43moptimizers_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mteardown()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py:27\u001b[0m, in \u001b[0;36moptimizers_to_device\u001b[0;34m(optimizers, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"Moves optimizer states for a sequence of optimizers to the device.\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizers:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43moptimizer_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py:33\u001b[0m, in \u001b[0;36moptimizer_to_device\u001b[0;34m(optimizer, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"Moves the state of a single optimizer to the device.\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, v \u001b[38;5;129;01min\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstate[p] \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove_data_to_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:107\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 107\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_none \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         out\u001b[38;5;241m.\u001b[39mappend((k, v))\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:354\u001b[0m, in \u001b[0;36mmove_data_to_device\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    353\u001b[0m dtype \u001b[38;5;241m=\u001b[39m (TransferableDataType, Batch) \u001b[38;5;28;01mif\u001b[39;00m _TORCHTEXT_LEGACY \u001b[38;5;28;01melse\u001b[39;00m TransferableDataType\n\u001b[0;32m--> 354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:347\u001b[0m, in \u001b[0;36mmove_data_to_device.<locals>.batch_to\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _CPU_DEVICES:\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_blocking\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m data_output \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "data_module = JetNetDataloader(120,\"t\") #this loads the data\n",
    "model = TransformerPredictor(input_dim=3,max_iters=10000, \n",
    "                             model_dim=3, num_heads=1, num_layers=1, lr=0.01, warmup=50,\n",
    "                             dropout=0.0, input_dropout=0.0) # the sets up the model,  config are hparams we want to optimize\n",
    "# Callbacks to use during the training, we  checkpoint our models\n",
    "\n",
    "# if True:#load_ckpt:\n",
    "#     model = model.load_from_checkpoint(\"/beegfs/desy/user/kaechben/t/2022_06_22-10_50-16/epoch=1749-val_logprob=0.94-val_w1m=0.0153.ckpt\")\n",
    "model.load_datamodule(data_module)#adds datamodule to model\n",
    "\n",
    "\n",
    "#log every n steps could be important as it decides how often it should log to tensorboard\n",
    "# Also check val every n epochs, as validation checking takes some time\n",
    "logger = TensorBoardLogger(\"./\")\n",
    "trainer = pl.Trainer(gpus=1,logger=logger,   # auto_scale_batch_size=\"binsearch\",\n",
    "                      max_steps=50000, num_sanity_val_steps=0,\n",
    "                      check_val_every_n_epoch=100,track_grad_norm=2,\n",
    "                     fast_dev_run=False,max_epochs=-1)\n",
    "# This calls the fit function which trains the model\n",
    "trainer.fit(model, train_dataloaders=data_module )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "64528eb3-cd4c-42e3-98c7-aae890cd7f0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/cuda/memory.py:114\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae545510-e52b-4b06-be7f-8a22dd43a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,c) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "#         self.point_encoding = nn.Sequential(\n",
    "#             nn.Conv1d(3, 6, kernel_size=1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv1d(6, 8, kernel_size=1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv1d(8, 16, kernel_size=1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#         )  \n",
    "#         self.latent_encoding = nn.Sequential(\n",
    "#             nn.Linear(16, 16),\n",
    "#             # nn.BatchNorm1d(),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(16, 16),\n",
    "#             # nn.BatchNorm1d(),\n",
    "            \n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#         )\n",
    "        \n",
    "        self.point_encoding = nn.Sequential(\n",
    "            nn.Linear(90, 60),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(60, 40),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(40, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )  \n",
    "        # self.mu_mapping = nn.Linear(32, 32)\n",
    "        #self.logvar_mapping = nn.Linear(32, 32)\n",
    "        self.latent_rep = nn.Linear(16, c)\n",
    "\n",
    "    def forward(self, points: torch.Tensor) -> torch.Tensor: # Tuple[torch.Tensor, torch.Tensor]:\n",
    "        points = self.point_encoding(points.reshape(-1,90))\n",
    "        # use channel as latent space\n",
    "        # summing is perm invariant\n",
    "        #latent = points.sum(axis=2)\n",
    "        #latent = self.latent_encoding(latent)\n",
    "        #mu = self.mu_mapping(latent)\n",
    "        #logvar = self.logvar_mapping(latent)\n",
    "        #return mu, logvar\n",
    "\n",
    "        return self.latent_rep(points)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44384de-7a9a-4aae-a85e-0458e05f1b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1144098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ee860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetnet",
   "language": "python",
   "name": "jetnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
