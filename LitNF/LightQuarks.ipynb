{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98744a-7b3b-4106-bb46-8168aedc92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetnet_dataloader import JetNetDataloader,StandardScaler\n",
    "from helpers import mass\n",
    "from plotting import *\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759c9e3-083a-406c-a22d-759b9ecbe416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nflows as nf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from jetnet.evaluation import cov_mmd, fpnd, w1efp, w1m, w1p\n",
    "from nflows.flows import base\n",
    "from nflows.nn import nets\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.coupling import PiecewiseRationalQuadraticCouplingTransform\n",
    "from nflows.utils.torchutils import create_random_binary_mask\n",
    "from torch import nn\n",
    "from torch.nn import functional as FF\n",
    "from torch.nn.functional import leaky_relu, sigmoid\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from helpers import CosineWarmupScheduler, mass\n",
    "from plotting import *\n",
    "\n",
    "\n",
    "class Gen(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dim=3,\n",
    "        l_dim=10,\n",
    "        hidden=300,\n",
    "        num_layers=3,\n",
    "        num_heads=1,\n",
    "        n_part=5,\n",
    "        fc=False,\n",
    "        dropout=0.5,\n",
    "        no_hidden=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes = hidden\n",
    "        self.n_dim = n_dim\n",
    "        self.l_dim = l_dim\n",
    "        self.n_part = n_part\n",
    "        self.no_hidden = no_hidden\n",
    "        self.fc = fc\n",
    "        if fc:\n",
    "            self.l_dim *= n_part\n",
    "            self.embbed_flat = nn.Linear(n_dim * n_part, l_dim)\n",
    "            self.flat_hidden = nn.Linear(l_dim, hidden)\n",
    "            self.flat_hidden2 = nn.Linear(hidden, hidden)\n",
    "            self.flat_hidden3 = nn.Linear(hidden, hidden)\n",
    "            self.flat_out = nn.Linear(hidden, n_dim * n_part)\n",
    "        else:\n",
    "            self.embbed = nn.Linear(n_dim, l_dim)\n",
    "            self.encoder = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=l_dim,\n",
    "                    nhead=num_heads,\n",
    "                    batch_first=True,\n",
    "                    norm_first=False,\n",
    "                    dim_feedforward=hidden,\n",
    "                    dropout=dropout,\n",
    "                ),\n",
    "                num_layers=num_layers,\n",
    "            )\n",
    "            self.hidden = nn.Linear(l_dim, hidden)\n",
    "            self.hidden2 = nn.Linear(hidden, hidden)\n",
    "            self.hidden3 = nn.Linear(hidden, hidden)\n",
    "            self.dropout = nn.Dropout(dropout / 2)\n",
    "            self.out = nn.Linear(hidden, n_dim)\n",
    "            self.out2 = nn.Linear(l_dim, n_dim)\n",
    "\n",
    "            self.out_flat = nn.Linear(hidden, n_dim * n_part)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        if self.fc:\n",
    "            x = x.reshape(len(x), self.n_part * self.n_dim)\n",
    "            x = self.embbed_flat(x)\n",
    "            x = leaky_relu(self.flat_hidden(x))\n",
    "            #             x = self.dropout(x)\n",
    "            x = self.flat_out(x)\n",
    "            x = x.reshape(len(x), self.n_part, self.n_dim)\n",
    "        else:\n",
    "            x = self.embbed(x)\n",
    "            x = self.encoder(x, src_key_padding_mask=mask)\n",
    "            if not self.no_hidden == True:\n",
    "\n",
    "                x = leaky_relu(self.hidden(x))\n",
    "                x = self.dropout(x)\n",
    "                x = leaky_relu(self.hidden2(x))\n",
    "                x = self.dropout(x)\n",
    "                x = self.out(x)\n",
    "            elif self.no_hidden == \"more\":\n",
    "                x = leaky_relu(self.hidden(x))\n",
    "                x = self.dropout(x)\n",
    "                x = leaky_relu(self.hidden2(x))\n",
    "                x = self.dropout(x)\n",
    "                x = leaky_relu(self.hidden3(x))\n",
    "                x = self.dropout(x)\n",
    "\n",
    "            else:\n",
    "                x = leaky_relu(x)\n",
    "                x = self.out2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Disc(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dim=3,\n",
    "        l_dim=10,\n",
    "        hidden=300,\n",
    "        num_layers=3,\n",
    "        num_heads=1,\n",
    "        n_part=2,\n",
    "        fc=False,\n",
    "        dropout=0.5,\n",
    "        mass=False,\n",
    "        clf=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes = hidden\n",
    "        self.n_dim = n_dim\n",
    "        #         l_dim=n_dim\n",
    "        self.l_dim = l_dim\n",
    "        self.n_part = n_part\n",
    "        self.fc = fc\n",
    "        self.clf = clf\n",
    "\n",
    "        if fc:\n",
    "            self.l_dim *= n_part\n",
    "            self.embbed_flat = nn.Linear(n_dim * n_part, l_dim)\n",
    "            self.flat_hidden = nn.Linear(l_dim, hidden)\n",
    "            self.flat_hidden2 = nn.Linear(hidden, hidden)\n",
    "            self.flat_hidden3 = nn.Linear(hidden, hidden)\n",
    "            self.flat_out = nn.Linear(hidden, 1)\n",
    "        else:\n",
    "            self.embbed = nn.Linear(n_dim, l_dim)\n",
    "            self.encoder = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=self.l_dim,\n",
    "                    nhead=num_heads,\n",
    "                    dim_feedforward=hidden,\n",
    "                    dropout=dropout,\n",
    "                    norm_first=False,\n",
    "                    activation=lambda x: leaky_relu(x, 0.2),\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "                num_layers=num_layers,\n",
    "            )\n",
    "            self.hidden = nn.Linear(l_dim + int(mass), 2 * hidden)\n",
    "            self.hidden2 = nn.Linear(2 * hidden, hidden)\n",
    "            self.out = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x, m=None, mask=None):\n",
    "\n",
    "        if self.fc == True:\n",
    "            x = x.reshape(len(x), self.n_dim * self.n_part)\n",
    "            x = self.embbed_flat(x)\n",
    "            x = leaky_relu(self.flat_hidden(x), 0.2)\n",
    "            x = leaky_relu(self.flat_hidden2(x), 0.2)\n",
    "            x = self.flat_out(x)\n",
    "        else:\n",
    "            x = self.embbed(x)\n",
    "            if self.clf:\n",
    "                x = torch.concat((torch.ones_like(x[:, 0, :]).reshape(len(x), 1, -1), x), axis=1)\n",
    "                mask = torch.concat((torch.ones_like((mask[:, 0]).reshape(len(x), 1)), mask), dim=1).to(x.device)\n",
    "\n",
    "                x = self.encoder(x, src_key_padding_mask=mask)\n",
    "                x = x[:, 0, :]\n",
    "            else:\n",
    "                x = self.encoder(x, src_key_padding_mask=mask)\n",
    "                x = torch.sum(x, axis=1)\n",
    "            if m is not None:\n",
    "                x = torch.concat((m.reshape(len(x), 1), x), axis=1)\n",
    "            x = leaky_relu(self.hidden(x), 0.2)\n",
    "            x = leaky_relu(self.hidden2(x), 0.2)\n",
    "            x = self.out(x)\n",
    "            x = x\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransGan(pl.LightningModule):\n",
    "    def create_resnet(self, in_features, out_features):\n",
    "        \"\"\"This is the network that outputs the parameters of the invertible transformation\n",
    "        The only arguments can be the in dimension and the out dimenson, the structure\n",
    "        of the network is defined over the config which is a class attribute\n",
    "        Context Features: Amount of features used to condition the flow - in our case\n",
    "        this is usually the mass\n",
    "        num_blocks: How many Resnet blocks should be used, one res net block is are 1 input+ 2 layers\n",
    "        and an additive skip connection from the first to the third\"\"\"\n",
    "        c = self.config[\"context_features\"]\n",
    "        return nets.ResidualNet(\n",
    "            in_features,\n",
    "            out_features,\n",
    "            hidden_features=self.config[\"network_nodes_nf\"],\n",
    "            context_features=c,\n",
    "            num_blocks=self.config[\"network_layers_nf\"],\n",
    "            activation=self.config[\"activation\"] if \"activation\" in self.config.keys() else FF.relu,\n",
    "            # dropout_probability=self.config[\"dropout\"] if \"dropout\" in self.config.keys() else 0,\n",
    "            use_batch_norm=self.config[\"batchnorm\"] if \"batchnorm\" in self.config.keys() else 0,\n",
    "        )\n",
    "\n",
    "    def __init__(self, config, hyperopt, num_batches):\n",
    "        \"\"\"This initializes the model and its hyperparameters\"\"\"\n",
    "        super().__init__()\n",
    "        self.hyperopt = True\n",
    "\n",
    "        self.start = time.time()\n",
    "        # self.batch_size=batch_size\n",
    "        # print(batch_size)\n",
    "        self.config = config\n",
    "        self.automatic_optimization = False\n",
    "        self.freq_d = config[\"freq\"]\n",
    "\n",
    "        self.wgan = config[\"wgan\"]\n",
    "        # Metrics to track during the training\n",
    "        self.metrics = {\n",
    "            \"val_w1p\": [],\n",
    "            \"val_w1m\": [],\n",
    "            \"val_w1efp\": [],\n",
    "            \"val_cov\": [],\n",
    "            \"val_mmd\": [],\n",
    "            \"val_fpnd\": [],\n",
    "            \"val_logprob\": [],\n",
    "            \"step\": [],\n",
    "        }\n",
    "        # Loss function of the Normalizing flows\n",
    "        self.logprobs = []\n",
    "        self.n_part = config[\"n_part\"]\n",
    "        # self.hparams.update(config)\n",
    "        self.save_hyperparameters()\n",
    "        self.flows = []\n",
    "        self.n_dim = self.config[\"n_dim\"]\n",
    "        self.n_part = config[\"n_part\"]\n",
    "        self.add_corr = config[\"corr\"]\n",
    "        self.alpha = 1\n",
    "        self.num_batches = int(num_batches)\n",
    "        self.build_flow()\n",
    "        self.gen_net = Gen(\n",
    "            n_dim=self.n_dim,\n",
    "            hidden=config[\"hidden\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            no_hidden=config[\"no_hidden\"],\n",
    "            fc=config[\"fc\"],\n",
    "            n_part=config[\"n_part\"],\n",
    "            l_dim=config[\"l_dim\"],\n",
    "            num_heads=config[\"heads\"],\n",
    "        ).cuda()\n",
    "        self.dis_net = Disc(\n",
    "            n_dim=self.n_dim,\n",
    "            hidden=config[\"hidden\"],\n",
    "            l_dim=config[\"l_dim\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            mass=self.config[\"mass\"],\n",
    "            num_heads=config[\"heads\"],\n",
    "            fc=config[\"fc\"],\n",
    "            n_part=config[\"n_part\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            clf=config[\"clf\"],\n",
    "        ).cuda()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fpnds = []\n",
    "        self.df = pd.DataFrame()\n",
    "        for p in self.dis_net.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal(p)\n",
    "        self.nf_train = True\n",
    "        self.train_nf = config[\"max_epochs\"] // config[\"frac_pretrain\"]\n",
    "\n",
    "    def load_datamodule(self, data_module):\n",
    "        \"\"\"needed for lightning training to work, it just sets the dataloader for training and validation\"\"\"\n",
    "        self.data_module = data_module\n",
    "\n",
    "    def on_after_backward(self) -> None:\n",
    "        \"\"\"This is a genious little hook, sometimes my model dies, i have no clue why. This saves the training from crashing and continues\"\"\"\n",
    "        valid_gradients = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                valid_gradients = not (torch.isnan(param.grad).any() or torch.isinf(param.grad).any())\n",
    "                if not valid_gradients:\n",
    "                    break\n",
    "        if not valid_gradients:\n",
    "            print(\"not valid grads\", self.counter)\n",
    "            self.zero_grad()\n",
    "            self.counter += 1\n",
    "            if self.counter > 5:\n",
    "                raise ValueError(\"5 nangrads in a row\")\n",
    "        else:\n",
    "            self.counter = 0\n",
    "\n",
    "    def build_flow(self):\n",
    "        K = self.config[\"coupling_layers\"]\n",
    "        for i in range(K):\n",
    "            \"\"\"This creates the masks for the coupling layers, particle masks are masks\n",
    "            created such that each feature particle (eta,phi,pt) is masked together or not\"\"\"\n",
    "            mask = create_random_binary_mask(self.n_dim * self.n_part)\n",
    "            self.flows += [\n",
    "                PiecewiseRationalQuadraticCouplingTransform(\n",
    "                    mask=mask,\n",
    "                    transform_net_create_fn=self.create_resnet,\n",
    "                    tails=\"linear\",\n",
    "                    tail_bound=self.config[\"tail_bound\"],\n",
    "                    num_bins=self.config[\"bins\"],\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.q0 = nf.distributions.normal.StandardNormal([self.n_dim * self.n_part])\n",
    "        # Creates working flow model from the list of layer modules\n",
    "        self.flows = CompositeTransform(self.flows)\n",
    "        # Construct flow model\n",
    "        self.flow = base.Flow(distribution=self.q0, transform=self.flows)\n",
    "\n",
    "    def scale(self, x):\n",
    "        x = x.reshape(len(x), self.n_part, self.n_dim)\n",
    "        self.data_module.scaler = self.data_module.scaler.to(x.device)\n",
    "        if self.config[\"quantile\"]:\n",
    "            x[:, :, 2] = torch.tensor(self.data_module.ptscaler.inverse_transform(x[:, :, 2].cpu().numpy())).to(x.device)\n",
    "            x[:, :, :2] = self.data_module.scaler.inverse_transform(x[:, :, :2])\n",
    "        else:\n",
    "            x = self.data_module.scaler.inverse_transform(x)\n",
    "        return x\n",
    "\n",
    "    def sampleandscale(self, batch, mask=None, mask_test=None, scale=False):\n",
    "        \"\"\"This is a helper function that samples from the flow (i.e. generates a new sample)\n",
    "        and reverses the standard scaling that is done in the preprocessing. This allows to calculate the mass\n",
    "        on the generative sample and to compare to the simulated one, we need to inverse the scaling before calculating the mass\n",
    "        because calculating the mass is a non linear transformation and does not commute with the mass calculation\"\"\"\n",
    "        # with torch.no_grad():\n",
    "        z = self.flow.sample(len(batch)).reshape(len(batch), self.n_part, self.n_dim).detach().requires_grad_(True)\n",
    "        fake = z + self.gen_net(z, mask=mask)  # (1-self.alpha)*\n",
    "        fake = fake.reshape(len(batch), self.n_part, self.n_dim)\n",
    "        if scale:\n",
    "            fake_scaled, z_scaled, true = (self.scale(fake).to(batch.device), self.scale(z).to(batch.device), self.scale(batch).to(batch.device))\n",
    "            true = true * mask.reshape(len(batch), self.n_part, 1)\n",
    "            z_scaled = z_scaled * mask.reshape(len(batch), self.n_part, 1)\n",
    "            fake_scaled = fake_scaled * mask.reshape(len(batch), self.n_part, 1)\n",
    "            return fake.to(batch.device), batch, z.to(batch.device), fake_scaled, true, z_scaled\n",
    "        else:\n",
    "            return fake\n",
    "\n",
    "    def gan_losses(self, fake, batch):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.losses = []\n",
    "        # mlosses are initialized with None during the time it is not turned on, makes it easier to plot\n",
    "        opt_nf = torch.optim.AdamW(self.flow.parameters(), lr=self.config[\"lr_nf\"])\n",
    "        if self.config[\"opt\"] == \"Adam\":\n",
    "            opt_g = torch.optim.Adam(self.gen_net.parameters(), lr=self.config[\"lr_g\"], betas=(0, 0.9))\n",
    "            opt_d = torch.optim.Adam(self.dis_net.parameters(), lr=self.config[\"lr_d\"], betas=(0, 0.9))\n",
    "        elif self.config[\"opt\"] == \"AdamW\":\n",
    "            opt_g = torch.optim.AdamW(self.gen_net.parameters(), lr=self.config[\"lr_g\"], betas=(0, 0.9))\n",
    "            opt_d = torch.optim.AdamW(self.dis_net.parameters(), lr=self.config[\"lr_d\"], betas=(0, 0.9))\n",
    "        elif self.config[\"opt\"] == \"SGD\":\n",
    "            opt_g = torch.optim.SGD(self.gen_net.parameters(), lr=self.config[\"lr_g\"])\n",
    "            opt_d = torch.optim.SGD(self.dis_net.parameters(), lr=self.config[\"lr_d\"])\n",
    "        else:\n",
    "            opt_g = torch.optim.RMSprop(self.gen_net.parameters(), lr=self.config[\"lr_g\"])\n",
    "            opt_d = torch.optim.RMSprop(self.dis_net.parameters(), lr=self.config[\"lr_d\"])\n",
    "        if self.config[\"sched\"] == \"cosine\":\n",
    "            lr_scheduler_nf = CosineWarmupScheduler(opt_nf, warmup=1, max_iters=10000000 * self.config[\"freq\"])\n",
    "            max_iter_d = (self.config[\"max_epochs\"] - self.train_nf // 2) * self.num_batches\n",
    "            max_iter_g = (self.config[\"max_epochs\"] - self.train_nf) * self.num_batches // self.freq_d\n",
    "            lr_scheduler_d = CosineWarmupScheduler(opt_d, warmup=15 * self.num_batches, max_iters=max_iter_d)\n",
    "            lr_scheduler_g = CosineWarmupScheduler(opt_g, warmup=15 * self.num_batches // self.freq_d, max_iters=max_iter_g)\n",
    "        elif self.config[\"sched\"] == \"cosine2\":\n",
    "            lr_scheduler_nf = CosineWarmupScheduler(opt_nf, warmup=1, max_iters=10000000 * self.config[\"freq\"])\n",
    "            max_iter_d = (self.config[\"max_epochs\"] - self.train_nf // 2) * self.num_batches \n",
    "            max_iter_g = (self.config[\"max_epochs\"] - self.train_nf) * self.num_batches  // self.freq_d\n",
    "            lr_scheduler_d = CosineWarmupScheduler(opt_d, warmup=15 * self.num_batches, max_iters=max_iter_d // 3)\n",
    "            lr_scheduler_g = CosineWarmupScheduler(opt_g, warmup=15 * self.num_batches // self.freq_d, max_iters=max_iter_g // 3)\n",
    "        else:\n",
    "            lr_scheduler_nf = None\n",
    "            lr_scheduler_d = None\n",
    "            lr_scheduler_g = None\n",
    "        if self.config[\"sched\"] != None:\n",
    "            return [opt_nf, opt_d, opt_g], [lr_scheduler_nf, lr_scheduler_d, lr_scheduler_g]\n",
    "        else:\n",
    "            return [opt_nf, opt_d, opt_g]\n",
    "\n",
    "    def sample_n(self, mask):\n",
    "\n",
    "        mask_test = torch.zeros_like(mask)\n",
    "        k = 0\n",
    "        x, y = np.unique(self.data_module.n, return_counts=True)\n",
    "        y = (y / len(self.data_module.n) * len(mask)).astype(int)\n",
    "        for i, j in zip(x, y):\n",
    "            mask_test[int(k) : int(k + j), : int(i)] = 1\n",
    "            k += j\n",
    "        return mask_test\n",
    "\n",
    "    def compute_gradient_penalty(self, D, real_samples, fake_samples, mask, phi):\n",
    "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "\n",
    "        alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1))).to(real_samples.device)\n",
    "        interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n",
    "        if self.config[\"mass\"]:\n",
    "            m = mass(interpolates, mask=mask, canonical=self.config[\"canonical\"].detach())\n",
    "            d_interpolates = D.train()(interpolates.requires_grad_(True), m.requires_grad_(True), mask=mask)\n",
    "        else:\n",
    "            d_interpolates = D.train()(interpolates.requires_grad_(True), mask=mask)\n",
    "        fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(real_samples.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    def compute_gradient_penalty2(self, real_data, fake_data, pred_real, pred_fake, k=2, p=6, device=torch.device(\"cuda\")):\n",
    "        real_grad_outputs = torch.full((pred_real.size(0),), 1, dtype=torch.float32, requires_grad=False, device=device)\n",
    "        fake_grad_outputs = torch.full((pred_real.size(0),), 1, dtype=torch.float32, requires_grad=False, device=device)\n",
    "        # print(real_outputs.grad_fn())\n",
    "        # real_data=real_data.requires_grad_(True)\n",
    "        # fake_data=self.gen_net(self.flow.sample(len(real_data)).reshape(-1,30,3))\n",
    "        # m_t = mass(real_data.reshape(len(real_data), self.n_part * self.n_dim),self.config[\"canonical\"])\n",
    "        # m_f = mass(fake_data.reshape(len(real_data), self.n_part * self.n_dim),self.config[\"canonical\"])\n",
    "        # real_outputs = self.dis_net(real_data, None if not self.config[\"mass\"] else m_t, mask=mask)\n",
    "        # fake_outputs = self.dis_net(fake_data, None if not self.config[\"mass\"] else m_f, mask=mask)\n",
    "\n",
    "        real_gradient = torch.autograd.grad(\n",
    "            outputs=pred_real.reshape(-1),\n",
    "            inputs=real_data,\n",
    "            grad_outputs=real_grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        fake_gradient = torch.autograd.grad(\n",
    "            outputs=pred_fake.reshape(-1),\n",
    "            inputs=fake_data,\n",
    "            grad_outputs=fake_grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        real_gradient_norm = real_gradient.view(real_gradient.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "        fake_gradient_norm = fake_gradient.view(fake_gradient.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "        gradient_penalty = torch.mean(real_gradient_norm + fake_gradient_norm) * k / 2\n",
    "        return gradient_penalty\n",
    "\n",
    "    def _summary(self, temp):\n",
    "        self.summary_path = \"/beegfs/desy/user/{}/{}/summary.csv\".format(os.environ[\"USER\"], self.config[\"name\"])\n",
    "        if os.path.isfile(self.summary_path):\n",
    "\n",
    "            summary = pd.read_csv(self.summary_path).set_index([\"path_index\"])\n",
    "        else:\n",
    "            print(\"summary not found\")\n",
    "            summary = pd.DataFrame()\n",
    "\n",
    "        summary.loc[self.logger.log_dir, self.config.keys()] = self.config.values()\n",
    "        summary.loc[self.logger.log_dir, temp.keys()] = temp.values()\n",
    "        summary.loc[self.logger.log_dir, \"time\"] = time.time() - self.start\n",
    "        summary.to_csv(self.summary_path, index_label=[\"path_index\"])\n",
    "        return summary\n",
    "\n",
    "    def _results(self, temp):\n",
    "        self.metrics[\"step\"].append(self.current_epoch)\n",
    "        self.df = self.df.append(pd.DataFrame([temp], index=[self.current_epoch]))\n",
    "        self.df.to_csv(self.logger.log_dir + \"result.csv\", index_label=[\"index\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"training loop of the model, here all the data is passed forward to a gaussian\n",
    "        This is the important part what is happening here. This is all the training we do\"\"\"\n",
    "        mask = batch[:, 90:]\n",
    "        gradient_penalty = 0\n",
    "        batch = batch[:, :90]\n",
    "        opt_nf, opt_d, opt_g = self.optimizers()\n",
    "        if self.config[\"sched\"]:\n",
    "            sched_nf, sched_d, sched_g = self.lr_schedulers()\n",
    "\n",
    "        # ### NF PART\n",
    "        if self.config[\"sched\"] != None:\n",
    "            self.log(\"lr_g\", sched_g.get_last_lr()[-1], logger=True, on_epoch=True)\n",
    "            self.log(\"lr_nf\", sched_nf.get_last_lr()[-1], logger=True, on_epoch=True)\n",
    "            self.log(\"lr_d\", sched_d.get_last_lr()[-1], logger=True, on_epoch=True)\n",
    "\n",
    "        # if (self.current_epoch > self.train_nf and self.global_step % self.freq_d < 2) or self.global_step == 2:\n",
    "        #\n",
    "        # Only train nf for first few epochs\n",
    "        if self.current_epoch < self.train_nf:\n",
    "            if self.config[\"sched\"] != None:\n",
    "                sched_nf.step()\n",
    "            nf_loss = -self.flow.to(self.device).log_prob(batch).mean()\n",
    "            nf_loss /= self.n_dim * self.n_part\n",
    "            self.flow.zero_grad()\n",
    "            self.manual_backward(nf_loss)\n",
    "            opt_nf.step()\n",
    "            self.log(\"logprob\", nf_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
    "\n",
    "        # GAN PART\n",
    "        if self.current_epoch >= self.train_nf / 2 or self.global_step == 1:\n",
    "            if self.config[\"sched\"] != None:\n",
    "                sched_d.step()\n",
    "            if self.wgan:  # this is for grad div\n",
    "                batch.requires_grad = True\n",
    "            batch = batch.reshape(len(batch), self.n_part, self.n_dim)\n",
    "            fake = self.sampleandscale(batch, mask, scale=False)\n",
    "            fake = fake.detach()\n",
    "            if self.wgan:  # this allows calculation of gradients with respect to inputs\n",
    "                fake = fake.requires_grad_(True)\n",
    "            if self.config[\"mass\"]:\n",
    "                m_t = mass(batch, mask=mask, canonical=self.config[\"canonical\"])\n",
    "                m_f = mass(fake, mask=mask, canonical=self.config[\"canonical\"])\n",
    "            pred_real = self.dis_net(batch, None if not self.config[\"mass\"] else m_t, mask=mask)\n",
    "            pred_fake = self.dis_net(fake, None if not self.config[\"mass\"] else m_f, mask=mask)\n",
    "            if self.wgan:\n",
    "                # gradient_penalty = self.compute_gradient_penalty(self.dis_net, batch, fake.detach(), mask, 1)\n",
    "                gradient_penalty = self.compute_gradient_penalty2(batch, fake, pred_real, pred_fake)\n",
    "                self.log(\"gradient penalty\", gradient_penalty, logger=True)\n",
    "                d_loss = -torch.mean(pred_real.view(-1)) + torch.mean(pred_fake.view(-1))\n",
    "                self.log(\"d_loss\", d_loss, logger=True, prog_bar=True)\n",
    "                d_loss += gradient_penalty\n",
    "            else:\n",
    "                target_real = torch.ones_like(pred_real)\n",
    "                target_fake = torch.zeros_like(pred_fake)\n",
    "                pred = torch.vstack((pred_real, pred_fake))\n",
    "                target = torch.vstack((target_real, target_fake))\n",
    "                d_loss = nn.MSELoss()(pred, target).mean()\n",
    "                self.log(\"d_loss\", d_loss, logger=True, prog_bar=True)\n",
    "            self.dis_net.zero_grad()\n",
    "            self.manual_backward(d_loss)\n",
    "            if self.global_step > 10:\n",
    "                opt_d.step()\n",
    "            else:\n",
    "                opt_d.zero_grad()\n",
    "            if self.global_step == 2:\n",
    "                print(\"passed test disc\")\n",
    "            if (self.current_epoch > self.train_nf and self.global_step % self.freq_d < 2) or self.global_step <= 3:\n",
    "                self.gen_net.zero_grad()\n",
    "                fake = self.sampleandscale(batch, mask, scale=False)\n",
    "                m_f = mass(fake, mask=mask, canonical=self.config[\"canonical\"])\n",
    "                pred_fake = self.dis_net(fake, None if not self.config[\"mass\"] else m_f, mask=mask)\n",
    "                if self.wgan:\n",
    "                    g_loss = -torch.mean(pred_fake.view(-1))\n",
    "                else:\n",
    "                    target_real = torch.ones_like(pred_fake)\n",
    "                    g_loss = nn.MSELoss()((pred_fake.view(-1)), target_real.view(-1))\n",
    "                self.manual_backward(g_loss)\n",
    "                if self.global_step > 10:\n",
    "                    opt_g.step()\n",
    "                else:\n",
    "                    opt_g.zero_grad()\n",
    "                self.log(\"g_loss\", g_loss, logger=True, prog_bar=True)\n",
    "                if self.config[\"sched\"] != None:\n",
    "                    sched_g.step()\n",
    "                if self.global_step == 3:\n",
    "                    print(\"passed test gen\")\n",
    "\n",
    "            # Control plot train\n",
    "            if self.current_epoch % 5 == 0 and self.current_epoch > self.train_nf / 2:\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.hist(pred_fake.detach().cpu().numpy(), label=\"fake\", bins=np.linspace(0, 1, 30) if not self.wgan else 30, histtype=\"step\")\n",
    "                ax.hist(pred_real.detach().cpu().numpy(), label=\"real\", bins=np.linspace(0, 1, 30) if not self.wgan else 30, histtype=\"step\")\n",
    "                ax.legend()\n",
    "                plt.ylabel(\"Counts\")\n",
    "                plt.xlabel(\"Critic Score\")\n",
    "                self.logger.experiment.add_figure(\"class_train\", fig, global_step=self.current_epoch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"This calculates some important metrics on the hold out set (checking for overtraining)\"\"\"\n",
    "\n",
    "        mask = batch[:, 90:].cpu()\n",
    "        mask_test = self.sample_n(mask)\n",
    "        batch = batch[:, :90].cpu()\n",
    "        self.dis_net.train()\n",
    "        self.gen_net.train()\n",
    "        self.data_module.scaler.to(\"cpu\")\n",
    "        batch = batch.to(\"cpu\")\n",
    "        self.flow = self.flow.to(\"cpu\")\n",
    "        self.dis_net = self.dis_net.cpu()\n",
    "        self.gen_net = self.gen_net.cpu()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen, true, z, fake_scaled, true_scaled, z_scaled = self.sampleandscale(batch, mask_test, scale=True)\n",
    "            if self.config[\"mass\"]:\n",
    "                m_t = mass(batch, mask=mask, canonical=self.config[\"canonical\"])\n",
    "                m_f = mass(gen, mask=mask_test, canonical=self.config[\"canonical\"])\n",
    "        pred_real = self.dis_net(batch.reshape(len(batch), self.n_part, self.n_dim), None if not self.config[\"mass\"] else m_t, mask=mask)\n",
    "        pred_fake = self.dis_net(gen, None if not self.config[\"mass\"] else m_f, mask=mask_test)\n",
    "        if self.wgan:\n",
    "            d_loss = -torch.mean(pred_real.view(-1)) + torch.mean(pred_fake.view(-1))\n",
    "            g_loss = -torch.mean(pred_fake.view(-1))\n",
    "        else:\n",
    "            target_real = torch.ones_like(pred_real)\n",
    "            target_fake = torch.zeros_like(pred_fake)\n",
    "            pred = torch.vstack((pred_real, pred_fake))\n",
    "            target = torch.vstack((target_real, target_fake))\n",
    "            d_loss = nn.MSELoss()(pred, target).mean()\n",
    "            g_loss = nn.MSELoss()((pred_fake.view(-1)), target_real.view(-1))\n",
    "\n",
    "        true_scaled, fake_scaled, z_scaled = (true_scaled.reshape(-1, 90), fake_scaled.reshape(-1, 90), z_scaled.reshape(-1, 90))\n",
    "        # Reverse Standard Scaling (this has nothing to do with flows, it is a standard preprocessing step)\n",
    "        m_t = mass(true_scaled, canonical=self.config[\"canonical\"], mask=mask).cpu()\n",
    "        m_gen = mass(z_scaled, mask=mask_test, canonical=self.config[\"canonical\"]).cpu()\n",
    "        m_c = mass(fake_scaled, mask=mask_test, canonical=self.config[\"canonical\"]).cpu()\n",
    "        for i in range(30):\n",
    "            i = 2 + 3 * i\n",
    "            # gen[gen[:,i]<0,i]=0\n",
    "            fake_scaled[fake_scaled[:, i] < 0, i] = 0\n",
    "            true_scaled[true_scaled[:, i] < 0, i] = 0\n",
    "        # metrics\n",
    "\n",
    "        cov, mmd = cov_mmd(fake_scaled.reshape(-1, self.n_part, self.n_dim), true_scaled.reshape(-1, self.n_part, self.n_dim), use_tqdm=False)\n",
    "        try:\n",
    "            fpndv = fpnd(fake_scaled.reshape(-1, self.n_part, self.n_dim).numpy(), use_tqdm=False, jet_type=self.config[\"parton\"])\n",
    "        except:\n",
    "            fpndv = 1000\n",
    "        self.fpnds.append(fpndv)\n",
    "        if (np.array(self.fpnds)[-10:] > 30).all() and self.current_epoch > 200:\n",
    "            print(\"fpnd to high, stop training\")\n",
    "            raise\n",
    "        w1m_ = w1m(fake_scaled.reshape(len(batch), self.n_part, self.n_dim), true_scaled.reshape(len(batch), self.n_part, self.n_dim))[0]\n",
    "        w1p_ = w1p(fake_scaled.reshape(len(batch), self.n_part, self.n_dim), true_scaled.reshape(len(batch), self.n_part, self.n_dim))[0]\n",
    "        w1efp_ = w1efp(fake_scaled.reshape(len(batch), self.n_part, self.n_dim), true_scaled.reshape(len(batch), self.n_part, self.n_dim))[0]\n",
    "\n",
    "        temp = {\n",
    "            \"val_fpnd\": fpndv,\n",
    "            \"val_mmd\": mmd,\n",
    "            \"val_cov\": cov,\n",
    "            \"val_w1m\": w1m_,\n",
    "            \"val_w1efp\": w1efp_,\n",
    "            \"val_w1p\": w1p_,\n",
    "            \"step\": self.global_step,\n",
    "            \"g_loss\": float(g_loss.numpy()),\n",
    "            \"d_loss\": float(d_loss.numpy()),\n",
    "        }\n",
    "        print(\"epoch {}: \".format(self.current_epoch), temp)\n",
    "        if self.hyperopt and self.global_step > 3:\n",
    "\n",
    "            if self.current_epoch < self.train_nf:\n",
    "                with torch.no_grad():\n",
    "                    logprob = -self.flow.log_prob(batch).mean() / 90\n",
    "                self.log(\"val_logprob\", logprob, prog_bar=True, logger=True)\n",
    "                temp[\"val_logprob\"] = float(logprob.numpy())\n",
    "            try:\n",
    "                self._results(temp)\n",
    "            except:\n",
    "                print(\"error in results\")\n",
    "            self._summary(temp)\n",
    "\n",
    "        self.log(\"hp_metric\", w1m_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1m\", w1m_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1p\", w1p_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_w1efp\", w1efp_, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_cov\", cov, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_fpnd\", fpndv, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mmd\", mmd, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.plot = plotting(\n",
    "            model=self, gen=z_scaled, gen_corr=fake_scaled, true=true_scaled, config=self.config, step=self.global_step, logger=self.logger.experiment\n",
    "        )\n",
    "        try:\n",
    "            self.plot.plot_mass(m=m_gen.cpu().numpy(), m_t=m_t.cpu().numpy(), m_c=m_c.cpu().numpy(), save=True, bins=50, quantile=True, plot_vline=False)\n",
    "            self.plot.plot_class(pred_fake=pred_fake, pred_real=pred_real, bins=50, step=self.current_epoch)\n",
    "            # self.plot.plot_2d(save=True)\n",
    "        #             self.plot.var_part(true=true[:,:self.n_dim],gen=gen_corr[:,:self.n_dim],true_n=n_true,gen_n=n_gen_corr,m_true=m_t,m_gen=m_test ,save=True)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "        self.flow = self.flow.to(\"cuda\")\n",
    "        self.gen_net = self.gen_net.to(\"cuda\")\n",
    "        self.dis_net = self.dis_net.to(\"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetnet",
   "language": "python",
   "name": "jetnet"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
