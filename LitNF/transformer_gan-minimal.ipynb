{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cddfa09-f01f-47b9-9bbb-072a4ff90294",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thx max\n",
      "good boy\n"
     ]
    }
   ],
   "source": [
    "print(\"thx max\")\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import hist\n",
    "import mplhep as hep\n",
    "import torch\n",
    "import numpy as np\n",
    "import hist\n",
    "from hist import Hist\n",
    "import traceback\n",
    "from helpers import mass\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from helpers import *\n",
    "import os\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import time\n",
    "from torch import nn\n",
    "gen_step = 0\n",
    "\n",
    "# train mode\n",
    "import nflows as nf\n",
    "from nflows.utils.torchutils import create_random_binary_mask\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.coupling import *\n",
    "from nflows.nn import nets\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.flows import base\n",
    "from nflows.transforms.coupling import *\n",
    "from nflows.transforms.autoregressive import *\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "# from plotting import plotting\n",
    "from torch.nn import functional as FF\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import OneCycleLR,ReduceLROnPlateau,ExponentialLR\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as FF\n",
    "import numpy as np\n",
    "from jetnet.evaluation import w1p, w1efp, w1m, cov_mmd,fpnd\n",
    "import mplhep as hep\n",
    "import hist\n",
    "from hist import Hist\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from collections import OrderedDict\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "print(\"good boy\")\n",
    "# from torch.nn import MultiheadAttention,TransformerEncoder,TransformerEncoderLayer\n",
    "def mass(data,canonical=False):\n",
    "    if canonical:\n",
    "        n_dim=data.shape[1]\n",
    "        p=data.reshape(-1,n_dim//3,3)\n",
    "        px=p[:,:,0]\n",
    "        py=p[:,:,1]\n",
    "        pz=p[:,:,2]\n",
    "        \n",
    "       \n",
    "    else:\n",
    "        n_dim=data.shape[1]\n",
    "        p=data.reshape(-1,n_dim//3,3)\n",
    "        px=torch.cos(p[:,:,1])*p[:,:,2]\n",
    "        py=torch.sin(p[:,:,1])*p[:,:,2]\n",
    "        pz=torch.sinh(p[:,:,0])*p[:,:,2]\n",
    "    px=torch.clamp(px,min=-100,max=100)\n",
    "    py=torch.clamp(py,min=-100,max=100)\n",
    "    pz=torch.clamp(pz,min=-100,max=100)\n",
    "    E=torch.sqrt(px**2+py**2+pz**2)\n",
    "    E=E.sum(axis=1)**2\n",
    "    p=px.sum(axis=1)**2+py.sum(axis=1)**2+pz.sum(axis=1)**2\n",
    "    m2=E-p\n",
    "    # if m2.isnan().any():\n",
    "    #     print(\"px:{} py:{} pz:{} \".format(px.abs().max(),py.abs().max(),pz.abs().max()))\n",
    "    # assert m2.isnan().sum()==0  \n",
    "    return torch.sqrt(torch.max(m2,torch.zeros(len(E)).to(E.device)))\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "class plotting():\n",
    "    '''This is a class that takes care of  plotting steps in the script,\n",
    "        It is initialized with the following arguments:\n",
    "        true=the simulated data, note that it needs to be scaled\n",
    "        gen= Generated data , needs to be scaled\n",
    "        step=The current step of the training, this is need for tensorboard\n",
    "        model=the model that is trained, a bit of an overkill as it is only used to access the losses\n",
    "        config=the config used for training\n",
    "        logger=The logger used for tensorboard logging'''\n",
    "    def __init__(self,true,gen,gen_corr,config,step,model=None,logger=None,):\n",
    "        self.config=model.config\n",
    "        self.n_dim=self.config[\"n_dim\"]\n",
    "        self.gen=gen.numpy()\n",
    "        self.gen_corr=gen_corr.numpy()\n",
    "        self.test_set=true.numpy()\n",
    "        self.step=step\n",
    "        self.model=model\n",
    "        if logger is not None:\n",
    "            self.summary=logger\n",
    "   \n",
    "\n",
    "    def plot_2d(self,save=False):\n",
    "        #This creates a 2D histogram of the inclusive distribution for all 3 feature combinations\n",
    "        #Inclusive means that is the distribution of pt of all particles per jet and sample\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        data=self.test_set[:,:self.n_dim].reshape(-1,3)\n",
    "        gen=self.gen[:,:self.n_dim].reshape(-1,3)\n",
    "        labels=[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}$\",r\"$p_T^{rel}$\"]\n",
    "        names=[\"eta\",\"p3hi\",\"pt\"]\n",
    "        for index in [[0,1],[0,2],[1,2]]:\n",
    "\n",
    "            fig,ax=plt.subplots(ncols=2,figsize=(16, 8))\n",
    "            _,x,y,_=ax[0].hist2d(data[:,index[0]],data[:,index[1]],bins=30)\n",
    "            \n",
    "            if index[1]==2:\n",
    "                \n",
    "                y = np.logspace(np.log(y[0]),np.log(y[-1]),len(y))\n",
    "                ax[0].hist2d(data[:,index[0]],gen[:,index[1]],bins=[x,y])\n",
    "            ax[1].hist2d(gen[:,index[0]],gen[:,index[1]],bins=[x,y])\n",
    "            plt.tight_layout(pad=2)\n",
    "            ax[0].set_xlabel( labels[index[0]])\n",
    "            ax[0].set_ylabel( labels[index[1]])\n",
    "            \n",
    "            ax[0].set_title(\"Data\")\n",
    "            ax[1].set_xlabel( labels[index[0]])\n",
    "            ax[1].set_ylabel( labels[index[1]])\n",
    "            \n",
    "            ax[1].set_title(\"Gen\")\n",
    "           \n",
    "            if save:\n",
    "                self.summary.add_figure(\"2d{}-{}\".format(names[index[0]],names[index[1]]),fig,global_step=self.step)\n",
    "                \n",
    "                # self.summary.close()\n",
    "            else:\n",
    "                plt.show()\n",
    " \n",
    "        \n",
    "    def plot_mass(self,m,m_t,m_c,save=False,quantile=False,bins=15,plot_vline=True):\n",
    "        #This creates a histogram of the inclusive distributions and calculates the mass of each jet\n",
    "        #and creates a histogram of that\n",
    "        #if save, the histograms are logged to tensorboard otherwise they are shown\n",
    "        #if quantile, this also creates a histogram of a subsample of the generated data, \n",
    "        # where the mass used to condition the flow is in the first 10% percentile of the simulated mass dist\n",
    "        i=0\n",
    "\n",
    "\n",
    "        for v,name in zip([\"eta\",\"phi\",\"pt\",\"m\"],[r\"$\\eta^{rel}$\",r\"$\\phi^{rel}$\",r\"$p_T^{rel}$\",r\"$m_T^{rel}$\"]):\n",
    "            \n",
    "            if v!=\"m\":\n",
    "                a=np.quantile(self.test_set[:,i],0.001)\n",
    "                b=np.quantile(self.test_set[:,i],0.999)\n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h3=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                \n",
    "                h.fill(self.test_set[:,i])\n",
    "                h2.fill(self.gen[:,i])\n",
    "                h3.fill(self.gen_corr[:,i])\n",
    "                i+=1\n",
    "            else:\n",
    "                a=np.quantile(m_t,0.001)\n",
    "                b=np.quantile(m_t,0.999)\n",
    "                h=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h2=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                h3=hist.Hist(hist.axis.Regular(bins,a,b))\n",
    "                bins = h.axes[0].edges\n",
    "                h.fill(m_t)\n",
    "                h2.fill(m)\n",
    "                h3.fill(m_c)\n",
    "            fig,ax=plt.subplots(2,1,gridspec_kw={'height_ratios': [3, 1]},figsize=(8,8))\n",
    "            \n",
    "#             hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            try:\n",
    "                main_ax_artists, sublot_ax_arists = h.plot_ratio(\n",
    "                    h3,\n",
    "                    ax_dict={\"main_ax\":ax[0],\"ratio_ax\":ax[1]},\n",
    "                    rp_ylabel=r\"Ratio\",\n",
    "                    rp_num_label=\"MC Simulated\",\n",
    "                    rp_denom_label=\"Flow Generated+Corrected\",\n",
    "                    rp_uncert_draw_type=\"line\",  # line or bar\n",
    "                )\n",
    "                \n",
    "                h2.plot(ax=ax[0],label=\"Flow Generated\")\n",
    "                ax[0].set_xlabel(\"\")\n",
    "#                 if quantile and v==\"m\" and plot_vline:\n",
    "#                     ax[0].hist(m[m_t<np.quantile(m_t,0.1)],histtype='step',bins=bins,alpha=1,color=\"red\",label=\"10% quantile Flow Generated\",hatch=\"/\")\n",
    "#                     ax[0].vlines(np.quantile(m_t,0.1),0,np.max(h[:]),color=\"red\",label='10% quantile MC Simulated')\n",
    "                    \n",
    "                ax[1].set_ylim(0.25,2)\n",
    "                ax[0].set_xlim(a,b)\n",
    "                ax[0].legend()\n",
    "                ax[1].set_xlim(a,b)\n",
    "#                 if v!=\"m\":\n",
    "#                     ax[0].legend([\"Flow Generated\",\"MC Simulated\"])\n",
    "#                 elif plot_vline:\n",
    "#                     ax[0].legend([\"Flow Generated\",\"MC Simulated\"] )\n",
    "            except:\n",
    "                print(\"mass plot failed reverting to simple plot mass bins\")\n",
    "                traceback.print_exc()\n",
    "                plt.close()\n",
    "                plt.figure()\n",
    "                _,b,_=plt.hist(m_t,15,label=\"Sim\",alpha=0.5)\n",
    "                plt.hist(m,b,label=\"Gen\",alpha=0.5)\n",
    "                plt.legend()  \n",
    "           # hep.cms.label(data=False,lumi=None ,year=None,rlabel=\"\",llabel=\"Private Work\",ax=ax[0] )\n",
    "            ax[1].set_xlabel(name,fontsize=28)\n",
    "            ax[0].set_ylabel(\"Counts\",fontsize=28)\n",
    "            ax[1].set_ylabel(\"Ratio\",fontsize=28)\n",
    "            \n",
    "            plt.tight_layout(pad=2)\n",
    "            if save:\n",
    "                if v!=\"m\":\n",
    "                     self.summary.add_figure(\"inclusive\"+v,fig,self.step)\n",
    "                else:\n",
    "                    self.summary.add_figure(\"jet_mass\",fig,self.step)\n",
    "    #             print(\"added figure\")\n",
    "    #             self.summary.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    def plot_correlations(self,save=True):\n",
    "        #Plots correlations between all particles for i=0 eta,i=1 phi,i=2 pt\n",
    "        self.plot_corr(i=0,save=save)\n",
    "        self.plot_corr(i=1,save=save)\n",
    "        self.plot_corr(i=2,save=save)\n",
    "\n",
    "    def plot_corr(self,i=0,names=[\"$\\eta^{rel}$\",\"$\\phi^{rel}$\",\"$p_T$\"],save=True):\n",
    "        if i==2:\n",
    "            c=1\n",
    "        else:\n",
    "            c=.25\n",
    "        df_g=pd.DataFrame(self.gen[:,:self.n_dim][:,range(i,90,3)])\n",
    "        df_h=pd.DataFrame(self.test_set[:,:self.n_dim][:,range(i,90,3)])\n",
    "        \n",
    "        fig,ax=plt.subplots(ncols=2,figsize=(15,7.5))\n",
    "        corr_g = ax[0].matshow(df_g.corr())\n",
    "        corr_g.set_clim(-c,c)\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar=fig.colorbar(corr_g,cax=cax)\n",
    "\n",
    "        corr_h = ax[1].matshow(df_h.corr())\n",
    "        corr_h.set_clim(-c,c)\n",
    "        divider = make_axes_locatable(ax[1])\n",
    "\n",
    "        cax2 = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar=fig.colorbar(corr_h,cax=cax2)\n",
    "        plt.suptitle(\"{} Correlation between Particles\".format(names[i]),fontsize=38)\n",
    "        ax[0].set_title(\"Flow Generated\",fontsize=34)\n",
    "        ax[1].set_title(\"MC Simulated\",fontsize=28)\n",
    "        ax[0].set_xlabel(\"Particles\",fontsize=28)\n",
    "        ax[0].set_ylabel(\"Particles\",fontsize=28)\n",
    "        ax[1].set_xlabel(\"Particles\",fontsize=28)\n",
    "        ax[1].set_ylabel(\"Particles\",fontsize=28)\n",
    "        ax[0].set_xticks([])\n",
    "        ax[1].set_xticks([])\n",
    "        ax[0].set_yticks([])\n",
    "        ax[1].set_yticks([])\n",
    "        if save:\n",
    "                title=[\"corr_eta\",\"corr_phi\",\"corr_pt\"]\n",
    "                self.summary.add_figure(title[i],fig,self.step)\n",
    "                \n",
    "    #             self.summary.close()\n",
    "        else:\n",
    "                plt.show()\n",
    "from torch import optim\n",
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "            def __init__(self, optimizer, warmup, max_iters):\n",
    "                self.warmup = warmup\n",
    "                self.max_num_iters = max_iters\n",
    "                super().__init__(optimizer)\n",
    "\n",
    "            def get_lr(self):\n",
    "                lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "                return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "            def get_lr_factor(self, epoch):\n",
    "                lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "                if epoch <= self.warmup:\n",
    "                    lr_factor *= epoch * 1.0 / self.warmup\n",
    "                return lr_factor\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self, mean=None, std=None, epsilon=1e-7):\n",
    "        \"\"\"Standard Scaler.\n",
    "        The class can be used to normalize PyTorch Tensors using native functions. The module does not expect the\n",
    "        tensors to be of any specific shape; as long as the features are the last dimension in the tensor, the module\n",
    "        will work fine.\n",
    "        :param mean: The mean of the features. The property will be set after a call to fit.\n",
    "        :param std: The standard deviation of the features. The property will be set after a call to fit.\n",
    "        :param epsilon: Used to avoid a Division-By-Zero exception.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, values):\n",
    "        dims = list(range(values.dim() - 1))\n",
    "        self.mean = torch.mean(values, dim=dims)\n",
    "        self.std = torch.std(values, dim=dims)\n",
    "\n",
    "    def transform(self, values):\n",
    "        return (values - self.mean) / (self.std + self.epsilon)\n",
    "    def inverse_transform(self,values):\n",
    "        return (values *self.std)+self.mean\n",
    "    def fit_transform(self, values):\n",
    "        self.fit(values)\n",
    "        return self.transform(values)\n",
    "    def to(self,dev):\n",
    "        self.std=self.std.to(dev)\n",
    "        self.mean=self.mean.to(dev)\n",
    "        return self\n",
    "  \n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "class JetNetDataloader(pl.LightningDataModule):\n",
    "    '''This is more or less standard boilerplate coded that builds the data loader of the training\n",
    "       one thing to note is the custom standard scaler that works on tensors\n",
    "       Currently only jets with 30 particles are used but this maybe changes soon'''\n",
    "    def __init__(self,config): \n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.n_dim=config[\"n_dim\"]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        self.batch_size=config[\"batch_size\"]\n",
    "    def setup(self,stage):\n",
    "    # This just sets up the dataloader, nothing particularly important. it reads in a csv, calculates mass and reads out the number particles per jet\n",
    "    # And adds it to the dataset as variable. The only important thing is that we add noise to zero padded jets\n",
    "        data_dir=os.environ[\"HOME\"]+\"/JetNet_NF/train_{}_jets.csv\".format(self.config[\"parton\"])\n",
    "        data=pd.read_csv(data_dir,sep=\" \",header=None)\n",
    "        jets=[]\n",
    "        limit=int(self.config[\"limit\"]*1.1)\n",
    "        for njets in range(1,31):\n",
    "            masks=np.sum(data.values[:,np.arange(3,120,4)],axis=1)\n",
    "            df=data.loc[masks==njets,:]\n",
    "            df=df.drop(np.arange(3,120,4),axis=1)\n",
    "            df[\"n\"]=njets\n",
    "            if len(df)>100:\n",
    "                jets.append(df[:self.config[\"limit\"]])\n",
    "        #stacking together differnet samples with different number particles per jet\n",
    "        self.n=torch.empty((0,1))\n",
    "        self.data=torch.empty((0,self.n_dim*self.n_part))\n",
    "        for i in range(len(jets)):\n",
    "            x=torch.tensor(jets[i].values[:,:self.n_dim*self.n_part]).float()\n",
    "            n=torch.tensor(jets[i][\"n\"].values).float()\n",
    "            self.data=torch.vstack((self.data,x))\n",
    "            self.n=torch.vstack((self.n.reshape(-1,1),n.reshape(-1,1)))        \n",
    "        \n",
    "      \n",
    "        # calculating mass per jet\n",
    "#         self.m=mass(self.data[:,:self.n_dim]).reshape(-1,1)  \n",
    "      # Adding noise to zero padded jets.\n",
    "        for i in torch.unique(self.n):\n",
    "            i=int(i)\n",
    "            self.data[self.data[:,-1]==i,3*i:90]=torch.normal(mean=torch.zeros_like(self.data[self.data[:,-1]==i,3*i:90]),std=1).abs()*1e-7\n",
    "        #standard scaling \n",
    "        self.scaler=StandardScaler()\n",
    "#         self.data=torch.hstack((self.data,self.m))        \n",
    "        self.scaler.fit(self.data)\n",
    "        self.data=self.scaler.transform(self.data)\n",
    "#         self.min_m=self.scaler.transform(torch.zeros((1,self.n_dim+1)))[0,-1]\n",
    "# #         self.data=torch.hstack((self.data,self.n))\n",
    "        \n",
    "#         #calculating mass dist in different bins, this is needed for the testcase where we need to generate the conditoon\n",
    "#         if self.config[\"variable\"]:\n",
    "#             self.mdists={}\n",
    "#             for i in torch.unique(self.n):\n",
    "#                 self.mdists[int(i)]=F(self.data[self.n[:,0]==i,-2])    \n",
    "        self.data,self.test_set=train_test_split(self.data.cpu().numpy(),test_size=0.3)\n",
    "        \n",
    "#         self.n_train=self.data[:,-1]\n",
    "#         self.n_test=self.test_set[:,-1]\n",
    "        \n",
    "            \n",
    "        self.test_set=torch.tensor(self.test_set).float()\n",
    "        self.data=torch.tensor(self.data).float()\n",
    "#         assert self.data.shape[1]==92\n",
    "        assert (torch.isnan(self.data)).sum()==0\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data, batch_size=self.batch_size,drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=len(self.test_set),drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0962d61-06a4-42ec-9857-7b74c2a738c3",
   "metadata": {},
   "source": [
    " # STOP\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3cfa48-d309-4d02-bd40-9ce2386e721c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import leaky_relu,sigmoid\n",
    "\n",
    "class Disc(nn.Module):\n",
    "    def __init__(self,n_dim=3,l_dim=10,hidden=300,num_layers=3,num_heads=1,n_part=2,fc=True):\n",
    "        super().__init__()\n",
    "        self.hidden_nodes=hidden\n",
    "        self.n_dim=n_dim\n",
    "        self.l_dim=l_dim\n",
    "        self.n_part=n_part\n",
    "        self.fc=fc\n",
    "\n",
    "        \n",
    "        if fc:\n",
    "            self.l_dim*=n_part \n",
    "            self.embbed_flat=nn.Linear(n_dim*n_part,l_dim)\n",
    "            self.flat_hidden=nn.Linear(l_dim,hidden)\n",
    "            self.flat_hidden2=nn.Linear(hidden,hidden)\n",
    "            self.flat_hidden3=nn.Linear(hidden,hidden)\n",
    "            self.flat_out=nn.Linear(hidden,1)\n",
    "        else:\n",
    "            self.embbed=nn.Linear(n_dim,l_dim)\n",
    "            self.encoder=nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=l_dim,nhead=num_heads,dim_feedforward=hidden,dropout=0.1,norm_first=True,\n",
    "                                       activation=\"relu\",batch_first=True) ,num_layers=num_layers)\n",
    "            self.hidden=nn.Linear(l_dim,hidden)\n",
    "            self.hidden2=nn.Linear(hidden,hidden)\n",
    "            self.out=nn.Linear(hidden,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        if self.fc==True:\n",
    "            x=x.reshape(len(x),self.n_dim*self.n_part)\n",
    "            x=self.embbed_flat(x)\n",
    "            x=leaky_relu(self.flat_hidden(x),0.2)\n",
    "            x=leaky_relu(self.flat_hidden2(x),0.2)\n",
    "            x=self.flat_out(x)\n",
    "        else:\n",
    "            x=self.embbed(x)\n",
    "#             x=torch.concat((torch.ones_like(x[:,0,:]).reshape(len(x),1,-1),x),axis=1)\n",
    "            x=self.encoder(x)\n",
    "            x=torch.sum(x,axis=1)\n",
    "#             x=x[:,0,:]\n",
    "            \n",
    "            x=leaky_relu(self.hidden(x),0.2)\n",
    "            x=self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# disc=Disc(fc=True,n_part=1)\n",
    "# opt=torch.optim.Adam(disc.parameters())\n",
    "# sig=nn.Sigmoid()\n",
    "# losses=[]\n",
    "# for i in range(100):\n",
    "#     a=sig(disc(torch.rand(1000,3)))\n",
    "#     b=sig(disc(torch.rand(1000,3)*100))\n",
    "#     true=torch.ones(len(a)).reshape(-1,1)\n",
    "#     fake=torch.zeros(len(b)).reshape(-1,1)\n",
    "#     best=torch.vstack((a,b))\n",
    "#     true=torch.vstack((true,fake))\n",
    "#     loss=nn.BCELoss()(best,true)\n",
    "#     opt.zero_grad()\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     losses.append(loss.detach().numpy())\n",
    "# plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b79b3a6-a629-4c2d-a98b-7e34e64df1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/torch/nn/init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtfman\n",
      "model initialized\n",
      "data_module loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | q0      | StandardNormal     | 0     \n",
      "1 | q_test  | StandardNormal     | 0     \n",
      "2 | flows   | CompositeTransform | 1.6 M \n",
      "3 | flow    | Flow               | 1.6 M \n",
      "4 | dis_net | Disc               | 507 K \n",
      "5 | sig     | Sigmoid            | 0     \n",
      "-----------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.289     Total estimated model params size (MB)\n",
      "/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de26f3a29b24961967f49206fc68f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 203>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger,  log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# auto_scale_batch_size=\"binsearch\",\u001b[39;00m\n\u001b[1;32m    199\u001b[0m                      max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m,\u001b[38;5;66;03m#,gradient_clip_val=0.01, gradient_clip_algorithm=\"norm\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m                      check_val_every_n_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m ,num_sanity_val_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,track_grad_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    201\u001b[0m                     fast_dev_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# This calls the fit function which trains the model\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 768\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    805\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    807\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    808\u001b[0m )\n\u001b[0;32m--> 809\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1234\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(\n\u001b[1;32m    266\u001b[0m     dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:90\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_loop\u001b[38;5;241m.\u001b[39mrun(split_batch, optimizers, batch_idx)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# automatic: can be empty if all optimizers skip their batches\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# manual: #9052 added support for raising `StopIteration` in the `training_step`. If that happens,\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# then `advance` doesn't finish and an empty dict is returned\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py:115\u001b[0m, in \u001b[0;36mManualOptimization.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    110\u001b[0m step_kwargs \u001b[38;5;241m=\u001b[39m _build_training_step_kwargs(\n\u001b[1;32m    111\u001b[0m     lightning_module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, batch, batch_idx, opt_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hiddens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hiddens\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m step_kwargs\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1763\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:333\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m\"\"\"The actual training step.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mTransGan.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 123\u001b[0m         z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(batch),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_part,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dim)\n\u001b[1;32m    124\u001b[0m     fake\u001b[38;5;241m=\u001b[39mz\n\u001b[1;32m    125\u001b[0m     pred_real\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msig(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdis_net(batch))\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/distributions/base.py:65\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, num_samples, context, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m     context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(context)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check\u001b[38;5;241m.\u001b[39mis_positive_int(batch_size):\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/flows/base.py:54\u001b[0m, in \u001b[0;36mFlow._sample\u001b[0;34m(self, num_samples, context)\u001b[0m\n\u001b[1;32m     49\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torchutils\u001b[38;5;241m.\u001b[39mmerge_leading_dims(noise, num_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     50\u001b[0m     embedded_context \u001b[38;5;241m=\u001b[39m torchutils\u001b[38;5;241m.\u001b[39mrepeat_rows(\n\u001b[1;32m     51\u001b[0m         embedded_context, num_reps\u001b[38;5;241m=\u001b[39mnum_samples\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 54\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedded_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedded_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Split the context dimension from sample dimension.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     samples \u001b[38;5;241m=\u001b[39m torchutils\u001b[38;5;241m.\u001b[39msplit_leading_dim(samples, shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_samples])\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/base.py:60\u001b[0m, in \u001b[0;36mCompositeTransform.inverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m     funcs \u001b[38;5;241m=\u001b[39m (transform\u001b[38;5;241m.\u001b[39minverse \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transforms[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cascade\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuncs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/base.py:50\u001b[0m, in \u001b[0;36mCompositeTransform._cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m total_logabsdet \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mnew_zeros(batch_size)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m funcs:\n\u001b[0;32m---> 50\u001b[0m     outputs, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     total_logabsdet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logabsdet\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, total_logabsdet\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/coupling.py:121\u001b[0m, in \u001b[0;36mCouplingTransform.inverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m    116\u001b[0m     identity_split, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconditional_transform\u001b[38;5;241m.\u001b[39minverse(\n\u001b[1;32m    117\u001b[0m         identity_split, context\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    120\u001b[0m transform_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_net(identity_split, context)\n\u001b[0;32m--> 121\u001b[0m transform_split, logabsdet_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coupling_transform_inverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_params\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m logabsdet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logabsdet_split\n\u001b[1;32m    126\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty_like(inputs)\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/coupling.py:277\u001b[0m, in \u001b[0;36mPiecewiseCouplingTransform._coupling_transform_inverse\u001b[0;34m(self, inputs, transform_params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_coupling_transform_inverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, transform_params):\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coupling_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/coupling.py:291\u001b[0m, in \u001b[0;36mPiecewiseCouplingTransform._coupling_transform\u001b[0;34m(self, inputs, transform_params, inverse)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# For 2D data, reshape transform_params from Bx(D*?) to BxDx?\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     transform_params \u001b[38;5;241m=\u001b[39m transform_params\u001b[38;5;241m.\u001b[39mreshape(b, d, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m outputs, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_piecewise_cdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, torchutils\u001b[38;5;241m.\u001b[39msum_except_batch(logabsdet)\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/coupling.py:579\u001b[0m, in \u001b[0;36mPiecewiseRationalQuadraticCouplingTransform._piecewise_cdf\u001b[0;34m(self, inputs, transform_params, inverse)\u001b[0m\n\u001b[1;32m    577\u001b[0m     spline_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtails\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtails, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail_bound\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtail_bound}\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m#print(\"max:\",inputs.max(axis=1),\"min:\",inputs.min(axis=1))\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspline_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43munnormalized_widths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munnormalized_widths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43munnormalized_heights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munnormalized_heights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43munnormalized_derivatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munnormalized_derivatives\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_bin_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_bin_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_bin_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_bin_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_derivative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_derivative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mspline_kwargs\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/splines/rational_quadratic.py:43\u001b[0m, in \u001b[0;36munconstrained_rational_quadratic_spline\u001b[0;34m(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse, tails, tail_bound, min_bin_width, min_bin_height, min_derivative)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# logabsdet += outside_interval_mask*0\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m tails are not implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tails))\n\u001b[1;32m     40\u001b[0m (\n\u001b[1;32m     41\u001b[0m     inside_outputs,\n\u001b[1;32m     42\u001b[0m     inside_logabsdet,\n\u001b[0;32m---> 43\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mrational_quadratic_spline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Clamp inputs to the domain to prevent out of domain errors\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtail_bound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_bound\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43munnormalized_widths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munnormalized_widths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43munnormalized_heights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munnormalized_heights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43munnormalized_derivatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munnormalized_derivatives\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtail_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtail_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_bin_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_bin_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_bin_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_bin_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_derivative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_derivative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inside_interval_mask\u001b[38;5;241m*\u001b[39minside_outputs\n\u001b[1;32m     60\u001b[0m logabsdet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inside_interval_mask\u001b[38;5;241m*\u001b[39minside_logabsdet\n",
      "File \u001b[0;32m/beegfs/desy/user/kaechben/.conda/envs/jetnet/lib/python3.8/site-packages/nflows/transforms/splines/rational_quadratic.py:167\u001b[0m, in \u001b[0;36mrational_quadratic_spline\u001b[0;34m(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse, left, right, bottom, top, min_bin_width, min_bin_height, min_derivative)\u001b[0m\n\u001b[1;32m    161\u001b[0m     derivative_numerator \u001b[38;5;241m=\u001b[39m input_delta\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    162\u001b[0m         input_derivatives_plus_one \u001b[38;5;241m*\u001b[39m root\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m input_delta \u001b[38;5;241m*\u001b[39m theta_one_minus_theta\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;241m+\u001b[39m input_derivatives \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m root)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m     logabsdet \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(derivative_numerator) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(denominator)\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (outputs\u001b[38;5;241m==\u001b[39moutputs)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, \u001b[38;5;241m-\u001b[39mlogabsdet\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    " class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "            def __init__(self, optimizer, warmup, max_iters):\n",
    "                self.warmup = warmup\n",
    "                self.max_num_iters = max_iters\n",
    "                super().__init__(optimizer)\n",
    "\n",
    "            def get_lr(self):\n",
    "                lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "                return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "            def get_lr_factor(self, epoch):\n",
    "                lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "                if epoch <= self.warmup:\n",
    "                    lr_factor *= epoch * 1.0 / self.warmup\n",
    "                return lr_factor\n",
    "        \n",
    "class TransGan(pl.LightningModule):\n",
    "    def create_resnet(self,in_features, out_features):\n",
    "        '''This is the network that outputs the parameters of the invertible transformation\n",
    "        The only arguments can be the in dimension and the out dimenson, the structure\n",
    "        of the network is defined over the config which is a class attribute\n",
    "        Context Features: Amount of features used to condition the flow - in our case \n",
    "        this is usually the mass\n",
    "        num_blocks: How many Resnet blocks should be used, one res net block is are 1 input+ 2 layers\n",
    "        and an additive skip connection from the first to the third'''\n",
    "        c=self.config[\"context_features\"]\n",
    "        return nets.ResidualNet(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                hidden_features=self.config[\"network_nodes_nf\"],\n",
    "                context_features=c,\n",
    "                num_blocks=self.config[\"network_layers_nf\"],\n",
    "                activation=self.config[\"activation\"]  if \"activation\" in self.config.keys() else FF.relu,\n",
    "                dropout_probability=config[\"dropout\"] if \"dropout\" in self.config.keys() else 0,\n",
    "                use_batch_norm=self.config[\"batchnorm\"] if \"batchnorm\" in self.config.keys() else 0\n",
    "        )\n",
    "\n",
    "    def __init__(self,config,hyperopt):\n",
    "        \n",
    "        '''This initializes the model and its hyperparameters'''\n",
    "        super().__init__()\n",
    "        self.automatic_optimization=False\n",
    "        self.freq_d=config[\"freq\"]\n",
    "        self.wgan=config[\"wgan\"]\n",
    "        self.config=config\n",
    "        self.counter=0 #This counts how many nan grads we have, we break after 5 in a row\n",
    "        self.hyperopt=hyperopt\n",
    "        #Metrics to track during the training\n",
    "        self.metrics={\"val_w1p\":[],\"val_w1m\":[],\"val_w1efp\":[],\"val_cov\":[],\"val_mmd\":[],\"val_fpnd\":[],\"val_logprob\":[],\"step\":[]}\n",
    "        #Loss function of the Normalizing flows\n",
    "        self.logprobs=[]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        self.hparams.update(config)\n",
    "        self.save_hyperparameters()\n",
    "        #This is the Normalizing flow model to be used later, it uses as many\n",
    "        #coupling_layers as given in the config \n",
    "        self.flows = []\n",
    "        self.n_dim=self.config[\"n_dim\"]\n",
    "        self.n_part=config[\"n_part\"]\n",
    "        K=self.config[\"coupling_layers\"]\n",
    "        for i in range(K):\n",
    "            '''This creates the masks for the coupling layers, particle masks are masks\n",
    "            created such that each feature particle (eta,phi,pt) is masked together or not'''\n",
    "            if self.config[\"autoreg\"]:\n",
    "                self.flows += [MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "#                         random_mask=True,\n",
    "                        features=self.n_dim,\n",
    "                        hidden_features=128,\n",
    "                        use_residual_blocks=True, \n",
    "                        tails='linear',\n",
    "                        tail_bound=self.config[\"tail_bound\"],\n",
    "                        num_bins=self.config[\"bins\"] )]\n",
    "            else:\n",
    "                mask=create_random_binary_mask(self.n_dim*self.n_part)            \n",
    "                self.flows += [PiecewiseRationalQuadraticCouplingTransform(\n",
    "                            mask=mask,\n",
    "                            transform_net_create_fn= self.create_resnet, \n",
    "                            tails='linear',\n",
    "                            tail_bound=self.config[\"tail_bound\"],\n",
    "                            num_bins=self.config[\"bins\"] )]\n",
    "\n",
    "        self.q0 = nf.distributions.normal.StandardNormal([self.n_dim*self.n_part])\n",
    "        self.q_test =nf.distributions.normal.StandardNormal([self.n_dim*self.n_part])\n",
    "        self.flows=CompositeTransform(self.flows)\n",
    "        self.flow = base.Flow(distribution=self.q0, transform=self.flows)\n",
    "        self.dis_net = Disc(n_dim=self.n_dim,hidden=config[\"hidden\"],num_layers=config[\"num_layers\"],\n",
    "                            num_heads=config[\"heads\"],fc=config[\"fc\"],n_part=config[\"n_part\"]).cuda()\n",
    "        self.alpha=1\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.add_corr=config[\"corr\"]\n",
    "    def load_datamodule(self,data_module):\n",
    "        '''needed for lightning training to work, it just sets the dataloader for training and validation'''\n",
    "        self.data_module=data_module\n",
    "        \n",
    "\n",
    "     \n",
    "    def configure_optimizers(self):\n",
    "        self.batch_size=self.config[\"batch_size\"]\n",
    "        self.lr=0.01\n",
    "        opt_d = torch.optim.Adam(self.dis_net.parameters(), lr=self.lr)\n",
    "        opt_nf = torch.optim.AdamW(self.flow.parameters(), lr=self.lr)\n",
    "\n",
    "        return [opt_nf,opt_d]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"training loop of the model, here all the data is passed forward to a gaussian\n",
    "            This is the important part what is happening here. This is all the training we do \"\"\"\n",
    "        opt_nf,opt_d=self.optimizers()\n",
    "        nf_loss=0\n",
    "        d_loss_avg=0\n",
    "        gradient_penalty=0\n",
    "        if self.global_step<1200:\n",
    "            nf_loss -=self.flow.to(self.device).log_prob(batch).mean()#c if self.config[\"context_features\"] else None\n",
    "            nf_loss/=(self.n_dim*self.n_part) \n",
    "            opt_nf.zero_grad()\n",
    "            self.manual_backward(nf_loss)\n",
    "            opt_nf.step()\n",
    "            self.log(\"logprob\", nf_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True) \n",
    "        batch=batch.reshape(len(batch),self.n_part,self.n_dim)\n",
    "        if self.current_epoch>-1:\n",
    "            with torch.no_grad():\n",
    "                z=self.flow.sample(len(batch)).reshape(len(batch),self.n_part,self.n_dim)\n",
    "            fake=z\n",
    "            pred_real=self.sig(self.dis_net(batch))\n",
    "            pred_fake=self.sig(self.dis_net(fake.detach()))\n",
    "            target_real=torch.ones_like(pred_real)\n",
    "            target_fake=torch.zeros_like(pred_fake)\n",
    "            d_loss=nn.BCELoss()(pred_fake.view(-1),target_fake.view(-1)).mean()\n",
    "            d_loss+=nn.BCELoss()(pred_real.view(-1),target_real.view(-1)).mean()\n",
    "            d_loss/=2\n",
    "            opt_d.zero_grad()\n",
    "            self.manual_backward(d_loss)\n",
    "            opt_d.step()\n",
    "            d_loss_avg+=d_loss.cpu().detach().numpy()-10*gradient_penalty\n",
    "            self.log(\"d_loss\",d_loss_avg/self.freq_d,logger=False,prog_bar=True)\n",
    "            self.logger.experiment.add_scalars(\"d_losses\",{\"train_disc\":d_loss_avg/self.freq_d},global_step=self.global_step)\n",
    "            if self.global_step%200==0 :\n",
    "\n",
    "                    fig,ax=plt.subplots()\n",
    "                    ax.hist(self.sig(pred_fake).detach().cpu().numpy(),label=\"fake\",bins=np.linspace(0,1,30) if not self.wgan else 30,histtype='step')\n",
    "                    ax.hist(self.sig(pred_real).detach().cpu().numpy(),label=\"real\",bins=np.linspace(0,1,30) if not self.wgan else 30,histtype='step')\n",
    "                    ax.legend()\n",
    "                    plt.ylabel(\"Counts\")\n",
    "                    plt.xlabel(\"Score\")\n",
    "                    self.logger.experiment.add_figure(\"train_class\",fig,global_step=self.global_step)\n",
    "\n",
    "  \n",
    "        \n",
    "   \n",
    "config = {\n",
    "       \"network_layers\": 3,  # sets amount hidden layers in transformation networks -scannable\n",
    "        \"network_nodes\": 6,  # amount nodes in hidden layers in transformation networks -scannable\n",
    "        \"network_layers_nf\": 2,  # sets amount hidden layers in transformation networks -scannable\n",
    "        \"network_nodes_nf\": 256,  # amount nodes in hidden layers in transformation networks -scannable\n",
    "        \"batch_size\": 5000,  # sets batch size -scannable\n",
    "        \"embedding_features\":8,\n",
    "        \"coupling_layers\": 5,  # amount of invertible transformations to use -scannable\n",
    "        \"lr\": 0.001,  # sets learning rate -scannable\n",
    "        \"batchnorm\": False,  # use batchnorm or not -scannable\n",
    "        \"bins\": 8,  # amount of bins to use in rational quadratic splines -scannable\n",
    "        \"tail_bound\": 6,  # splines:max value that is transformed, over this value theree is id  -scannable\n",
    "        \"limit\": 150000,  # how many data points to use, test_set is 10% of this -scannable in a sense use 10 k for faster training\n",
    "        \"n_dim\": 3,  # how many dimensions to use or equivalently /3 gives the amount of particles to use NEVER EVER CHANGE THIS\n",
    "        \"dropout\": 0.4,  # use droput proportion, for 0 there is no dropout -scannable\n",
    "        \"canonical\": False,  # transform data coordinates to px,py,pz -scannable\n",
    "        \"max_steps\": 100000,  # how many steps to use at max - lower for quicker training\n",
    "        \"lambda\": 10,  # balance between massloss and nll -scannable\n",
    "        \"name\": \"t\",  # name for logging folder\n",
    "        \"disc\": False,  # whether to train gan style discriminator that decides whether point is simulated or generated-semi-scannable\n",
    "        \"calc_massloss\": False, # whether to calculate mass loss, makes training slower, do not use with autoregressive! \n",
    "        \"context_features\":0, #amount of variables used for conditioning, for 0 no conditioning is used, for 1 o nly the mass is used, for 2 also the number part is used\n",
    "        \"variable\":1, #use variable amount of particles otherwise only use 30, options are true or false \n",
    "        \"parton\":\"t\", #choose the dataset you want to train options: t for top,q for quark,g for gluon\n",
    "        \"oversampling\":False,\n",
    "        \"wgan\":False,\n",
    "        \"corr\":True,\n",
    "        \"num_layers\":4,\n",
    "        \"autoreg\":False,\n",
    "        \"freq\":1,\n",
    "        \"n_part\":5,\n",
    "        \"fc\":True,\n",
    "        \"hidden\":500,\n",
    "        \"heads\":2\n",
    "    }     \n",
    "print(\"wtfman\")\n",
    "model=TransGan(config,False)\n",
    "print(\"model initialized\")\n",
    "\n",
    "data_module = JetNetDataloader(config) #this loads the data\n",
    "\n",
    "model.load_datamodule(data_module)#adds datamodule to model\n",
    "print(\"data_module loader\")\n",
    "model.config = config #config are our hyperparams, we make this a class property now\n",
    "logger = TensorBoardLogger(\"./lightning_logs\")\n",
    "#log every n steps could be important as it decides how often it should log to tensorboard\n",
    "# Also check val every n epochs, as validation checking takes some time\n",
    "trainer = pl.Trainer(gpus=1, logger=logger,  log_every_n_steps=10,  # auto_scale_batch_size=\"binsearch\",\n",
    "                      max_steps=1000000,#,gradient_clip_val=0.01, gradient_clip_algorithm=\"norm\"\n",
    "                      check_val_every_n_epoch=20 ,num_sanity_val_steps=1,track_grad_norm=2,\n",
    "                     fast_dev_run=False,max_epochs=-1)\n",
    "# This calls the fit function which trains the model\n",
    "trainer.fit(model, train_dataloaders=data_module )  \n",
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a41622-19c2-4c9e-b644-d21f2c809087",
   "metadata": {},
   "outputs": [],
   "source": [
    " def validation_step(self, batch, batch_idx):\n",
    "        '''This calculates some important metrics on the hold out set (checking for overtraining)'''\n",
    "        batch=batch[:1000,:]\n",
    "        \n",
    "        self.data_module.scaler.to(\"cpu\")  \n",
    "        batch=batch.to(\"cpu\")\n",
    "        self.dis_net=self.dis_net.train()\n",
    "\n",
    "        c=None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "#             z=torch.empty((len(batch),self.n_dim))\n",
    "#             for i in range(self.n_dim//3):\n",
    "#                 temp=self.flow.sample(len(batch)).detach().cpu()\n",
    "#                 z[:,3*i:3*(i+1)]=temp\n",
    "            z=self.flow.sample(len(batch)).detach().cpu()\n",
    "            z=z.reshape(len(batch),self.n_part,self.n_dim)\n",
    "            \n",
    "            # if self.config[\"oversampling\"]:\n",
    "            #     order=torch.sort(test.reshape(-1,30,3)[:,:,2],dim=1,descending=True)[1]\n",
    "            #     test=torch.gather(input=test.reshape(-1,30,3),index=order.unsqueeze(-1).repeat(1,1,3),dim=1).reshape(-1,90)\n",
    "            #test=test.reshape(-1,30,3)[order.repeat(1,1,3)].reshape(-1,90)\n",
    "            #z+(1-alpha)*\n",
    "            gen=z\n",
    "#             if self.add_corr:\n",
    "#                 gen_corr=z+(1-self.alpha)*self.gen_net.to(\"cpu\")(z)#\n",
    "#             else:\n",
    "#                 gen_corr=self.alpha*z.to(\"cpu\")+(1-self.alpha)*self.gen_net.to(\"cpu\")(z)\n",
    "            gen_corr=gen\n",
    "            \n",
    "            \n",
    "            scores_f=(self.dis_net.to(\"cpu\").train()(z).view(-1))\n",
    "            scores_t=(self.dis_net.to(\"cpu\").train()(batch.reshape(len(batch),self.n_part,self.n_dim)).view(-1))\n",
    "            scores_c=(self.dis_net.to(\"cpu\").train()(gen_corr).view(-1))\n",
    "            \n",
    "            bins=50\n",
    "            if not self.wgan:\n",
    "                bins=np.linspace(0,1,bins)\n",
    "                scores_f=nn.Sigmoid()(scores_f)\n",
    "                scores_t=nn.Sigmoid()(scores_t)\n",
    "                scores_c=nn.Sigmoid()(scores_c)\n",
    "                real_loss=nn.BCELoss()(scores_t,torch.ones_like(scores_t))\n",
    "                fake_loss_gan=nn.BCELoss()(scores_c,torch.zeros_like(scores_t))\n",
    "                fake_loss_nf=nn.BCELoss()(scores_f,torch.zeros_like(scores_t))\n",
    "                g_loss=nn.BCELoss()(scores_c,torch.ones_like(scores_c))\n",
    "                num=2\n",
    "    #                 if self.global_step<100+pretrain:\n",
    "    #                     fake_loss=0\n",
    "    #                     num=2\n",
    "                    #d_loss=(real_loss+(1-self.alpha)*fake_loss_gan+self.alpha*fake_loss_nf)/num\n",
    "                d_loss=(real_loss+fake_loss_nf)/num\n",
    "                self.logger.experiment.add_scalars(\"d_losses\",{\"val_disc\":d_loss},global_step=self.global_step)\n",
    "                self.logger.experiment.add_scalars(\"g_losses\",{\"val_gen\":g_loss},global_step=self.global_step)\n",
    "                \n",
    "            fig=plt.figure()\n",
    "            _,bins,_=plt.hist(scores_f.numpy(),bins=bins,label=\"fake\",alpha=0.5)\n",
    "            plt.hist(scores_t.numpy(),bins=bins,label=\"true\",alpha=0.5)\n",
    "            plt.hist(scores_c.numpy(),bins=bins,label=\"corr\",alpha=0.5)\n",
    "            plt.legend()\n",
    "            self.logger.experiment.add_figure(\"class_dist\",fig,global_step=self.global_step)\n",
    "            gen=gen[:,:self.n_dim*self.n_part].cpu().detach().reshape(-1,self.n_dim*self.n_part)\n",
    "            \n",
    "        # Reverse Standard Scaling (this has nothing to do with flows, it is a standard preprocessing step)\n",
    "\n",
    "        gen=self.data_module.scaler.inverse_transform(gen)\n",
    "        gen_corr=self.data_module.scaler.inverse_transform(gen_corr.reshape(-1,self.n_dim*self.n_part))\n",
    "        true=self.data_module.scaler.inverse_transform(batch[:,:self.n_dim*self.n_part])[:,:self.n_dim*self.n_part]\n",
    "        m_t=mass(true[:,:self.n_dim*self.n_part].to(self.device),self.config[\"canonical\"]).cpu()\n",
    "        m_gen=mass(gen[:,:self.n_dim*self.n_part],self.config[\"canonical\"]).cpu()\n",
    "        m_c=mass(gen_corr[:,:self.n_dim*self.n_part],self.config[\"canonical\"]).cpu()\n",
    "        self.plot=plotting(model=self,gen=gen[:,:self.n_dim*self.n_part],gen_corr=gen_corr[:,:self.n_dim*self.n_part],true=true[:,:self.n_dim*self.n_part],config=self.config,step=self.global_step,logger=self.logger.experiment)\n",
    "        self.flow=self.flow.to(self.device)\n",
    "        self.gen_net=self.gen_net.to(\"cuda\")\n",
    "        self.dis_net=self.dis_net.to(\"cuda\")\n",
    "        try:\n",
    "            self.plot.plot_mass(m=m_gen.cpu().numpy(),m_t=m_t.cpu().numpy(),m_c=m_c.cpu().numpy(),save=True,bins=15,quantile=True,plot_vline=False)\n",
    "#             self.plot.plot_marginals(save=True)\n",
    "            self.plot.plot_2d(save=True)\n",
    "#             self.plot.var_part(true=true[:,:self.n_dim],gen=test[:,:self.n_dim],true_n=n_true,gen_n=n_test,m_true=m_t,m_gen=m_test ,save=True)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75ff0ae9-845a-450b-8bd6-32667358b831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67686"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import mass\n",
    "del model\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d67c30-d4ca-486d-bd8f-b7b968a0b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004051065444946289\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=15, nhead=1).cuda()\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6).cuda()\n",
    "lin=nn.Sequential(*[nn.Linear(99,99) for i in range(6)]).cuda()\n",
    "import time\n",
    "error=[]\n",
    "for i in range(100):\n",
    "    start=time.time()\n",
    "    src = torch.rand(5000, 99).cuda()\n",
    "    out = lin(src).detach()\n",
    "    error.append(time.time()-start)\n",
    "    \n",
    "print(np.average(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764480a-86cd-499c-86ca-cb7b31c4ad2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetnet",
   "language": "python",
   "name": "jetnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
